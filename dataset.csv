id,name,formal_theorem,informal_theorem,formal_proof,informal_proof,commented_proof,header
504,Bornology.ext,"theorem Bornology.ext (t t' : Bornology α)
    (h_cobounded : @Bornology.cobounded α t = @Bornology.cobounded α t') :
    t = t' := by sorry","Bornology Extensionality: \( \text{cobounded}_t = \text{cobounded}_{t'} \to t = t' \) : For any type \( \alpha \) and any two bornologies \( t \) and \( t' \) on \( \alpha \), if the cobounded filters of \( t \) and \( t' \) are equal, then the bornologies \( t \) and \( t' \) are equal. In other words, if \( \text{cobounded}_t = \text{cobounded}_{t'} \), then \( t = t' \).","theorem Bornology.ext (t t' : Bornology α)
    (h_cobounded : @Bornology.cobounded α t = @Bornology.cobounded α t') :
    t = t' := by
  cases t
  cases t'
  congr","We start by considering the structure of the bornologies \( t \) and \( t' \). Since both \( t \) and \( t' \) are bornologies on \( \alpha \), they each consist of a cobounded filter and a proof that this filter contains the cofinite filter. Specifically, we can write \( t \) as \( \{ \text{cobounded}_t, \text{le\_cofinite}_t \} \) and \( t' \) as \( \{ \text{cobounded}_{t'}, \text{le\_cofinite}_{t'} \} \).

Given the assumption that \( \text{cobounded}_t = \text{cobounded}_{t'} \), we need to show that \( t = t' \). To do this, it suffices to show that the cobounded filters \( \text{cobounded}_t \) and \( \text{cobounded}_{t'} \) are equal and that the proofs \( \text{le\_cofinite}_t \) and \( \text{le\_cofinite}_{t'} \) are equal. Since \( \text{cobounded}_t = \text{cobounded}_{t'} \) by assumption, and both \( \text{le\_cofinite}_t \) and \( \text{le\_cofinite}_{t'} \) are proofs that the cobounded filters contain the cofinite filter, they are trivially equal. Therefore, \( t = t' \).

This completes the proof.","theorem Bornology.ext (t t' : Bornology α)
    (h_cobounded : @Bornology.cobounded α t = @Bornology.cobounded α t') :
    t = t' := by
/- We will consider the structure of the bornology \( t \). Since \( t \) is a bornology, it consists of a cobounded filter \( \text{cobounded}_t \) and a proof that this filter contains the cofinite filter. Thus, we can write \( t \) as \( \{ \text{cobounded}_t, \text{le\_cofinite}_t \} \). -/
  cases t
/- Similarly, we will consider the structure of the bornology \( t' \). Since \( t' \) is a bornology, it consists of a cobounded filter \( \text{cobounded}_{t'} \) and a proof that this filter contains the cofinite filter. Thus, we can write \( t' \) as \( \{ \text{cobounded}_{t'}, \text{le\_cofinite}_{t'} \} \). -/
  cases t'
/- To establish the equality \( t = t' \), it suffices to show that the cobounded filters \( \text{cobounded}_t \) and \( \text{cobounded}_{t'} \) are equal and that the proofs \( \text{le\_cofinite}_t \) and \( \text{le\_cofinite}_{t'} \) are equal. Given that \( \text{cobounded}_t = \text{cobounded}_{t'} \) by assumption, and both \( \text{le\_cofinite}_t \) and \( \text{le\_cofinite}_{t'} \) are proofs that the cobounded filters contain the cofinite filter, they are trivially equal. Therefore, \( t = t' \). -/
  congr","import Mathlib.Order.Filter.Cofinite
import Mathlib.Topology.Bornology.Basic

open Bornology
open Set Filter
variable {ι α β : Type*}
"
1055,Option.mem_map₂_iff,"theorem Option.mem_map₂_iff {c : γ} : c ∈ map₂ f a b ↔ ∃ a' b', a' ∈ a ∧ b' ∈ b ∧ f a' b' = c := by sorry","Membership in Binary Map of Optional Values: \( c \in \text{Option.map₂} \, f \, a \, b \leftrightarrow \exists a', b' \, (a' \in a \land b' \in b \land f(a', b') = c \) : For any types \( \alpha \), \( \beta \), and \( \gamma \), and for any binary function \( f : \alpha \to \beta \to \gamma \), and for any optional values \( a : \text{Option} \, \alpha \) and \( b : \text{Option} \, \beta \), and for any element \( c : \gamma \), the following holds:
\[
c \in \text{Option.map₂} \, f \, a \, b \quad \text{if and only if} \quad \exists a' \in \alpha, \, b' \in \beta, \, a' \in a \, \land \, b' \in b \, \land \, f(a', b') = c
\]
In other words, \( c \) is a member of the result of the binary map of optional values \( \text{Option.map₂} \, f \, a \, b \) if and only if there exist elements \( a' \) and \( b' \) such that \( a' \) is a member of \( a \), \( b' \) is a member of \( b \), and \( f(a', b') = c \).","theorem Option.mem_map₂_iff {c : γ} : c ∈ map₂ f a b ↔ ∃ a' b', a' ∈ a ∧ b' ∈ b ∧ f a' b' = c := by
  simp [map₂, bind_eq_some]","To prove the theorem, we need to show that \( c \in \text{Option.map₂} \, f \, a \, b \) if and only if there exist \( a' \in \alpha \) and \( b' \in \beta \) such that \( a' \in a \), \( b' \in b \), and \( f(a', b') = c \).

First, we use the definition of `Option.map₂` and the property that `Option.bind` is equivalent to `some` when the optional value is `some`. This simplifies our goal to showing that \( c \in \text{Option.map₂} \, f \, a \, b \) if and only if there exist \( a' \) and \( b' \) such that \( a' \in a \), \( b' \in b \), and \( f(a', b') = c \).

By the definition of `Option.map₂`, the expression \( \text{Option.map₂} \, f \, a \, b \) is equivalent to:
\[
\text{Option.bind} \, a \, (\lambda a', \text{Option.bind} \, b \, (\lambda b', \text{some} \, (f(a', b')))
\]
This means that \( c \in \text{Option.map₂} \, f \, a \, b \) if and only if there exist \( a' \) and \( b' \) such that \( a' \in a \), \( b' \in b \), and \( f(a', b') = c \).

Thus, the theorem is proved. This completes the proof.","theorem Option.mem_map₂_iff {c : γ} : c ∈ map₂ f a b ↔ ∃ a' b', a' ∈ a ∧ b' ∈ b ∧ f a' b' = c := by
/- Using the definition of `Option.map₂` and the property that `Option.bind` is equivalent to `some` when the optional value is `some`, we can simplify the proposition we want to show. The goal is to prove that \( c \in \text{Option.map₂} \, f \, a \, b \) if and only if there exist \( a' \) and \( b' \) such that \( a' \in a \), \( b' \in b \), and \( f(a', b') = c \). -/
  simp [map₂, bind_eq_some]","import Mathlib.Logic.Function.Defs
import Batteries.Tactic.Init
import Mathlib.Data.Option.NAry

open Option
open Function
variable {α β γ δ : Type*} {f : α → β → γ} {a : Option α} {b : Option β} {c : Option γ}
"
1056,pow_two_semiclosed,"theorem pow_two_semiclosed 
    {a : α} (ha : 1 ≤ a ^ 2) : 1 ≤ a := by sorry","Square of Element Greater Than or Equal to One Implies Element Greater Than or Equal to One: \( 1 \leq a^2 \to 1 \leq a \) : For any lattice-ordered group \( \alpha \) and any element \( a \in \alpha \), if \( 1 \leq a^2 \), then \( 1 \leq a \).","theorem pow_two_semiclosed 
    {a : α} (ha : 1 ≤ a ^ 2) : 1 ≤ a := by
  suffices this : (a ⊓ 1) * (a ⊓ 1) = a ⊓ 1 by
    rwa [← inf_eq_right, ← mul_right_eq_self]
  rw [mul_inf, inf_mul, ← pow_two, mul_one, one_mul, inf_assoc, inf_left_idem, inf_comm,
    inf_assoc, inf_of_le_left ha]","To prove \( 1 \leq a \), it suffices to show that \( (a \sqcap 1) \cdot (a \sqcap 1) = a \sqcap 1 \). If this is true, then by the properties of meet and multiplication, we can conclude \( 1 \leq a \).

We simplify the expression \( (a \sqcap 1) \cdot (a \sqcap 1) \) step by step using the following properties:
1. Distributivity of meet over multiplication: \( c \cdot (a \sqcap b) = (c \cdot a) \sqcap (c \cdot b) \),
2. Distributivity of meet over multiplication: \( (a \sqcap b) \cdot c = (a \cdot c) \sqcap (b \cdot c) \),
3. Definition of square in a monoid: \( a^2 = a \cdot a \),
4. Multiplicative identity: \( a \cdot 1 = a \),
5. Multiplicative identity: \( 1 \cdot a = a \),
6. Associativity of meet: \( (a \sqcap b) \sqcap c = a \sqcap (b \sqcap c) \),
7. Left idempotence of meet: \( a \sqcap (a \sqcap b) = a \sqcap b \),
8. Commutativity of meet: \( a \sqcap b = b \sqcap a \),
9. Associativity of meet: \( (a \sqcap b) \sqcap c = a \sqcap (b \sqcap c) \),
10. Meet identity: \( 1 \leq a^2 \) implies \( 1 \sqcap a^2 = 1 \).

Thus, we have:
\[
(a \sqcap 1) \cdot (a \sqcap 1) = (a \cdot a) \sqcap (1 \cdot a) \sqcap (a \sqcap 1) = a^2 \sqcap a \sqcap (a \sqcap 1) = a \sqcap (1 \sqcap a^2) = a \sqcap 1
\]
This completes the proof of the new goal, and hence \( 1 \leq a \). Therefore, the theorem is proved. \(\blacksquare\)","theorem pow_two_semiclosed 
    {a : α} (ha : 1 ≤ a ^ 2) : 1 ≤ a := by
/- To prove \( 1 \leq a \), it suffices to show that \( (a \sqcap 1) \cdot (a \sqcap 1) = a \sqcap 1 \). If this is true, then by the properties of meet and multiplication, we can conclude \( 1 \leq a \). -/
  suffices this : (a ⊓ 1) * (a ⊓ 1) = a ⊓ 1 by
    rwa [← inf_eq_right, ← mul_right_eq_self]
/- We simplify the expression \( (a \sqcap 1) \cdot (a \sqcap 1) \) step by step using the following properties:
1. \( c \cdot (a \sqcap b) = (c \cdot a) \sqcap (c \cdot b) \) (distributivity of meet over multiplication),
2. \( (a \sqcap b) \cdot c = (a \cdot c) \sqcap (b \cdot c) \) (distributivity of meet over multiplication),
3. \( a^2 = a \cdot a \) (definition of square in a monoid),
4. \( a \cdot 1 = a \) (multiplicative identity),
5. \( 1 \cdot a = a \) (multiplicative identity),
6. \( (a \sqcap b) \sqcap c = a \sqcap (b \sqcap c) \) (associativity of meet),
7. \( a \sqcap (a \sqcap b) = a \sqcap b \) (left idempotence of meet),
8. \( a \sqcap b = b \sqcap a \) (commutativity of meet),
9. \( (a \sqcap b) \sqcap c = a \sqcap (b \sqcap c) \) (associativity of meet),
10. \( 1 \leq a^2 \) implies \( 1 \sqcap a^2 = 1 \) (meet identity).

Thus, we have:
\[
(a \sqcap 1) \cdot (a \sqcap 1) = (a \cdot a) \sqcap (1 \cdot a) \sqcap (a \sqcap 1) = a^2 \sqcap a \sqcap (a \sqcap 1) = a \sqcap (1 \sqcap a^2) = a \sqcap 1
\]
This completes the proof of the new goal. -/
  rw [mul_inf, inf_mul, ← pow_two, mul_one, one_mul, inf_assoc, inf_left_idem, inf_comm,
    inf_assoc, inf_of_le_left ha]","import Mathlib.Algebra.Order.Group.OrderIso
import Mathlib.Algebra.Order.Group.Lattice


open Function
variable {α β : Type*}
variable [Lattice α] [Group α]
variable [CovariantClass α α (· * ·) (· ≤ ·)] [CovariantClass α α (swap (· * ·)) (· ≤ ·)]
"
1059,Option.map₂_swap,"theorem Option.map₂_swap (f : α → β → γ) (a : Option α) (b : Option β) :
    map₂ f a b = map₂ (fun a b => f b a) b a := by sorry","Swapping Arguments in Binary Map of Optional Values : For any types \( \alpha \), \( \beta \), and \( \gamma \), and for any binary function \( f : \alpha \to \beta \to \gamma \), and for any optional values \( a : \text{Option} \, \alpha \) and \( b : \text{Option} \, \beta \), the following holds:
\[
\text{Option.map₂} \, f \, a \, b = \text{Option.map₂} \, (fun \, a \, b \, => \, f \, b \, a) \, b \, a
\]
In other words, swapping the arguments of the binary function \( f \) and the optional values \( a \) and \( b \) in the `Option.map₂` operation results in the same output.","theorem Option.map₂_swap (f : α → β → γ) (a : Option α) (b : Option β) :
    map₂ f a b = map₂ (fun a b => f b a) b a := by
  cases a <;> cases b <;> rfl","To prove the theorem, we will consider every possible case for the optional values \( a \) and \( b \).

1. **Case 1: \( a = \text{none} \) and \( b = \text{none} \)**
\[
\text{Option.map₂} \, f \, \text{none} \, \text{none} = \text{Option.map₂} \, (fun \, a \, b \, => \, f \, b \, a) \, \text{none} \, \text{none}
\]
The equality holds trivially due to the reflexive property.

2. **Case 2: \( a = \text{none} \) and \( b = \text{some} \, val \)**
\[
\text{Option.map₂} \, f \, \text{none} \, (\text{some} \, val) = \text{Option.map₂} \, (fun \, a \, b \, => \, f \, b \, a) \, (\text{some} \, val) \, \text{none}
\]
The equality holds trivially due to the reflexive property.

3. **Case 3: \( a = \text{some} \, val \) and \( b = \text{none} \)**
\[
\text{Option.map₂} \, f \, (\text{some} \, val) \, \text{none} = \text{Option.map₂} \, (fun \, a \, b \, => \, f \, b \, a) \, \text{none} \, (\text{some} \, val)
\]
The equality holds trivially due to the reflexive property.

4. **Case 4: \( a = \text{some} \, val_1 \) and \( b = \text{some} \, val_2 \)**
\[
\text{Option.map₂} \, f \, (\text{some} \, val_1) \, (\text{some} \, val_2) = \text{Option.map₂} \, (fun \, a \, b \, => \, f \, b \, a) \, (\text{some} \, val_2) \, (\text{some} \, val_1)
\]
The equality holds trivially due to the reflexive property.

Since all cases have been considered and the equality holds in each case, the theorem is proved.","theorem Option.map₂_swap (f : α → β → γ) (a : Option α) (b : Option β) :
    map₂ f a b = map₂ (fun a b => f b a) b a := by
  cases a <;> cases b <;> rfl","import Mathlib.Logic.Function.Defs
import Batteries.Tactic.Init
import Mathlib.Data.Option.NAry

open Option
open Function
variable {α β γ δ : Type*} {f : α → β → γ} {a : Option α} {b : Option β} {c : Option γ}
"
1061,Option.map₂_map_right,"theorem Option.map₂_map_right (f : α → γ → δ) (g : β → γ) :
    map₂ f a (b.map g) = map₂ (fun a b => f a (g b)) a b := by sorry","Right Mapping in Binary Option Map Preserves Equality : For any types \( \alpha, \beta, \gamma, \delta \) and any optional values \( a : \text{Option} \, \alpha \) and \( b : \text{Option} \, \beta \), if \( f : \alpha \to \gamma \to \delta \) and \( g : \beta \to \gamma \) are functions, then the following equality holds:
\[
\text{Option.map₂ } f \, a \, (\text{Option.map } g \, b) = \text{Option.map₂ } (\lambda a \, b, f \, a \, (g \, b)) \, a \, b
\]
This theorem states that applying the binary map `Option.map₂` to \( a \) and the result of mapping \( g \) over \( b \) is the same as applying `Option.map₂` to \( a \) and \( b \) with the function \( f \) modified to first apply \( g \) to the second argument.","theorem Option.map₂_map_right (f : α → γ → δ) (g : β → γ) :
    map₂ f a (b.map g) = map₂ (fun a b => f a (g b)) a b := by
  cases b <;> rfl","To prove the theorem, we will consider every possible case of \( b \).

1. **Case 1: \( b = \text{none} \)**
- If \( b \) is `none`, then the left-hand side of the equation becomes:
\[
\text{Option.map₂ } f \, a \, (\text{Option.map } g \, \text{none})
\]
Since `Option.map g none` is `none`, the left-hand side simplifies to:
\[
\text{Option.map₂ } f \, a \, \text{none}
\]
The right-hand side of the equation becomes:
\[
\text{Option.map₂ } (\lambda a \, b, f \, a \, (g \, b)) \, a \, \text{none}
\]
Since both sides are `none` when \( b \) is `none`, the equality holds trivially due to the reflexive property.

2. **Case 2: \( b = \text{some } val \)**
- If \( b \) is `some val`, then the left-hand side of the equation becomes:
\[
\text{Option.map₂ } f \, a \, (\text{Option.map } g \, (\text{some } val))
\]
Since `Option.map g (some val)` is `some (g val)`, the left-hand side simplifies to:
\[
\text{Option.map₂ } f \, a \, (\text{some } (g \, val))
\]
The right-hand side of the equation becomes:
\[
\text{Option.map₂ } (\lambda a \, b, f \, a \, (g \, b)) \, a \, (\text{some } val)
\]
Since both sides are `some (f a (g val))` when \( b \) is `some val`, the equality holds trivially due to the reflexive property.

In both cases, the equality holds, thus completing the proof.","theorem Option.map₂_map_right (f : α → γ → δ) (g : β → γ) :
    map₂ f a (b.map g) = map₂ (fun a b => f a (g b)) a b := by
  cases b <;> rfl","import Mathlib.Logic.Function.Defs
import Batteries.Tactic.Init
import Mathlib.Data.Option.NAry

open Option
open Function
variable {α β γ δ : Type*} {f : α → β → γ} {a : Option α} {b : Option β} {c : Option γ}
"
1728,Minimal.or,theorem Minimal.or (h : Minimal (fun x ↦ P x ∨ Q x) x) : Minimal P x ∨ Minimal Q x := by sorry,"Minimal Element with Respect to Disjunction of Predicates : For any ordered type \( \alpha \) and any predicates \( P \) and \( Q \) on \( \alpha \), if \( x \) is a minimal element satisfying \( P(x) \lor Q(x) \), then \( x \) is either a minimal element satisfying \( P(x) \) or a minimal element satisfying \( Q(x) \). Formally, if \( \text{Minimal}(P \lor Q, x) \), then \( \text{Minimal}(P, x) \lor \text{Minimal}(Q, x) \).","theorem Minimal.or (h : Minimal (fun x ↦ P x ∨ Q x) x) : Minimal P x ∨ Minimal Q x := by
  obtain ⟨h | h, hmin⟩ := h
  · exact .inl ⟨h, fun y hy hyx ↦ hmin (Or.inl hy) hyx⟩
  exact .inr ⟨h, fun y hy hyx ↦ hmin (Or.inr hy) hyx⟩","We start by assuming that \( x \) is a minimal element satisfying \( P(x) \lor Q(x) \). This means that for any \( y \in \alpha \) such that \( P(y) \lor Q(y) \) and \( y \leq x \), we have \( x \leq y \).

We need to show that \( x \) is either a minimal element satisfying \( P(x) \) or a minimal element satisfying \( Q(x) \). We will handle these cases separately.

1. **Case 1: \( P(x) \) is true.**
- We need to show that \( x \) is a minimal element satisfying \( P(x) \).
- For any \( y \in \alpha \) such that \( P(y) \) and \( y \leq x \), we have \( P(y) \lor Q(y) \) (since \( P(y) \) is true).
- By the minimality of \( x \) for \( P(x) \lor Q(x) \), we have \( x \leq y \).
- Therefore, \( x \) is a minimal element satisfying \( P(x) \).

2. **Case 2: \( Q(x) \) is true.**
- We need to show that \( x \) is a minimal element satisfying \( Q(x) \).
- For any \( y \in \alpha \) such that \( Q(y) \) and \( y \leq x \), we have \( P(y) \lor Q(y) \) (since \( Q(y) \) is true).
- By the minimality of \( x \) for \( P(x) \lor Q(x) \), we have \( x \leq y \).
- Therefore, \( x \) is a minimal element satisfying \( Q(x) \).

Since \( x \) is either a minimal element satisfying \( P(x) \) or a minimal element satisfying \( Q(x) \), we conclude that \( \text{Minimal}(P, x) \lor \text{Minimal}(Q, x) \). This completes the proof.","theorem Minimal.or (h : Minimal (fun x ↦ P x ∨ Q x) x) : Minimal P x ∨ Minimal Q x := by
/- We assume that \( x \) is a minimal element satisfying \( P(x) \lor Q(x) \). This means we have two cases to consider: either \( P(x) \) is true or \( Q(x) \) is true. We will handle these cases separately. -/
  obtain ⟨h | h, hmin⟩ := h
/- First, we consider the case where \( P(x) \) is true. We need to show that \( x \) is a minimal element satisfying \( P(x) \). To do this, we use the fact that \( x \) is minimal for \( P(x) \lor Q(x) \). For any \( y \in \alpha \) such that \( P(y) \) and \( y \leq x \), we have \( x \leq y \) by the minimality of \( x \) for \( P(x) \lor Q(x) \). Therefore, \( x \) is a minimal element satisfying \( P(x) \). -/
  · exact .inl ⟨h, fun y hy hyx ↦ hmin (Or.inl hy) hyx⟩
/- Next, we consider the case where \( Q(x) \) is true. We need to show that \( x \) is a minimal element satisfying \( Q(x) \). To do this, we use the fact that \( x \) is minimal for \( P(x) \lor Q(x) \). For any \( y \in \alpha \) such that \( Q(y) \) and \( y \leq x \), we have \( x \leq y \) by the minimality of \( x \) for \( P(x) \lor Q(x) \). Therefore, \( x \) is a minimal element satisfying \( Q(x) \). -/
  exact .inr ⟨h, fun y hy hyx ↦ hmin (Or.inr hy) hyx⟩","import Mathlib.Order.Antichain
import Mathlib.Order.UpperLower.Basic
import Mathlib.Order.Interval.Set.Basic
import Mathlib.Order.Minimal

open Minimal
open Set OrderDual
variable {α : Type*} {P Q : α → Prop} {a x y : α}
variable [LE α]
"
2513,IsPrimal.mul,"theorem IsPrimal.mul {α} [CancelCommMonoidWithZero α] {m n : α}
    (hm : IsPrimal m) (hn : IsPrimal n) : IsPrimal (m * n) := by sorry","Product of Primal Elements is Primal in Cancel Commutative Monoids with Zero : For any elements \( m \) and \( n \) in a type \( \alpha \) that is a cancel commutative monoid with zero, if \( m \) and \( n \) are both primal, then their product \( m \cdot n \) is also primal. Formally, if \( m \) and \( n \) are primal elements in \( \alpha \), then for any \( b, c \in \alpha \), if \( m \cdot n \mid b \cdot c \), there exist \( a_1, a_2 \in \alpha \) such that \( a_1 \mid b \), \( a_2 \mid c \), and \( m \cdot n = a_1 \cdot a_2 \).","theorem IsPrimal.mul {α} [CancelCommMonoidWithZero α] {m n : α}
    (hm : IsPrimal m) (hn : IsPrimal n) : IsPrimal (m * n) := by
  obtain rfl | h0 := eq_or_ne m 0; · rwa [zero_mul]
  intro b c h
  obtain ⟨a₁, a₂, ⟨b, rfl⟩, ⟨c, rfl⟩, rfl⟩ := hm (dvd_of_mul_right_dvd h)
  rw [mul_mul_mul_comm, mul_dvd_mul_iff_left h0] at h
  obtain ⟨a₁', a₂', h₁, h₂, rfl⟩ := hn h
  exact ⟨a₁ * a₁', a₂ * a₂', mul_dvd_mul_left _ h₁, mul_dvd_mul_left _ h₂, mul_mul_mul_comm _ _ _ _⟩","We start by considering two cases: either \( m = 0 \) or \( m \neq 0 \).

1. **Case 1: \( m = 0 \)**
- If \( m = 0 \), then \( m \cdot n = 0 \cdot n = 0 \).
- Since \( 0 \) is primal, the product \( m \cdot n \) is primal. This case is trivial and we move to the next case.

2. **Case 2: \( m \neq 0 \)**
- Let \( b \) and \( c \) be arbitrary elements in \( \alpha \), and assume \( m \cdot n \mid b \cdot c \).
- Since \( m \) is primal, there exist elements \( a_1 \) and \( a_2 \) in \( \alpha \) such that \( a_1 \mid b \), \( a_2 \mid c \), and \( m \cdot n = a_1 \cdot a_2 \). This follows from the property of primal elements that if \( m \mid b \cdot c \), then there exist \( a_1 \) and \( a_2 \) such that \( a_1 \mid b \), \( a_2 \mid c \), and \( m = a_1 \cdot a_2 \).
- Using the commutativity of triple multiplication, we can rewrite \( m \cdot n \mid b \cdot c \) as \( n \mid b \cdot c \).
- Since \( m \neq 0 \), we can use the property that left multiplication preserves divisibility to conclude that \( n \mid b \cdot c \).
- Since \( n \) is primal and \( n \mid b \cdot c \), there exist elements \( a_1' \) and \( a_2' \) in \( \alpha \) such that \( a_1' \mid b \), \( a_2' \mid c \), and \( n = a_1' \cdot a_2' \). This follows from the property of primal elements that if \( n \mid b \cdot c \), then there exist \( a_1' \) and \( a_2' \) such that \( a_1' \mid b \), \( a_2' \mid c \), and \( n = a_1' \cdot a_2' \).
- We now show that \( m \cdot n \) is primal by constructing the elements \( a_1 \cdot a_1' \) and \( a_2 \cdot a_2' \).
- Since \( a_1' \mid b \) and \( a_2' \mid c \), it follows that \( a_1 \cdot a_1' \mid a_1 \cdot b \) and \( a_2 \cdot a_2' \mid a_2 \cdot c \).
- Moreover, we have \( m \cdot n = (a_1 \cdot a_1') \cdot (a_2 \cdot a_2') \) by the commutativity of triple multiplication.
- Therefore, \( m \cdot n \) is primal.

This completes the proof.","theorem IsPrimal.mul {α} [CancelCommMonoidWithZero α] {m n : α}
    (hm : IsPrimal m) (hn : IsPrimal n) : IsPrimal (m * n) := by
/- We consider two cases: either \( m = 0 \) or \( m \neq 0 \). If \( m = 0 \), then \( m \cdot n = 0 \cdot n = 0 \). Since \( 0 \) is primal, the product \( m \cdot n \) is primal. This case is trivial and we move to the next case where \( m \neq 0 \). -/
  obtain rfl | h0 := eq_or_ne m 0; · rwa [zero_mul]
/- Let \( b \) and \( c \) be arbitrary elements in \( \alpha \), and assume \( m \cdot n \mid b \cdot c \). We need to show that there exist elements \( a_1 \) and \( a_2 \) in \( \alpha \) such that \( a_1 \mid b \), \( a_2 \mid c \), and \( m \cdot n = a_1 \cdot a_2 \). -/
  intro b c h
/- Since \( m \) is primal, and \( m \cdot n \mid b \cdot c \), we can find elements \( a_1 \) and \( a_2 \) in \( \alpha \) such that \( a_1 \mid b \), \( a_2 \mid c \), and \( m \cdot n = a_1 \cdot a_2 \). This follows from the property of primal elements that if \( m \mid b \cdot c \), then there exist \( a_1 \) and \( a_2 \) such that \( a_1 \mid b \), \( a_2 \mid c \), and \( m = a_1 \cdot a_2 \). -/
  obtain ⟨a₁, a₂, ⟨b, rfl⟩, ⟨c, rfl⟩, rfl⟩ := hm (dvd_of_mul_right_dvd h)
/- Using the commutativity of triple multiplication, we can rewrite \( m \cdot n \mid b \cdot c \) as \( n \mid b \cdot c \). Since \( m \neq 0 \), we can use the property that left multiplication preserves divisibility to conclude that \( n \mid b \cdot c \). -/
  rw [mul_mul_mul_comm, mul_dvd_mul_iff_left h0] at h
/- Since \( n \) is primal and \( n \mid b \cdot c \), we can find elements \( a_1' \) and \( a_2' \) in \( \alpha \) such that \( a_1' \mid b \), \( a_2' \mid c \), and \( n = a_1' \cdot a_2' \). This follows from the property of primal elements that if \( n \mid b \cdot c \), then there exist \( a_1' \) and \( a_2' \) such that \( a_1' \mid b \), \( a_2' \mid c \), and \( n = a_1' \cdot a_2' \). -/
  obtain ⟨a₁', a₂', h₁, h₂, rfl⟩ := hn h
/- We now show that \( m \cdot n \) is primal by constructing the elements \( a_1 \cdot a_1' \) and \( a_2 \cdot a_2' \). Since \( a_1' \mid b \) and \( a_2' \mid c \), it follows that \( a_1 \cdot a_1' \mid a_1 \cdot b \) and \( a_2 \cdot a_2' \mid a_2 \cdot c \). Moreover, we have \( m \cdot n = (a_1 \cdot a_1') \cdot (a_2 \cdot a_2') \) by the commutativity of triple multiplication. This completes the proof. -/
  exact ⟨a₁ * a₁', a₂ * a₂', mul_dvd_mul_left _ h₁, mul_dvd_mul_left _ h₂, mul_mul_mul_comm _ _ _ _⟩","import Mathlib.Algebra.GroupWithZero.Units.Basic
import Mathlib.Algebra.Divisibility.Units
import Mathlib.Algebra.GroupWithZero.Divisibility

open IsPrimal
variable {α : Type*}
variable [SemigroupWithZero α] {a : α}
variable [CommMonoidWithZero α]
variable {x y : α}
variable [MonoidWithZero α]
"
3602,Cardinal.continuum_lt_lift,theorem Cardinal.continuum_lt_lift {c : Cardinal.{u}} : 𝔠 < lift.{v} c ↔ 𝔠 < c := by sorry,"Continuum is Strictly Less Than Lifted Cardinal if and Only if Continuum is Strictly Less Than Original Cardinal: \( \mathfrak{c} < \text{Cardinal.lift} \, c \leftrightarrow \mathfrak{c} < c \) : For any cardinal number \( c \) in a universe \( u \), the cardinality of the continuum \( \mathfrak{c} \) is strictly less than the lifted cardinal number \( \text{Cardinal.lift} \, c \) in the universe \( \max(v, u) \) if and only if \( \mathfrak{c} \) is strictly less than \( c \). Formally, this is expressed as:
\[ \mathfrak{c} < \text{Cardinal.lift} \, c \leftrightarrow \mathfrak{c} < c \]","theorem Cardinal.continuum_lt_lift {c : Cardinal.{u}} : 𝔠 < lift.{v} c ↔ 𝔠 < c := by
  rw [← lift_continuum.{v, u}, lift_lt]","We start by using the property that the lift of the continuum cardinality is equal to the continuum itself, i.e., \( \mathfrak{c}^{\max(v, u)} = \mathfrak{c} \). This allows us to rewrite the left-hand side of the goal as \( \mathfrak{c} < \text{Cardinal.lift} \, c \). Next, we apply the theorem that lifting preserves strict inequality of cardinal numbers, i.e., \( \text{Cardinal.lift} \, a < \text{Cardinal.lift} \, b \leftrightarrow a < b \). By applying this theorem, we transform the goal into \( \mathfrak{c} < c \leftrightarrow \mathfrak{c} < c \), which is trivially true. Therefore, the original statement holds.","theorem Cardinal.continuum_lt_lift {c : Cardinal.{u}} : 𝔠 < lift.{v} c ↔ 𝔠 < c := by
/- First, we use the fact that the lift of the continuum cardinality is equal to the continuum itself, i.e., \( \mathfrak{c}^{\max(v, u)} = \mathfrak{c} \), to rewrite the left-hand side of the goal. Then, we use the property that lifting preserves strict inequality of cardinal numbers, i.e., \( \text{Cardinal.lift} \, a < \text{Cardinal.lift} \, b \leftrightarrow a < b \), to transform the goal into a form that is trivially true. -/
  rw [← lift_continuum.{v, u}, lift_lt]","import Mathlib.SetTheory.Cardinal.Ordinal
import Mathlib.SetTheory.Cardinal.Continuum

open Cardinal
open Cardinal
"
3616,Cardinal.lift_continuum,theorem Cardinal.lift_continuum : lift.{v} 𝔠 = 𝔠 := by sorry,"Lift of Continuum Cardinality is Equal to Continuum: \( \mathfrak{c}^{\max(v, u_1)} = \mathfrak{c} \) : The cardinality of the continuum, denoted by \( \mathfrak{c} \), remains unchanged when lifted to a higher universe. Specifically, for any universe \( v \) and \( u_1 \), the lift of \( \mathfrak{c} \) from universe \( v \) to universe \( \max(v, u_1) \) is equal to \( \mathfrak{c} \) itself, i.e., \( \mathfrak{c}^{\max(v, u_1)} = \mathfrak{c} \).","theorem Cardinal.lift_continuum : lift.{v} 𝔠 = 𝔠 := by
  rw [← two_power_aleph0, lift_two_power, lift_aleph0, two_power_aleph0]","We begin by expressing the continuum cardinality \( \mathfrak{c} \) as \( 2^{\aleph_0} \). Using the property that lifting the exponentiation of 2 to any cardinal \( a \) results in the exponentiation of 2 to the lifted cardinal \( a \), we have:

\[
\text{Cardinal.lift}(2^{\aleph_0}) = 2^{\text{Cardinal.lift}(\aleph_0)}
\]

Next, we use the fact that lifting \( \aleph_0 \) to any universe level results in \( \aleph_0 \) itself:

\[
\text{Cardinal.lift}(\aleph_0) = \aleph_0
\]

Thus, we get:

\[
2^{\text{Cardinal.lift}(\aleph_0)} = 2^{\aleph_0}
\]

Finally, we recognize that \( 2^{\aleph_0} \) is equal to \( \mathfrak{c} \):

\[
2^{\aleph_0} = \mathfrak{c}
\]

Therefore, we have:

\[
\text{Cardinal.lift}(\mathfrak{c}) = \mathfrak{c}
\]

This completes the proof that the lift of the continuum cardinality to any higher universe is equal to the continuum cardinality itself.","theorem Cardinal.lift_continuum : lift.{v} 𝔠 = 𝔠 := by
/- We start by expressing the continuum cardinality \( \mathfrak{c} \) as \( 2^{\aleph_0} \). Then, we use the property that lifting the exponentiation of 2 to any cardinal \( a \) results in the exponentiation of 2 to the lifted cardinal \( a \). Next, we use the fact that lifting \( \aleph_0 \) to any universe level results in \( \aleph_0 \) itself. Finally, we recognize that \( 2^{\aleph_0} \) is equal to \( \mathfrak{c} \). -/
  rw [← two_power_aleph0, lift_two_power, lift_aleph0, two_power_aleph0]","import Mathlib.SetTheory.Cardinal.Ordinal
import Mathlib.SetTheory.Cardinal.Continuum

open Cardinal
open Cardinal
"
5646,IsIntegral.map,"theorem IsIntegral.map {B C F : Type*} [Ring B] [Ring C] [Algebra R B] [Algebra A B] [Algebra R C]
    [IsScalarTower R A B] [Algebra A C] [IsScalarTower R A C] {b : B}
    [FunLike F B C] [AlgHomClass F A B C] (f : F)
    (hb : IsIntegral R b) : IsIntegral R (f b) := by sorry","Integral Element Under Algebra Homomorphism: \( \text{IsIntegral}(R, b) \to \text{IsIntegral}(R, f(b)) \) : For any commutative rings \( R \) and \( A \), and any rings \( B \) and \( C \) with algebra structures over \( R \) and \( A \), if \( b \) is an integral element in \( B \) over \( R \) and \( f : A \to B \to C \) is an algebra homomorphism, then \( f(b) \) is also an integral element in \( C \) over \( R \).","theorem IsIntegral.map {B C F : Type*} [Ring B] [Ring C] [Algebra R B] [Algebra A B] [Algebra R C]
    [IsScalarTower R A B] [Algebra A C] [IsScalarTower R A C] {b : B}
    [FunLike F B C] [AlgHomClass F A B C] (f : F)
    (hb : IsIntegral R b) : IsIntegral R (f b) := by
  obtain ⟨P, hP⟩ := hb
  refine ⟨P, hP.1, ?_⟩
  rw [← aeval_def, ← aeval_map_algebraMap A,
    aeval_algHom_apply, aeval_map_algebraMap, aeval_def, hP.2, _root_.map_zero]","Let \( b \) be an integral element in \( B \) over \( R \). By definition, there exists a monic polynomial \( P \) in \( R[X] \) such that \( P(b) = 0 \).

To show that \( f(b) \) is integral over \( R \), we need to find a monic polynomial \( P \) in \( R[X] \) such that \( P(f(b)) = 0 \). We already have \( P \) as a monic polynomial from the assumption.

We start by rewriting the goal using the definition of the algebra evaluation map \( \text{aeval} \). Then, we use the fact that the polynomial \( P \) can be mapped through the algebra map from \( R \) to \( A \) without changing its evaluation. Next, we apply the property of the algebra homomorphism \( f \) that \( \text{aeval}(f(b), P) = f(\text{aeval}(b, P)) \). Since \( P(b) = 0 \) by the assumption, we have \( \text{aeval}(b, P) = 0 \). Finally, since \( f \) is a homomorphism, \( f(0) = 0 \), thus completing the proof.

Therefore, \( f(b) \) is integral over \( R \).","theorem IsIntegral.map {B C F : Type*} [Ring B] [Ring C] [Algebra R B] [Algebra A B] [Algebra R C]
    [IsScalarTower R A B] [Algebra A C] [IsScalarTower R A C] {b : B}
    [FunLike F B C] [AlgHomClass F A B C] (f : F)
    (hb : IsIntegral R b) : IsIntegral R (f b) := by
/- Let \( P \) be a monic polynomial in \( R[X] \) such that \( P(b) = 0 \), as given by the assumption that \( b \) is integral over \( R \). -/
  obtain ⟨P, hP⟩ := hb
/- To show that \( f(b) \) is integral over \( R \), it suffices to find a monic polynomial \( P \) in \( R[X] \) such that \( P(f(b)) = 0 \). We already have \( P \) as a monic polynomial from the previous step, so we need to show that \( P(f(b)) = 0 \). -/
  refine ⟨P, hP.1, ?_⟩
/- We start by rewriting the goal using the definition of the algebra evaluation map \( \text{aeval} \). Then, we use the fact that the polynomial \( P \) can be mapped through the algebra map from \( R \) to \( A \) without changing its evaluation. Next, we apply the property of the algebra homomorphism \( f \) that \( \text{aeval}(f(b), P) = f(\text{aeval}(b, P)) \). Since \( P(b) = 0 \) by \( hP.2 \), we have \( \text{aeval}(b, P) = 0 \). Finally, since \( f \) is a homomorphism, \( f(0) = 0 \), thus completing the proof. -/
  rw [← aeval_def, ← aeval_map_algebraMap A,
    aeval_algHom_apply, aeval_map_algebraMap, aeval_def, hP.2, _root_.map_zero]","import Mathlib.RingTheory.IntegralClosure.IsIntegral.Defs
import Mathlib.Algebra.Polynomial.Expand
import Mathlib.RingTheory.Polynomial.Tower
import Mathlib.RingTheory.IntegralClosure.IsIntegral.Basic

open IsIntegral
open Polynomial Submodule
variable {R S A : Type*}
variable [CommRing R] [Ring A] [Ring S] (f : R →+* S)
variable [Algebra R A]
variable [CommRing R] [Ring A] [Ring S] (f : R →+* S)
variable [Algebra R A]
variable [Algebra R A]
variable {R A B S : Type*}
variable [CommRing R] [CommRing A] [Ring B] [CommRing S]
variable [Algebra R A] (f : R →+* S)
variable [CommRing R] [CommRing A] [Ring B] [CommRing S]
variable [Algebra R A] (f : R →+* S)
variable [Algebra R A] (f : R →+* S)
"
5755,Finpartition.exists_equipartition_card_eq,"theorem Finpartition.exists_equipartition_card_eq (hn : n ≠ 0) (hs : n ≤ s.card) :
    ∃ P : Finpartition s, P.IsEquipartition ∧ P.parts.card = n := by sorry","Existence of Equipartitions with Given Number of Parts : For any finite set \( s \) of type \( \alpha \) with decidable equality, and for any natural number \( n \) such that \( n \neq 0 \) and \( n \leq |s| \), there exists a finite partition \( P \) of \( s \) such that \( P \) is an equipartition and the number of parts in \( P \) is exactly \( n \).","theorem Finpartition.exists_equipartition_card_eq (hn : n ≠ 0) (hs : n ≤ s.card) :
    ∃ P : Finpartition s, P.IsEquipartition ∧ P.parts.card = n := by
  rw [← pos_iff_ne_zero] at hn
  have : (n - s.card % n) * (s.card / n) + s.card % n * (s.card / n + 1) = s.card := by
    rw [tsub_mul, mul_add, ← add_assoc,
      tsub_add_cancel_of_le (Nat.mul_le_mul_right _ (mod_lt _ hn).le), mul_one, add_comm,
      mod_add_div]
  refine
    ⟨(indiscrete (card_pos.1 <| hn.trans_le hs).ne_empty).equitabilise this,
      equitabilise_isEquipartition, ?_⟩
  rw [card_parts_equitabilise _ _ (Nat.div_pos hs hn).ne', tsub_add_cancel_of_le (mod_lt _ hn).le]","First, note that \( n \neq 0 \) is equivalent to \( 0 < n \). Given \( 0 < n \leq |s| \), we need to show the existence of a partition \( P \) of \( s \) such that \( P \) is an equipartition and \( |P| = n \).

We start by constructing a partition \( P \) by equitabilising the indiscrete partition of \( s \). This partition \( P \) is an equipartition by construction. To show that \( |P| = n \), we use the fact that the number of parts in the equitabilised partition is given by \( n - |s| \% n + |s| \% n \). Using the identity for subtraction and addition cancellation, we conclude that the number of parts is \( n \).

Thus, we have shown the existence of a partition \( P \) of \( s \) such that \( P \) is an equipartition and \( |P| = n \).","theorem Finpartition.exists_equipartition_card_eq (hn : n ≠ 0) (hs : n ≤ s.card) :
    ∃ P : Finpartition s, P.IsEquipartition ∧ P.parts.card = n := by
/- Since \( n \neq 0 \) is equivalent to \( 0 < n \), we can replace \( n \neq 0 \) with \( 0 < n \) in our assumption. -/
  rw [← pos_iff_ne_zero] at hn
/- We need to show that the expression \((n - s.\text{card} \% n) \cdot (s.\text{card} / n) + s.\text{card} \% n \cdot (s.\text{card} / n + 1)\) is equal to \( s.\text{card} \). -/
  have : (n - s.card % n) * (s.card / n) + s.card % n * (s.card / n + 1) = s.card := by
/- Using the distributive property of multiplication over subtraction, the distributive property of multiplication over addition, associativity of addition, the fact that \( n \cdot (s.\text{card} / n) \geq s.\text{card} \% n \cdot (s.\text{card} / n) \) due to the modulo operation being less than \( n \), the multiplicative identity, commutativity of addition, and the identity for modulo and division, we can simplify the expression to \( s.\text{card} \). -/
    rw [tsub_mul, mul_add, ← add_assoc,
      tsub_add_cancel_of_le (Nat.mul_le_mul_right _ (mod_lt _ hn).le), mul_one, add_comm,
      mod_add_div]
/- We construct a partition \( P \) by equitabilising the indiscrete partition of \( s \) (which is nonempty since \( 0 < n \leq s.\text{card} \)). This partition \( P \) is an equipartition by construction. Now, we need to show that the number of parts in \( P \) is \( n \). -/
  refine
    ⟨(indiscrete (card_pos.1 <| hn.trans_le hs).ne_empty).equitabilise this,
      equitabilise_isEquipartition, ?_⟩
/- Using the fact that the number of parts in the equitabilised partition is \( n - s.\text{card} \% n + s.\text{card} \% n \) and the identity for subtraction and addition cancellation, we conclude that the number of parts is \( n \). -/
  rw [card_parts_equitabilise _ _ (Nat.div_pos hs hn).ne', tsub_add_cancel_of_le (mod_lt _ hn).le]","import Mathlib.Order.Partition.Equipartition
import Mathlib.Combinatorics.SimpleGraph.Regularity.Equitabilise

open Finpartition
open Finset Nat
variable {α : Type*} [DecidableEq α] {s t : Finset α} {m n a b : ℕ} {P : Finpartition s}
variable (h : a * m + b * (m + 1) = s.card)
variable {h}
variable (P h)
variable (s)
"
6144,Ordering.swap_eq_iff_eq_swap,theorem Ordering.swap_eq_iff_eq_swap {o o' : Ordering} : o.swap = o' ↔ o = o'.swap := by sorry,"Swapping Orderings is an Involution: \( o.\text{swap} = o' \leftrightarrow o = o'.\text{swap} \) : For any two orderings \( o \) and \( o' \) in the type `Ordering`, the swapped version of \( o \) is equal to \( o' \) if and only if \( o \) is equal to the swapped version of \( o' \). In other words, \( o.\text{swap} = o' \) if and only if \( o = o'.\text{swap} \).","theorem Ordering.swap_eq_iff_eq_swap {o o' : Ordering} : o.swap = o' ↔ o = o'.swap := by
  rw [← swap_inj, swap_swap]","We begin by leveraging the injectivity of the swap operation on ordering results, which states that \( o_1.swap = o_2.swap \) if and only if \( o_1 = o_2 \). This allows us to rewrite the goal \( o.swap = o' \) as \( o.swap.swap = o'.swap \). Next, we use the property that applying the swap operation twice results in the original ordering, i.e., \( o.swap.swap = o \). This simplifies our goal to \( o = o'.swap \), which is exactly what we need to show. Therefore, we have shown that \( o.\text{swap} = o' \) if and only if \( o = o'.\text{swap} \).","theorem Ordering.swap_eq_iff_eq_swap {o o' : Ordering} : o.swap = o' ↔ o = o'.swap := by
/- We start by using the injectivity of the swap operation on ordering results, which states that \( o_1.swap = o_2.swap \) if and only if \( o_1 = o_2 \). This allows us to rewrite the goal \( o.swap = o' \) as \( o.swap.swap = o'.swap \). Then, we use the property that applying the swap operation twice results in the original ordering, i.e., \( o.swap.swap = o \). This simplifies our goal to \( o = o'.swap \), which is exactly what we need to show. -/
  rw [← swap_inj, swap_swap]","import Mathlib.Data.Ordering.Basic
import Mathlib.Order.Synonym
import Mathlib.Order.Compare

open Ordering
variable {α β : Type*}
"
6151,toDual_compares_toDual,"theorem toDual_compares_toDual [LT α] {a b : α} {o : Ordering} :
    Compares o (toDual a) (toDual b) ↔ Compares o b a := by sorry","Comparison in Dual Order Type is Reversed : For any type \( \alpha \) equipped with a less-than relation \( < \), and for any elements \( a, b \in \alpha \) and any ordering \( o \), the predicate \( o \) compares \( \text{toDual}(a) \) and \( \text{toDual}(b) \) in the dual order type \( \alpha^\mathrm{od} \) if and only if \( o \) compares \( b \) and \( a \) in the original type \( \alpha \). Specifically:
- If \( o = \text{lt} \), then \( \text{toDual}(a) < \text{toDual}(b) \) if and only if \( b < a \).
- If \( o = \text{eq} \), then \( \text{toDual}(a) = \text{toDual}(b) \) if and only if \( b = a \).
- If \( o = \text{gt} \), then \( \text{toDual}(a) > \text{toDual}(b) \) if and only if \( b > a \).","theorem toDual_compares_toDual [LT α] {a b : α} {o : Ordering} :
    Compares o (toDual a) (toDual b) ↔ Compares o b a := by
  cases o
  exacts [Iff.rfl, eq_comm, Iff.rfl]","We will discuss every possible case of the ordering \( o \):

1. **Case \( o = \text{lt} \):**
The statement \( \text{lt.Compares} (\text{toDual}(a)) (\text{toDual}(b)) \leftrightarrow \text{lt.Compares} b a \) is trivially true by reflexivity.

2. **Case \( o = \text{eq} \):**
The statement \( \text{eq.Compares} (\text{toDual}(a)) (\text{toDual}(b)) \leftrightarrow \text{eq.Compares} b a \) is equivalent to \( a = b \leftrightarrow b = a \), which is true by the symmetry of equality.

3. **Case \( o = \text{gt} \):**
The statement \( \text{gt.Compares} (\text{toDual}(a)) (\text{toDual}(b)) \leftrightarrow \text{gt.Compares} b a \) is trivially true by reflexivity.

Thus, for each possible ordering \( o \), the comparison in the dual order type \( \alpha^\mathrm{od} \) is correctly reversed with respect to the original type \( \alpha \). This completes the proof.","theorem toDual_compares_toDual [LT α] {a b : α} {o : Ordering} :
    Compares o (toDual a) (toDual b) ↔ Compares o b a := by
/- We will discuss every possible case of the ordering \( o \). Case 1, \( o = \text{lt} \); Case 2, \( o = \text{eq} \); Case 3, \( o = \text{gt} \). -/
  cases o
/- The remaining propositions are proved by:
1. For \( o = \text{lt} \), the statement \( \text{lt.Compares} (\text{toDual}(a)) (\text{toDual}(b)) \leftrightarrow \text{lt.Compares} b a \) is trivially true by reflexivity.
2. For \( o = \text{eq} \), the statement \( \text{eq.Compares} (\text{toDual}(a)) (\text{toDual}(b)) \leftrightarrow \text{eq.Compares} b a \) is equivalent to \( a = b \leftrightarrow b = a \), which is true by the symmetry of equality.
3. For \( o = \text{gt} \), the statement \( \text{gt.Compares} (\text{toDual}(a)) (\text{toDual}(b)) \leftrightarrow \text{gt.Compares} b a \) is trivially true by reflexivity. -/
  exacts [Iff.rfl, eq_comm, Iff.rfl]","import Mathlib.Data.Ordering.Basic
import Mathlib.Order.Synonym
import Mathlib.Order.Compare


variable {α β : Type*}
open Ordering OrderDual
"
7662,Int.zero_shiftRight,theorem Int.zero_shiftRight (n : Nat) : (0 : Int) >>> n = 0 := by sorry,"Right Shift of Zero is Zero: \( 0 >>> n = 0 \) : For any natural number \( n \), the right shift of the integer \( 0 \) by \( n \) is equal to \( 0 \), i.e., \( 0 >>> n = 0 \).","theorem Int.zero_shiftRight (n : Nat) : (0 : Int) >>> n = 0 := by
  simp [Int.shiftRight_eq_div_pow]","To prove that \( 0 >>> n = 0 \) for any natural number \( n \), we use the property that the right shift of an integer \( m \) by \( n \) is equal to the integer division of \( m \) by \( 2^n \). Specifically, we have:
\[
0 >>> n = 0 / 2^n
\]
Since \( 0 \) divided by any positive integer is \( 0 \), it follows that:
\[
0 / 2^n = 0
\]
Thus, \( 0 >>> n = 0 \) for any natural number \( n \). This completes the proof.","theorem Int.zero_shiftRight (n : Nat) : (0 : Int) >>> n = 0 := by
/- Using the property that the right shift of an integer \( m \) by \( n \) is equal to the integer division of \( m \) by \( 2^n \), we can simplify the proposition \( 0 >>> n = 0 \) to \( 0 / 2^n = 0 \). Since \( 0 \) divided by any positive integer is \( 0 \), the proposition is trivially true. -/
  simp [Int.shiftRight_eq_div_pow]","import Init.Data.Nat.Bitwise.Lemmas
import Init.Data.Int.Bitwise
import Init.Data.Int.Bitwise.Lemmas

open Int

"
7809,Nat.dist_self,theorem Nat.dist_self (n : ℕ) : dist n n = 0 := by sorry,"Distance of a Natural Number to Itself is Zero: \( \text{dist}(n, n) = 0 \) : For any natural number \( n \), the distance between \( n \) and itself is zero, i.e., \( \text{dist}(n, n) = 0 \).","theorem Nat.dist_self (n : ℕ) : dist n n = 0 := by
  simp [dist, tsub_self]","To prove that the distance between a natural number \( n \) and itself is zero, we start by using the definition of the distance function \( \text{dist}(a, b) \). The distance function is defined in terms of truncated subtraction, which is the subtraction operation in a canonically ordered commutative monoid. Specifically, \( \text{dist}(a, b) = a - b \).

We need to show that \( \text{dist}(n, n) = 0 \). By the definition of the distance function, this is equivalent to showing that \( n - n = 0 \).

Using the property of subtraction in a canonically ordered commutative monoid, we know that for any element \( a \), \( a - a = 0 \). Therefore, \( n - n = 0 \).

Thus, \( \text{dist}(n, n) = 0 \), which completes the proof.","theorem Nat.dist_self (n : ℕ) : dist n n = 0 := by
  simp [dist, tsub_self]","import Mathlib.Algebra.Order.Ring.Nat
import Mathlib.Data.Nat.Dist

open Nat

"
7818,Nat.dist_tri_right',theorem Nat.dist_tri_right' (n m : ℕ) : n ≤ m + dist n m := by sorry,"Right Triangle Inequality for Distance in Natural Numbers : For any natural numbers \( n \) and \( m \), it holds that \( n \leq m + \text{dist}(n, m) \).","theorem Nat.dist_tri_right' (n m : ℕ) : n ≤ m + dist n m := by
  rw [dist_comm]; apply dist_tri_right","We start by noting the symmetry of the distance function, which states that for any natural numbers \( n \) and \( m \), \(\text{dist}(n, m) = \text{dist}(m, n)\). Therefore, the goal of showing \( n \leq m + \text{dist}(n, m) \) is equivalent to showing \( n \leq m + \text{dist}(m, n) \).

To prove \( n \leq m + \text{dist}(m, n) \), we consider the definition of the distance function on natural numbers. The distance function \(\text{dist}(m, n)\) is defined as the absolute value of the difference between \( m \) and \( n \), i.e., \(\text{dist}(m, n) = |m - n|\).

We have two cases to consider:
1. **Case 1: \( m \geq n \)**
- In this case, \(\text{dist}(m, n) = m - n\).
- Therefore, \( m + \text{dist}(m, n) = m + (m - n) = 2m - n \).
- Since \( m \geq n \), it follows that \( 2m - n \geq n \), which simplifies to \( n \leq 2m - n \).
- Hence, \( n \leq m + \text{dist}(m, n) \).

2. **Case 2: \( m < n \)**
- In this case, \(\text{dist}(m, n) = n - m\).
- Therefore, \( m + \text{dist}(m, n) = m + (n - m) = n \).
- Clearly, \( n \leq n \).

In both cases, we have shown that \( n \leq m + \text{dist}(m, n) \). Therefore, the theorem holds for any natural numbers \( n \) and \( m \). This completes the proof.","theorem Nat.dist_tri_right' (n m : ℕ) : n ≤ m + dist n m := by
  rw [dist_comm]; apply dist_tri_right","import Mathlib.Algebra.Order.Ring.Nat
import Mathlib.Data.Nat.Dist

open Nat

"
7832,Nat.dist_eq_zero,theorem Nat.dist_eq_zero {n m : ℕ} (h : n = m) : dist n m = 0 := by sorry,"Equality Implies Zero Distance in Natural Numbers : For any natural numbers \( n \) and \( m \), if \( n = m \), then the distance between \( n \) and \( m \) is zero, i.e., \( \text{dist}(n, m) = 0 \).","theorem Nat.dist_eq_zero {n m : ℕ} (h : n = m) : dist n m = 0 := by
  rw [h, dist_self]","Given natural numbers \( n \) and \( m \) such that \( n = m \), we need to show that \( \text{dist}(n, m) = 0 \).

1. Since \( n = m \), we can substitute \( n \) with \( m \) in the distance function, which gives us \( \text{dist}(m, m) \).
2. By the property of the distance function, the distance from any element to itself is zero, i.e., \( \text{dist}(m, m) = 0 \).
3. Therefore, \( \text{dist}(n, m) = 0 \).

This completes the proof.","theorem Nat.dist_eq_zero {n m : ℕ} (h : n = m) : dist n m = 0 := by
  rw [h, dist_self]","import Mathlib.Algebra.Order.Ring.Nat
import Mathlib.Data.Nat.Dist

open Nat

"
7836,Nat.dist_eq_sub_of_le,theorem Nat.dist_eq_sub_of_le {n m : ℕ} (h : n ≤ m) : dist n m = m - n := by sorry,"Distance Equals Subtraction When \( n \leq m \) in Natural Numbers : For any natural numbers \( n \) and \( m \), if \( n \leq m \), then the distance between \( n \) and \( m \) is equal to \( m - n \), i.e., \( \text{dist}(n, m) = m - n \).","theorem Nat.dist_eq_sub_of_le {n m : ℕ} (h : n ≤ m) : dist n m = m - n := by
  rw [dist, tsub_eq_zero_iff_le.mpr h, zero_add]","To prove that \( \text{dist}(n, m) = m - n \) for natural numbers \( n \) and \( m \) where \( n \leq m \), we start by using the definition of the distance function on natural numbers. The distance function is defined as:
\[
\text{dist}(n, m) = n - m + (m - n)
\]
Given that \( n \leq m \), we know from the property of natural numbers that \( n - m = 0 \). Therefore, the expression simplifies to:
\[
\text{dist}(n, m) = 0 + (m - n)
\]
Using the property that adding zero to any number does not change the number, we have:
\[
0 + (m - n) = m - n
\]
Thus, we have shown that:
\[
\text{dist}(n, m) = m - n
\]
This completes the proof.","theorem Nat.dist_eq_sub_of_le {n m : ℕ} (h : n ≤ m) : dist n m = m - n := by
/- First, we use the definition of the distance function on natural numbers, which states that \( \text{dist}(n, m) = n - m + (m - n) \). Since \( n \leq m \), we know that \( n - m = 0 \) by the property that the subtraction of two natural numbers is zero if and only if the first number is less than or equal to the second. Therefore, the expression simplifies to \( 0 + (m - n) \). Finally, using the property that adding zero to any number does not change the number, we get \( 0 + (m - n) = m - n \). Thus, the goal is to show that \( \text{dist}(n, m) = m - n \). -/
  rw [dist, tsub_eq_zero_iff_le.mpr h, zero_add]","import Mathlib.Algebra.Order.Ring.Nat
import Mathlib.Data.Nat.Dist

open Nat

"
7849,Nat.dist_tri_left',theorem Nat.dist_tri_left' (n m : ℕ) : n ≤ dist n m + m := by sorry,"Left Triangle Inequality for Distance in Natural Numbers : For any natural numbers \( n \) and \( m \), it holds that \( n \leq \text{dist}(n, m) + m \).","theorem Nat.dist_tri_left' (n m : ℕ) : n ≤ dist n m + m := by
  rw [dist_comm]; apply dist_tri_left","We start with the goal of proving that for any natural numbers \( n \) and \( m \), \( n \leq \text{dist}(n, m) + m \).

1. **Symmetry of Distance Function:**
Since the distance function is symmetric, we have:
\[
\text{dist}(n, m) = \text{dist}(m, n)
\]
Therefore, the goal \( n \leq \text{dist}(n, m) + m \) can be rewritten as:
\[
n \leq \text{dist}(m, n) + m
\]

2. **Definition of Distance Function:**
The distance function \(\text{dist}(m, n)\) is defined as the truncated subtraction \(|m - n|\), which is the non-negative difference between \(m\) and \(n\). This means:
\[
\text{dist}(m, n) = |m - n|
\]

3. **Case Analysis:**
We consider two cases based on the values of \(m\) and \(n\):
- **Case 1: \(m \geq n\):**
In this case, \(\text{dist}(m, n) = m - n\). Substituting this into the goal, we get:
\[
n \leq (m - n) + m
\]
Simplifying the right-hand side:
\[
n \leq m - n + m = 2m - n
\]
Since \(m \geq n\), we have \(2m - n \geq n\). Therefore, the inequality \(n \leq 2m - n\) holds.

- **Case 2: \(m < n\):**
In this case, \(\text{dist}(m, n) = n - m\). Substituting this into the goal, we get:
\[
n \leq (n - m) + m
\]
Simplifying the right-hand side:
\[
n \leq n - m + m = n
\]
The inequality \(n \leq n\) is trivially true.

Since both cases hold, we conclude that for any natural numbers \( n \) and \( m \), the inequality \( n \leq \text{dist}(n, m) + m \) is true. This completes the proof. \(\blacksquare\)","theorem Nat.dist_tri_left' (n m : ℕ) : n ≤ dist n m + m := by
  rw [dist_comm]; apply dist_tri_left","import Mathlib.Algebra.Order.Ring.Nat
import Mathlib.Data.Nat.Dist

open Nat

"
7862,Nat.dist_mul_right,theorem Nat.dist_mul_right (n k m : ℕ) : dist (n * k) (m * k) = dist n m * k := by sorry,"Right Multiplication Preserves Distance in Natural Numbers : For any natural numbers \( n, k, \) and \( m \), the distance between \( n \cdot k \) and \( m \cdot k \) is equal to the distance between \( n \) and \( m \) multiplied by \( k \), i.e., \(\text{dist}(n \cdot k, m \cdot k) = \text{dist}(n, m) \cdot k\).","theorem Nat.dist_mul_right (n k m : ℕ) : dist (n * k) (m * k) = dist n m * k := by
  rw [dist, dist, right_distrib, tsub_mul n, tsub_mul m]","To prove the theorem, we start by using the definition of the distance function on natural numbers. The distance function \(\text{dist}(n, m)\) is defined as:
\[
\text{dist}(n, m) = n - m + (m - n)
\]
Applying this definition to both \(\text{dist}(n \cdot k, m \cdot k)\) and \(\text{dist}(n, m)\), we have:
\[
\text{dist}(n \cdot k, m \cdot k) = (n \cdot k - m \cdot k) + (m \cdot k - n \cdot k)
\]
and
\[
\text{dist}(n, m) = n - m + (m - n)
\]
Thus, the goal is to show:
\[
(n \cdot k - m \cdot k) + (m \cdot k - n \cdot k) = (n - m + (m - n)) \cdot k
\]
Next, we use the right distributive property of multiplication over addition, which states:
\[
(a + b) \cdot c = a \cdot c + b \cdot c
\]
Applying this property, we get:
\[
(n - m + (m - n)) \cdot k = (n - m) \cdot k + (m - n) \cdot k
\]
Finally, we use the distributive property of multiplication over subtraction, which states:
\[
(a - b) \cdot c = a \cdot c - b \cdot c
\]
Applying this property to both terms, we get:
\[
(n - m) \cdot k = n \cdot k - m \cdot k
\]
and
\[
(m - n) \cdot k = m \cdot k - n \cdot k
\]
Thus, we have:
\[
(n - m) \cdot k + (m - n) \cdot k = (n \cdot k - m \cdot k) + (m \cdot k - n \cdot k)
\]
This shows that:
\[
\text{dist}(n \cdot k, m \cdot k) = \text{dist}(n, m) \cdot k
\]
Therefore, the theorem is proved. \(\blacksquare\)","theorem Nat.dist_mul_right (n k m : ℕ) : dist (n * k) (m * k) = dist n m * k := by
/- First, we use the definition of the distance function on natural numbers, which is given by \(\text{dist}(n, m) = n - m + (m - n)\). Applying this definition to both \(\text{dist}(n \cdot k, m \cdot k)\) and \(\text{dist}(n, m)\), we get:
\[
(n \cdot k - m \cdot k) + (m \cdot k - n \cdot k) = (n - m + (m - n)) \cdot k
\]
Next, we use the right distributive property of multiplication over addition, which states that \((a + b) \cdot c = a \cdot c + b \cdot c\). Applying this property, we get:
\[
(n \cdot k - m \cdot k) + (m \cdot k - n \cdot k) = (n - m) \cdot k + (m - n) \cdot k
\]
Finally, we use the distributive property of multiplication over subtraction, which states that \((a - b) \cdot c = a \cdot c - b \cdot c\). Applying this property to both terms, we get:
\[
(n \cdot k - m \cdot k) + (m \cdot k - n \cdot k) = (n \cdot k - m \cdot k) + (m \cdot k - n \cdot k)
\]
This shows that the left-hand side and the right-hand side are equal, thus proving the theorem. -/
  rw [dist, dist, right_distrib, tsub_mul n, tsub_mul m]","import Mathlib.Algebra.Order.Ring.Nat
import Mathlib.Data.Nat.Dist

open Nat

"
10118,Function.iterate_add_apply,theorem Function.iterate_add_apply (m n : ℕ) (x : α) : f^[m + n] x = f^[m] (f^[n] x) := by sorry,"Function Iteration: \( f^{\circ (m + n)}(x) = f^{\circ m}(f^{\circ n}(x)) \) : For any function \( f : \alpha \to \alpha \) and any natural numbers \( m \) and \( n \), the \( (m + n) \)-fold iteration of \( f \) applied to an element \( x \in \alpha \) is equal to the \( m \)-fold iteration of \( f \) applied to the \( n \)-fold iteration of \( f \) applied to \( x \). In other words, \( f^{\circ (m + n)}(x) = f^{\circ m}(f^{\circ n}(x)) \).","theorem Function.iterate_add_apply (m n : ℕ) (x : α) : f^[m + n] x = f^[m] (f^[n] x) := by
  rw [iterate_add f m n]
  rfl","We start by using the theorem that the \( (m + n) \)-fold iteration of \( f \) is equal to the composition of the \( m \)-fold iteration of \( f \) with the \( n \)-fold iteration of \( f \). This allows us to rewrite the goal \( f^{\circ (m + n)}(x) = f^{\circ m}(f^{\circ n}(x)) \) as \( (f^{\circ m} \circ f^{\circ n})(x) = f^{\circ m}(f^{\circ n}(x)) \).

Since both sides of the equation \( (f^{\circ m} \circ f^{\circ n})(x) \) and \( f^{\circ m}(f^{\circ n}(x)) \) are definitionally equal, the goal is trivially true due to the reflexive property of equality. Therefore, we have shown that \( f^{\circ (m + n)}(x) = f^{\circ m}(f^{\circ n}(x)) \).

This completes the proof.","theorem Function.iterate_add_apply (m n : ℕ) (x : α) : f^[m + n] x = f^[m] (f^[n] x) := by
/- Using the theorem that the \( (m + n) \)-fold iteration of \( f \) is equal to the composition of the \( m \)-fold iteration of \( f \) with the \( n \)-fold iteration of \( f \), we can rewrite the goal \( f^{\circ (m + n)}(x) = f^{\circ m}(f^{\circ n}(x)) \) as \( (f^{\circ m} \circ f^{\circ n})(x) = f^{\circ m}(f^{\circ n}(x)) \). -/
  rw [iterate_add f m n]
/- The current goal \( (f^{\circ m} \circ f^{\circ n})(x) = f^{\circ m}(f^{\circ n}(x)) \) is trivially true due to the reflexive property of equality, since both sides are definitionally equal. -/
  rfl","import Mathlib.Logic.Function.Conjugate
import Mathlib.Logic.Function.Iterate

open Function
variable {α : Type u} {β : Type v}
open Function (Commute)
variable (f : α → α)
"
10269,sq_add_sq_mul_sq_add_sq,"theorem sq_add_sq_mul_sq_add_sq :
    (x₁ ^ 2 + x₂ ^ 2) * (y₁ ^ 2 + y₂ ^ 2) = (x₁ * y₁ - x₂ * y₂) ^ 2 + (x₁ * y₂ + x₂ * y₁) ^ 2 := by sorry","Brahmagupta-Fibonacci Identity: \((x_1^2 + x_2^2) \cdot (y_1^2 + y_2^2) = (x_1 y_1 - x_2 y_2)^2 + (x_1 y_2 + x_2 y_1)^2\) : For any commutative ring \( R \) and any elements \( x_1, x_2, y_1, y_2 \in R \), the following identity holds:
\[
(x_1^2 + x_2^2) \cdot (y_1^2 + y_2^2) = (x_1 y_1 - x_2 y_2)^2 + (x_1 y_2 + x_2 y_1)^2
\]","theorem sq_add_sq_mul_sq_add_sq :
    (x₁ ^ 2 + x₂ ^ 2) * (y₁ ^ 2 + y₂ ^ 2) = (x₁ * y₁ - x₂ * y₂) ^ 2 + (x₁ * y₂ + x₂ * y₁) ^ 2 := by
  ring","To prove the Brahmagupta-Fibonacci identity, we start with the left-hand side of the equation:
\[
(x_1^2 + x_2^2) \cdot (y_1^2 + y_2^2)
\]
We expand this product:
\[
(x_1^2 + x_2^2) \cdot (y_1^2 + y_2^2) = x_1^2 y_1^2 + x_1^2 y_2^2 + x_2^2 y_1^2 + x_2^2 y_2^2
\]

Next, we consider the right-hand side of the equation:
\[
(x_1 y_1 - x_2 y_2)^2 + (x_1 y_2 + x_2 y_1)^2
\]
We expand each square term:
\[
(x_1 y_1 - x_2 y_2)^2 = x_1^2 y_1^2 - 2 x_1 y_1 x_2 y_2 + x_2^2 y_2^2
\]
\[
(x_1 y_2 + x_2 y_1)^2 = x_1^2 y_2^2 + 2 x_1 y_2 x_2 y_1 + x_2^2 y_1^2
\]

Adding these two expanded forms together:
\[
(x_1 y_1 - x_2 y_2)^2 + (x_1 y_2 + x_2 y_1)^2 = (x_1^2 y_1^2 - 2 x_1 y_1 x_2 y_2 + x_2^2 y_2^2) + (x_1^2 y_2^2 + 2 x_1 y_2 x_2 y_1 + x_2^2 y_1^2)
\]
\[
= x_1^2 y_1^2 + x_1^2 y_2^2 + x_2^2 y_1^2 + x_2^2 y_2^2
\]

We see that both the left-hand side and the right-hand side simplify to the same expression:
\[
x_1^2 y_1^2 + x_1^2 y_2^2 + x_2^2 y_1^2 + x_2^2 y_2^2
\]

Thus, we have shown that:
\[
(x_1^2 + x_2^2) \cdot (y_1^2 + y_2^2) = (x_1 y_1 - x_2 y_2)^2 + (x_1 y_2 + x_2 y_1)^2
\]

This completes the proof.","theorem sq_add_sq_mul_sq_add_sq :
    (x₁ ^ 2 + x₂ ^ 2) * (y₁ ^ 2 + y₂ ^ 2) = (x₁ * y₁ - x₂ * y₂) ^ 2 + (x₁ * y₂ + x₂ * y₁) ^ 2 := by
/- By calculation in the commutative ring \( R \), we can simplify the left-hand side and the right-hand side of the equation to show that:
\[
(x_1^2 + x_2^2) \cdot (y_1^2 + y_2^2) = (x_1 y_1 - x_2 y_2)^2 + (x_1 y_2 + x_2 y_1)^2
\] -/
  ring","import Mathlib.Tactic.Ring
import Mathlib.Algebra.Ring.Identities


variable {R : Type*} [CommRing R] {a b x₁ x₂ x₃ x₄ x₅ x₆ x₇ x₈ y₁ y₂ y₃ y₄ y₅ y₆ y₇ y₈ n : R}
"
10413,logDeriv_const,theorem logDeriv_const (a : 𝕜') : logDeriv (fun _ : 𝕜 ↦ a) = 0 := by sorry,"Logarithmic Derivative of Constant Function is Zero : For any nontrivially normed fields \(\mathbb{𝕜}\) and \(\mathbb{𝕜}'\) and any constant \( a \in \mathbb{𝕜}' \), the logarithmic derivative of the constant function \( f(x) = a \) is zero, i.e., \(\logDeriv(f(x)) = 0\).","theorem logDeriv_const (a : 𝕜') : logDeriv (fun _ : 𝕜 ↦ a) = 0 := by
  ext
  simp [logDeriv_apply]","We start by introducing a new variable \( x \) to reason about the logarithmic derivative of the constant function \( f(x) = a \). The logarithmic derivative of a function \( f \) at a point \( x \) is defined as \(\logDeriv(f, x) = \frac{f'(x)}{f(x)}\).

Since \( f(x) = a \) is a constant function, its derivative \( f'(x) = 0 \). Therefore, the logarithmic derivative of \( f \) at \( x \) is:
\[
\logDeriv(f, x) = \frac{f'(x)}{f(x)} = \frac{0}{a} = 0
\]
This holds for any \( x \in \mathbb{K} \). Hence, the logarithmic derivative of the constant function \( f(x) = a \) is zero. This completes the proof. \(\blacksquare\)","theorem logDeriv_const (a : 𝕜') : logDeriv (fun _ : 𝕜 ↦ a) = 0 := by
/- Introduce a new variable \( x \) into the context to reason about the expression \(\logDeriv(f(x))\), allowing us to consider cases or instances involving \( x \). -/
  ext
/- Using the definition of the logarithmic derivative, we can simplify the expression \(\logDeriv(f(x))\) to \(\frac{f'(x)}{f(x)}\). Since \( f(x) = a \) is a constant function, its derivative \( f'(x) = 0 \). Therefore, \(\logDeriv(f(x)) = \frac{0}{a} = 0\). -/
  simp [logDeriv_apply]","import Mathlib.Analysis.Calculus.Deriv.ZPow
import Mathlib.Analysis.Calculus.LogDeriv


open Filter Function
open scoped Topology BigOperators Classical
variable {𝕜 𝕜': Type*} [NontriviallyNormedField 𝕜] [NontriviallyNormedField 𝕜']
  [NormedAlgebra 𝕜 𝕜']
"
10419,logDeriv_id,theorem logDeriv_id (x : 𝕜) : logDeriv id x = 1 / x := by sorry,"Logarithmic Derivative of Identity Function: \(\frac{d}{dx} \log(x) = \frac{1}{x}\) : For any nontrivially normed field \(\mathbb{K}\) and any element \( x \in \mathbb{K} \), the logarithmic derivative of the identity function at \( x \) is equal to \(\frac{1}{x}\), i.e., \(\frac{d}{dx} \log(\text{id}(x)) = \frac{1}{x}\).","theorem logDeriv_id (x : 𝕜) : logDeriv id x = 1 / x := by
  simp [logDeriv_apply]","To prove the theorem, we start by using the definition of the logarithmic derivative. The logarithmic derivative of a function \( f \) at a point \( x \) is given by \(\logDeriv(f, x) = \frac{f'(x)}{f(x)}\).

For the identity function \(\text{id}(x) = x\), we have:
- The derivative of the identity function is \(\text{id}'(x) = 1\).
- The value of the identity function at \( x \) is \(\text{id}(x) = x\).

Using these, the logarithmic derivative of the identity function at \( x \) is:
\[
\logDeriv(\text{id}, x) = \frac{\text{id}'(x)}{\text{id}(x)} = \frac{1}{x}
\]

Thus, we have shown that \(\logDeriv(\text{id}, x) = \frac{1}{x}\). This completes the proof. \(\blacksquare\)","theorem logDeriv_id (x : 𝕜) : logDeriv id x = 1 / x := by
/- Using the definition of the logarithmic derivative, we can simplify the proposition we want to show to \(\logDeriv(\text{id}, x) = \frac{\text{id}'(x)}{\text{id}(x)}\). Since the identity function \(\text{id}(x) = x\) and its derivative \(\text{id}'(x) = 1\), this simplifies to \(\logDeriv(\text{id}, x) = \frac{1}{x}\). -/
  simp [logDeriv_apply]","import Mathlib.Analysis.Calculus.Deriv.ZPow
import Mathlib.Analysis.Calculus.LogDeriv


open Filter Function
open scoped Topology BigOperators Classical
variable {𝕜 𝕜': Type*} [NontriviallyNormedField 𝕜] [NontriviallyNormedField 𝕜']
  [NormedAlgebra 𝕜 𝕜']
"
10430,logDeriv_zpow,theorem logDeriv_zpow (x : 𝕜) (n : ℤ) : logDeriv (· ^ n) x = n / x := by sorry,"Logarithmic Derivative of Integer Power: \(\logDeriv (x^n) = \frac{n}{x}\) : For any element \( x \) in a nontrivially normed field \( \mathbb{𝕜} \) and any integer \( n \), the logarithmic derivative of the function \( f(x) = x^n \) at \( x \) is given by \( \frac{n}{x} \). Formally, this can be written as:
\[
\logDeriv (x^n) = \frac{n}{x}.
\]","theorem logDeriv_zpow (x : 𝕜) (n : ℤ) : logDeriv (· ^ n) x = n / x := by
  rw [logDeriv_fun_zpow (by fun_prop), logDeriv_id', mul_one_div]","To prove the theorem, we start by using the theorem that the logarithmic derivative of \( f(x)^n \) is \( n \cdot \logDeriv f(x) \). Specifically, for the function \( f(x) = x \), we have:
\[
\logDeriv (x^n) = n \cdot \logDeriv (x).
\]
Next, we use the theorem that the logarithmic derivative of the identity function \( f(x) = x \) is \( \frac{1}{x} \), so:
\[
\logDeriv (x) = \frac{1}{x}.
\]
Substituting this into our previous equation, we get:
\[
\logDeriv (x^n) = n \cdot \frac{1}{x} = \frac{n}{x}.
\]
Thus, the logarithmic derivative of \( x^n \) at \( x \) is indeed \( \frac{n}{x} \). This completes the proof. \(\blacksquare\)","theorem logDeriv_zpow (x : 𝕜) (n : ℤ) : logDeriv (· ^ n) x = n / x := by
/- First, we use the theorem that the logarithmic derivative of \( f(x)^n \) is \( n \cdot \logDeriv f(x) \) to rewrite the goal. Since the identity function \( f(x) = x \) is differentiable at \( x \), we have:
\[
\logDeriv (x^n) = n \cdot \logDeriv (x).
\]
Next, we use the theorem that the logarithmic derivative of the identity function \( f(x) = x \) is \( \frac{1}{x} \), so:
\[
\logDeriv (x) = \frac{1}{x}.
\]
Finally, we use the property that \( n \cdot \left( \frac{1}{x} \right) = \frac{n}{x} \), which simplifies our goal to:
\[
\logDeriv (x^n) = \frac{n}{x}.
\] -/
  rw [logDeriv_fun_zpow (by fun_prop), logDeriv_id', mul_one_div]","import Mathlib.Analysis.Calculus.Deriv.ZPow
import Mathlib.Analysis.Calculus.LogDeriv


open Filter Function
open scoped Topology BigOperators Classical
variable {𝕜 𝕜': Type*} [NontriviallyNormedField 𝕜] [NontriviallyNormedField 𝕜']
  [NormedAlgebra 𝕜 𝕜']
"
10556,legendreSym.mul,theorem legendreSym.mul (a b : ℤ) : legendreSym p (a * b) = legendreSym p a * legendreSym p b := by sorry,"Multiplicativity of Legendre Symbol: \(\left(\frac{a \cdot b}{p}\right) = \left(\frac{a}{p}\right) \cdot \left(\frac{b}{p}\right)\) : For any prime number \( p \) and integers \( a \) and \( b \), the Legendre symbol of the product \( a \cdot b \) modulo \( p \) is equal to the product of the Legendre symbols of \( a \) and \( b \) modulo \( p \). In other words, \(\left(\frac{a \cdot b}{p}\right) = \left(\frac{a}{p}\right) \cdot \left(\frac{b}{p}\right)\).","theorem legendreSym.mul (a b : ℤ) : legendreSym p (a * b) = legendreSym p a * legendreSym p b := by
  simp [legendreSym, Int.cast_mul, map_mul, quadraticCharFun_mul]","To prove the multiplicativity of the Legendre symbol, we start with the goal:
\[
\left(\frac{a \cdot b}{p}\right) = \left(\frac{a}{p}\right) \cdot \left(\frac{b}{p}\right)
\]

We use the following definitions and properties:
1. The Legendre symbol \(\left(\frac{a}{p}\right)\) is defined as the quadratic character of \(a\) modulo \(p\). Specifically:
- \(\left(\frac{a}{p}\right) = 0\) if \(a \equiv 0 \pmod{p}\),
- \(\left(\frac{a}{p}\right) = 1\) if \(a\) is a nonzero square modulo \(p\),
- \(\left(\frac{a}{p}\right) = -1\) otherwise.
2. The canonical homomorphism from the integers to the field \(\mathbb{Z}/p\mathbb{Z}\) preserves multiplication, i.e., for any integers \(m\) and \(n\),
\[
\text{Int.cast}(m \cdot n) = (\text{Int.cast}(m)) \cdot (\text{Int.cast}(n)).
\]
3. The quadratic character function \(\chi\) in a finite field \(\mathbb{Z}/p\mathbb{Z}\) is multiplicative, i.e., for any elements \(a, b \in \mathbb{Z}/p\mathbb{Z}\),
\[
\chi(a \cdot b) = \chi(a) \cdot \chi(b).
\]

Using these properties, we can simplify the goal:
\[
\left(\frac{a \cdot b}{p}\right) = \chi(a \cdot b) = \chi(a) \cdot \chi(b) = \left(\frac{a}{p}\right) \cdot \left(\frac{b}{p}\right).
\]

Thus, the multiplicativity of the Legendre symbol is established. This completes the proof. \(\blacksquare\)","theorem legendreSym.mul (a b : ℤ) : legendreSym p (a * b) = legendreSym p a * legendreSym p b := by
/- Using the definitions and properties of the Legendre symbol, the canonical homomorphism from integers to a field, and the multiplicative property of the quadratic character function, we can simplify the goal. Specifically, we use the following:
- The Legendre symbol \(\left(\frac{a}{p}\right)\) is defined as the quadratic character of \(a\) modulo \(p\).
- The canonical homomorphism from integers to a field preserves multiplication.
- The quadratic character function is multiplicative in finite fields.

Thus, the goal \(\left(\frac{a \cdot b}{p}\right) = \left(\frac{a}{p}\right) \cdot \left(\frac{b}{p}\right)\) is equivalent to the multiplicative property of the quadratic character function, which holds by definition. This completes the proof. -/
  simp [legendreSym, Int.cast_mul, map_mul, quadraticCharFun_mul]","import Mathlib.NumberTheory.LegendreSymbol.QuadraticChar.Basic
import Mathlib.NumberTheory.LegendreSymbol.Basic

open legendreSym
open Nat
variable (p : ℕ) [Fact p.Prime]
open ZMod
variable (p : ℕ) [Fact p.Prime]
"
10650,legendreSym.eq_pow,theorem legendreSym.eq_pow (a : ℤ) : (legendreSym p a : ZMod p) = (a : ZMod p) ^ (p / 2) := by sorry,"Legendre Symbol Congruence: \(\left(\frac{a}{p}\right) \equiv a^{(p / 2)} \pmod{p}\) : For any prime number \( p \) and any integer \( a \), the Legendre symbol \(\left(\frac{a}{p}\right)\) is congruent to \( a^{(p / 2)} \) modulo \( p \), i.e., \(\left(\frac{a}{p}\right) \equiv a^{(p / 2)} \pmod{p}\).","theorem legendreSym.eq_pow (a : ℤ) : (legendreSym p a : ZMod p) = (a : ZMod p) ^ (p / 2) := by
  rcases eq_or_ne (ringChar (ZMod p)) 2 with hc | hc
  · by_cases ha : (a : ZMod p) = 0
    · rw [legendreSym, ha, quadraticChar_zero,
        zero_pow (Nat.div_pos (@Fact.out p.Prime).two_le (succ_pos 1)).ne']
      norm_cast
    · have := (ringChar_zmod_n p).symm.trans hc
      -- p = 2
      subst p
      rw [legendreSym, quadraticChar_eq_one_of_char_two hc ha]
      revert ha
      push_cast
      generalize (a : ZMod 2) = b; fin_cases b
      · tauto
      · simp
  · convert quadraticChar_eq_pow_of_char_ne_two' hc (a : ZMod p)
    exact (card p).symm","We start by considering two cases based on the characteristic of the ring \(\mathbb{Z}/p\mathbb{Z}\).

1. **Case 1: The characteristic of \(\mathbb{Z}/p\mathbb{Z}\) is 2.**
- **Subcase 1.1: \(a \equiv 0 \pmod{p}\).**
- By the definition of the Legendre symbol, \(\left(\frac{a}{p}\right) = 0\).
- Since \(a \equiv 0 \pmod{p}\), we have \(a = 0\) in \(\mathbb{Z}/p\mathbb{Z}\).
- Therefore, \(a^{(p / 2)} = 0^{(p / 2)} = 0\).
- Hence, \(\left(\frac{a}{p}\right) = 0 = a^{(p / 2)}\).
- **Subcase 1.2: \(a \not\equiv 0 \pmod{p}\).**
- Since the characteristic of \(\mathbb{Z}/p\mathbb{Z}\) is 2, we have \(p = 2\).
- By the definition of the Legendre symbol, \(\left(\frac{a}{2}\right) = 1\) if \(a \not\equiv 0 \pmod{2}\).
- Since \(a \not\equiv 0 \pmod{2}\), we have \(a = 1\) in \(\mathbb{Z}/2\mathbb{Z}\).
- Therefore, \(a^{(2 / 2)} = 1^1 = 1\).
- Hence, \(\left(\frac{a}{2}\right) = 1 = a^{(2 / 2)}\).

2. **Case 2: The characteristic of \(\mathbb{Z}/p\mathbb{Z}\) is not 2.**
- By the property of the quadratic character in a field of characteristic not 2, we have \(\left(\frac{a}{p}\right) = a^{(\text{card}(\mathbb{Z}/p\mathbb{Z}) / 2)}\).
- Since the cardinality of \(\mathbb{Z}/p\mathbb{Z}\) is \(p\), we have \(\text{card}(\mathbb{Z}/p\mathbb{Z}) = p\).
- Therefore, \(\left(\frac{a}{p}\right) = a^{(p / 2)}\).

In both cases, we have shown that \(\left(\frac{a}{p}\right) \equiv a^{(p / 2)} \pmod{p}\). This completes the proof. \(\blacksquare\)","theorem legendreSym.eq_pow (a : ℤ) : (legendreSym p a : ZMod p) = (a : ZMod p) ^ (p / 2) := by
/- We consider two cases: either the characteristic of the ring \(\mathbb{Z}/p\mathbb{Z}\) is 2, or it is not 2. -/
  rcases eq_or_ne (ringChar (ZMod p)) 2 with hc | hc
/- In the case where the characteristic of \(\mathbb{Z}/p\mathbb{Z}\) is 2, we further consider two subcases: either \(a \equiv 0 \pmod{p}\) or \(a \not\equiv 0 \pmod{p}\). -/
  · by_cases ha : (a : ZMod p) = 0
/- In the subcase where \(a \equiv 0 \pmod{p}\), we use the definition of the Legendre symbol and the fact that the quadratic character of 0 is 0. We also use the property that \(0\) raised to any positive power is \(0\). This simplifies our goal to showing that \(0 = 0\). -/
    · rw [legendreSym, ha, quadraticChar_zero,
        zero_pow (Nat.div_pos (@Fact.out p.Prime).two_le (succ_pos 1)).ne']
/- Normalize the expression to show that \(0 = 0\), which is trivially true. -/
      norm_cast
/- In the subcase where \(a \not\equiv 0 \pmod{p}\), we use the fact that the characteristic of \(\mathbb{Z}/p\mathbb{Z}\) is 2 to show that \(p = 2\). -/
    · have := (ringChar_zmod_n p).symm.trans hc
      -- p = 2
/- Substitute \(p\) with 2 in the goal. -/
      subst p
/- Use the definition of the Legendre symbol and the fact that the quadratic character of a nonzero element in a field of characteristic 2 is 1. This simplifies our goal to showing that \(1 = a^1\). -/
      rw [legendreSym, quadraticChar_eq_one_of_char_two hc ha]
/- Strengthen the proposition we are proving. We show that for every \(a \in \mathbb{Z}\), if \(a \not\equiv 0 \pmod{2}\), then \(1 = a^1\). -/
      revert ha
/- Simplify the expression to show that \(1 = a^1\). -/
      push_cast
/- Generalize \(a \in \mathbb{Z}/2\mathbb{Z}\) to a new variable \(b\), and consider each possible value of \(b\) (which can be 0 or 1). -/
      generalize (a : ZMod 2) = b; fin_cases b
/- Automatically prove the current goal by systematically applying logical rules to decompose the goal and assumptions until a trivial case is reached. -/
      · tauto
/- Simplify the proposition we want to show, which is trivially true. -/
      · simp
/- In the case where the characteristic of \(\mathbb{Z}/p\mathbb{Z}\) is not 2, we use the fact that the quadratic character of \(a\) in a field of characteristic not 2 is equal to \(a\) raised to the power of \(\frac{\text{card}(\mathbb{Z}/p\mathbb{Z})}{2}\). This simplifies our goal to showing that \(p = \text{card}(\mathbb{Z}/p\mathbb{Z})\). -/
  · convert quadraticChar_eq_pow_of_char_ne_two' hc (a : ZMod p)
/- The current goal \(p = \text{card}(\mathbb{Z}/p\mathbb{Z})\) is exactly proved by the fact that the cardinality of \(\mathbb{Z}/p\mathbb{Z}\) is \(p\). -/
    exact (card p).symm","import Mathlib.NumberTheory.LegendreSymbol.QuadraticChar.Basic
import Mathlib.NumberTheory.LegendreSymbol.Basic

open legendreSym
open Nat
variable (p : ℕ) [Fact p.Prime]
open ZMod
variable (p : ℕ) [Fact p.Prime]
"
11597,intrinsicInterior_empty,theorem intrinsicInterior_empty : intrinsicInterior 𝕜 (∅ : Set P) = ∅ := by sorry,"Intrinsic Interior of the Empty Set is the Empty Set : For any field \( \mathbb{K} \), vector space \( V \), and affine space \( P \) over \( \mathbb{K} \) with the necessary structures (ring, additive commutative group, module, topological space, and additive torsor), the intrinsic interior of the empty set \( \emptyset \) is the empty set \( \emptyset \).","theorem intrinsicInterior_empty : intrinsicInterior 𝕜 (∅ : Set P) = ∅ := by
  simp [intrinsicInterior]","To prove that the intrinsic interior of the empty set \( \emptyset \) is the empty set \( \emptyset \), we start by using the definition of the intrinsic interior. The intrinsic interior of a set \( s \) in a normed additive torsor is the interior of \( s \) considered as a set in its affine span. Formally, for a set \( s \subseteq P \) in a normed additive torsor \( P \) over a vector space \( V \) with coefficients in a ring \( \mathbb{K} \), the intrinsic interior of \( s \) is the image of the interior of \( s \) under the canonical map from the affine span of \( s \) to \( P \).

For the empty set \( \emptyset \):
1. The affine span of \( \emptyset \) is also the empty set, since there are no points in \( \emptyset \) to span.
2. The interior of the empty set in any topological space is the empty set, because the empty set has no points and thus no interior points.
3. Therefore, the intrinsic interior of \( \emptyset \) is the image of the empty set under the canonical map from the affine span of \( \emptyset \) to \( P \), which is the empty set.

Hence, the intrinsic interior of the empty set \( \emptyset \) is the empty set \( \emptyset \). This completes the proof. \(\blacksquare\)","theorem intrinsicInterior_empty : intrinsicInterior 𝕜 (∅ : Set P) = ∅ := by
  simp [intrinsicInterior]","import Mathlib.Analysis.Normed.Affine.AddTorsorBases
import Mathlib.Analysis.Convex.Intrinsic


open AffineSubspace Set
open scoped Pointwise
variable {𝕜 V W Q P : Type*}
variable (𝕜) [Ring 𝕜] [AddCommGroup V] [Module 𝕜 V] [TopologicalSpace P] [AddTorsor V P]
  {s t : Set P} {x : P}
variable {𝕜}
"
11600,intrinsicFrontier_empty,theorem intrinsicFrontier_empty : intrinsicFrontier 𝕜 (∅ : Set P) = ∅ := by sorry,"Intrinsic Frontier of the Empty Set is the Empty Set : For any field \( \mathbb{K} \), vector space \( V \), and affine space \( P \) over \( \mathbb{K} \) with the necessary structures (ring, additive commutative group, module, topological space, and additive torsor), the intrinsic frontier of the empty set \( \emptyset \) is the empty set \( \emptyset \).","theorem intrinsicFrontier_empty : intrinsicFrontier 𝕜 (∅ : Set P) = ∅ := by
  simp [intrinsicFrontier]","To prove that the intrinsic frontier of the empty set \( \emptyset \) is the empty set \( \emptyset \), we start by recalling the definition of the intrinsic frontier. The intrinsic frontier of a set \( s \) in an affine space \( P \) over a field \( \mathbb{K} \) is defined as the image of the frontier of the preimage of \( s \) under the canonical map from the affine span of \( s \) to \( P \).

1. **Step 1: Simplify the Proposition Using the Definition**
- The preimage of the empty set \( \emptyset \) under the canonical map from the affine span of \( \emptyset \) to \( P \) is the empty set. This is because there are no points in \( \emptyset \) to map.
- The frontier of the empty set is the empty set. This is a fundamental property of the frontier in topology: the frontier of the empty set is the empty set.
- Therefore, the intrinsic frontier of the empty set is the image of the empty set under the canonical map, which is the empty set.

Thus, the intrinsic frontier of the empty set \( \emptyset \) is the empty set \( \emptyset \). This completes the proof. \(\blacksquare\)","theorem intrinsicFrontier_empty : intrinsicFrontier 𝕜 (∅ : Set P) = ∅ := by
  simp [intrinsicFrontier]","import Mathlib.Analysis.Normed.Affine.AddTorsorBases
import Mathlib.Analysis.Convex.Intrinsic


open AffineSubspace Set
open scoped Pointwise
variable {𝕜 V W Q P : Type*}
variable (𝕜) [Ring 𝕜] [AddCommGroup V] [Module 𝕜 V] [TopologicalSpace P] [AddTorsor V P]
  {s t : Set P} {x : P}
variable {𝕜}
"
11613,IsClosed.intrinsicClosure,"theorem IsClosed.intrinsicClosure (hs : IsClosed ((↑) ⁻¹' s : Set <| affineSpan 𝕜 s)) :
    intrinsicClosure 𝕜 s = s := by sorry","Intrinsic Closure of a Closed Set in Affine Span Equals the Set Itself : For any set \( s \subseteq P \) in a normed additive torsor, if the preimage of \( s \) under the canonical map from the affine span of \( s \) to \( P \) is closed, then the intrinsic closure of \( s \) is equal to \( s \) itself, i.e., \(\text{intrinsicClosure}(\mathbb{K}, s) = s\).","theorem IsClosed.intrinsicClosure (hs : IsClosed ((↑) ⁻¹' s : Set <| affineSpan 𝕜 s)) :
    intrinsicClosure 𝕜 s = s := by
  rw [intrinsicClosure, hs.closure_eq, image_preimage_eq_of_subset]
  exact (subset_affineSpan _ _).trans Subtype.range_coe.superset","We start by using the definition of the intrinsic closure. The intrinsic closure of a set \( s \) is the image of the closure of \( s \) in its affine span under the canonical map. Given that the preimage of \( s \) under the canonical map is closed, the closure of this preimage is equal to the preimage itself. Therefore, the intrinsic closure of \( s \) is the image of the preimage of \( s \) under the canonical map.

Next, we use the property of the image of the preimage. If \( s \) is a subset of the range of the canonical map, then the image of the preimage of \( s \) is equal to \( s \). To show that \( s \) is a subset of the range of the canonical map, we note that \( s \) is contained in its affine span. The range of the canonical map from the subtype to the set \( s \) is exactly \( s \). By transitivity of the subset relation, if \( s \) is a subset of its affine span and the range of the canonical map is \( s \), then \( s \) is a subset of the range of the canonical map.

Thus, the intrinsic closure of \( s \) is equal to \( s \). This completes the proof. \(\blacksquare\)","theorem IsClosed.intrinsicClosure (hs : IsClosed ((↑) ⁻¹' s : Set <| affineSpan 𝕜 s)) :
    intrinsicClosure 𝕜 s = s := by
/- First, we use the definition of intrinsic closure, which states that the intrinsic closure of a set \( s \) is the image of the closure of \( s \) in its affine span under the canonical map. Since the preimage of \( s \) under the canonical map is closed, the closure of this preimage is equal to the preimage itself. Therefore, the intrinsic closure of \( s \) is the image of the preimage of \( s \) under the canonical map. By the property of the image of the preimage, if \( s \) is a subset of the range of the canonical map, then the image of the preimage of \( s \) is equal to \( s \). Thus, the intrinsic closure of \( s \) is equal to \( s \). -/
  rw [intrinsicClosure, hs.closure_eq, image_preimage_eq_of_subset]
/- To show that \( s \) is a subset of the range of the canonical map, we use the fact that \( s \) is contained in its affine span. The range of the canonical map from the subtype to the set \( s \) is exactly \( s \). By transitivity of the subset relation, if \( s \) is a subset of its affine span and the range of the canonical map is \( s \), then \( s \) is a subset of the range of the canonical map. This completes the proof. -/
  exact (subset_affineSpan _ _).trans Subtype.range_coe.superset","import Mathlib.Analysis.Normed.Affine.AddTorsorBases
import Mathlib.Analysis.Convex.Intrinsic

open IsClosed
open AffineSubspace Set
open scoped Pointwise
variable {𝕜 V W Q P : Type*}
variable (𝕜) [Ring 𝕜] [AddCommGroup V] [Module 𝕜 V] [TopologicalSpace P] [AddTorsor V P]
  {s t : Set P} {x : P}
variable {𝕜}
"
11753,εNFA.evalFrom_empty,theorem εNFA.evalFrom_empty (x : List α) : M.evalFrom ∅ x = ∅ := by sorry,"Epsilon NFA Evaluation from Empty Set Results in Empty Set : For any epsilon Nondeterministic Finite Automaton \( M \) with alphabet \( \alpha \) and state type \( \sigma \), and for any list of characters \( x \) in \( \alpha \), the set of states that can be reached from the empty set of states by evaluating the string \( x \) is the empty set, i.e., \( M.\text{evalFrom}(\emptyset, x) = \emptyset \).","theorem εNFA.evalFrom_empty (x : List α) : M.evalFrom ∅ x = ∅ := by
  induction' x using List.reverseRecOn with x a ih
  · rw [evalFrom_nil, εClosure_empty]
  · rw [evalFrom_append_singleton, ih, stepSet_empty]","We prove the statement by induction on the list \( x \).

1. **Base Case:**
- We need to show that \( M.\text{evalFrom}(\emptyset, []) = \emptyset \).
- By the property that the evaluation of \( M \) from any set of states \( S \) with an empty input list is the ε-closure of \( S \), we have:
\[
M.\text{evalFrom}(\emptyset, []) = M.\text{εClosure}(\emptyset)
\]
- Since the ε-closure of the empty set of states is the empty set, we get:
\[
M.\text{εClosure}(\emptyset) = \emptyset
\]
- Therefore, \( M.\text{evalFrom}(\emptyset, []) = \emptyset \).

2. **Inductive Step:**
- Assume that \( M.\text{evalFrom}(\emptyset, x) = \emptyset \) for some list \( x \).
- We need to show that \( M.\text{evalFrom}(\emptyset, x ++ [a]) = \emptyset \) for any character \( a \) in the alphabet \( \alpha \).
- By the property that the evaluation of \( M \) from a set of states \( S \) with a list \( x \) followed by a singleton list containing \( a \) is equal to the step set of the evaluation of \( M \) from \( S \) with \( x \) and the input symbol \( a \), we have:
\[
M.\text{evalFrom}(\emptyset, x ++ [a]) = M.\text{stepSet}(M.\text{evalFrom}(\emptyset, x), a)
\]
- Using the inductive hypothesis \( M.\text{evalFrom}(\emptyset, x) = \emptyset \), we get:
\[
M.\text{stepSet}(\emptyset, a) = \emptyset
\]
- Since the step set of the empty set of states is the empty set, we conclude that:
\[
M.\text{evalFrom}(\emptyset, x ++ [a]) = \emptyset
\]

By induction, we have shown that for any list \( x \) of characters in \( \alpha \), \( M.\text{evalFrom}(\emptyset, x) = \emptyset \). This completes the proof.","theorem εNFA.evalFrom_empty (x : List α) : M.evalFrom ∅ x = ∅ := by
/- We prove the statement by induction on the list \( x \). For the base case, we need to show that \( M.\text{evalFrom}(\emptyset, []) = \emptyset \). For the inductive step, assuming that \( M.\text{evalFrom}(\emptyset, x) = \emptyset \) for some list \( x \), we need to show that \( M.\text{evalFrom}(\emptyset, x ++ [a]) = \emptyset \) for any character \( a \) in the alphabet \( \alpha \). -/
  induction' x using List.reverseRecOn with x a ih
/- First, we show that \( M.\text{evalFrom}(\emptyset, []) = \emptyset \). By the property that the evaluation of \( M \) from any set of states \( S \) with an empty input list is the ε-closure of \( S \), we have \( M.\text{evalFrom}(\emptyset, []) = M.\text{εClosure}(\emptyset) \). Since the ε-closure of the empty set of states is the empty set, we get \( M.\text{εClosure}(\emptyset) = \emptyset \). Therefore, \( M.\text{evalFrom}(\emptyset, []) = \emptyset \). -/
  · rw [evalFrom_nil, εClosure_empty]
/- Next, we show that \( M.\text{evalFrom}(\emptyset, x ++ [a]) = \emptyset \) for any list \( x \) and any character \( a \). By the property that the evaluation of \( M \) from a set of states \( S \) with a list \( x \) followed by a singleton list containing \( a \) is equal to the step set of the evaluation of \( M \) from \( S \) with \( x \) and the input symbol \( a \), we have \( M.\text{evalFrom}(\emptyset, x ++ [a]) = M.\text{stepSet}(M.\text{evalFrom}(\emptyset, x), a) \). Using the inductive hypothesis \( M.\text{evalFrom}(\emptyset, x) = \emptyset \), we get \( M.\text{stepSet}(\emptyset, a) = \emptyset \). Since the step set of the empty set of states is the empty set, we conclude that \( M.\text{evalFrom}(\emptyset, x ++ [a]) = \emptyset \). -/
  · rw [evalFrom_append_singleton, ih, stepSet_empty]","import Mathlib.Computability.NFA
import Mathlib.Computability.EpsilonNFA

open εNFA
open Set
open Computability
variable {α : Type u} {σ σ' : Type v} (M : εNFA α σ) {S : Set σ} {x : List α} {s : σ} {a : α}
variable {M}
variable (M)
"
11823,εNFA.evalFrom_append_singleton,"theorem εNFA.evalFrom_append_singleton (S : Set σ) (x : List α) (a : α) :
    M.evalFrom S (x ++ [a]) = M.stepSet (M.evalFrom S x) a := by sorry","Epsilon NFA Evaluation of Appended Singleton List Equals Step Set of Evaluation : For any epsilon Nondeterministic Finite Automaton (εNFA) \( M \) over an alphabet \( \alpha \) and a set of states \( \sigma \), and for any set of states \( S \subseteq \sigma \), any list of characters \( x \) in \( \alpha \), and any input symbol \( a \in \alpha \), the evaluation of \( M \) from \( S \) with the list \( x \) followed by the singleton list containing \( a \) (denoted as \( x ++ [a] \)) is equal to the set of states reached by applying the step function to the set of states obtained by evaluating \( M \) from \( S \) with the list \( x \) and the input symbol \( a \). Formally, \( M.\text{evalFrom}(S, x ++ [a]) = M.\text{stepSet}(M.\text{evalFrom}(S, x), a) \).","theorem εNFA.evalFrom_append_singleton (S : Set σ) (x : List α) (a : α) :
    M.evalFrom S (x ++ [a]) = M.stepSet (M.evalFrom S x) a := by
  rw [evalFrom, List.foldl_append, List.foldl_cons, List.foldl_nil]","To prove the theorem, we start by using the definition of \( M.\text{evalFrom} \). By definition, \( M.\text{evalFrom}(S, x) \) is the set of all possible states that can be reached from any state in \( S \) by following the transitions in \( M \) with input \( x \). This is computed as:
\[
M.\text{evalFrom}(S, x) = \text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x)
\]
where \( M.\text{εClosure}(S) \) is the set of all states reachable from \( S \) via epsilon transitions.

Next, we use the properties of the left fold operation over lists. Specifically, we use the property that the left fold over the concatenation of two lists is equivalent to the left fold over the second list starting with the result of the left fold over the first list:
\[
\text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x ++ [a]) = \text{foldl}(M.\text{stepSet}, \text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x), [a])
\]

We then use the property of the left fold over a cons list, which states that the left fold over a list starting with an element is equivalent to the left fold over the tail of the list starting with the result of applying the function to the initial value and the first element:
\[
\text{foldl}(M.\text{stepSet}, \text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x), [a]) = \text{foldl}(M.\text{stepSet}, M.\text{stepSet}(\text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x), a), \text{nil})
\]

Finally, we use the property of the left fold over an empty list, which states that the left fold over an empty list starting with any initial value is equal to that initial value:
\[
\text{foldl}(M.\text{stepSet}, M.\text{stepSet}(\text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x), a), \text{nil}) = M.\text{stepSet}(\text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x), a)
\]

Combining these properties, we get:
\[
M.\text{evalFrom}(S, x ++ [a]) = \text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x ++ [a]) = M.\text{stepSet}(\text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x), a) = M.\text{stepSet}(M.\text{evalFrom}(S, x), a)
\]

Thus, we have shown that:
\[
M.\text{evalFrom}(S, x ++ [a]) = M.\text{stepSet}(M.\text{evalFrom}(S, x), a)
\]

This completes the proof.","theorem εNFA.evalFrom_append_singleton (S : Set σ) (x : List α) (a : α) :
    M.evalFrom S (x ++ [a]) = M.stepSet (M.evalFrom S x) a := by
/- First, we use the definition of \( M.\text{evalFrom} \) and the properties of left fold over lists to rewrite the goal. Specifically, we use the fact that \( M.\text{evalFrom}(S, x) \) is equivalent to \( \text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x) \). By the property of left fold over concatenation of lists, we have:
\[
\text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x ++ [a]) = \text{foldl}(M.\text{stepSet}, \text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x), [a])
\]
By the property of left fold over a cons list, we further have:
\[
\text{foldl}(M.\text{stepSet}, \text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x), [a]) = \text{foldl}(M.\text{stepSet}, M.\text{stepSet}(\text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x), a), \text{nil})
\]
Finally, by the property of left fold over an empty list, we get:
\[
\text{foldl}(M.\text{stepSet}, M.\text{stepSet}(\text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x), a), \text{nil}) = M.\text{stepSet}(\text{foldl}(M.\text{stepSet}, M.\text{εClosure}(S), x), a)
\]
Thus, the goal is equivalent to:
\[
M.\text{evalFrom}(S, x ++ [a]) = M.\text{stepSet}(M.\text{evalFrom}(S, x), a)
\] -/
  rw [evalFrom, List.foldl_append, List.foldl_cons, List.foldl_nil]","import Mathlib.Computability.NFA
import Mathlib.Computability.EpsilonNFA

open εNFA
open Set
open Computability
variable {α : Type u} {σ σ' : Type v} (M : εNFA α σ) {S : Set σ} {x : List α} {s : σ} {a : α}
variable {M}
variable (M)
"
11915,MvQPF.id_map,theorem MvQPF.id_map {α : TypeVec n} (x : F α) : TypeVec.id <$$> x = x := by sorry,"Identity Map on Multivariate Polynomial Functor Elements : For any natural number \( n \) and any multivariate polynomial functor \( F \) of \( n \) variables that is an instance of `MvQPF`, and for any type vector \( \alpha \) of length \( n \), the identity function applied to an element \( x \) of the type \( F \alpha \) results in the same element \( x \). In other words, for any \( x \in F \alpha \), it holds that \( \text{id}_\alpha \cdot x = x \).","theorem MvQPF.id_map {α : TypeVec n} (x : F α) : TypeVec.id <$$> x = x := by
  rw [← abs_repr x, ← abs_map]
  rfl","To prove that \( \text{id}_\alpha \cdot x = x \) for any \( x \in F \alpha \), we start by using the properties of the multivariate polynomial functor \( F \). Specifically, we use the fact that the absolute value and representation of a multivariate polynomial functor are inverses, and the property that the absolute value of a multivariate polynomial functor preserves mapping. This allows us to rewrite the goal as follows:

\[
\text{id}_\alpha \cdot x = x \quad \text{is equivalent to} \quad \text{abs}(\text{id}_\alpha \cdot \text{repr}(x)) = \text{abs}(\text{repr}(x))
\]

Since the identity function \( \text{id}_\alpha \) applied to \( \text{repr}(x) \) is just \( \text{repr}(x) \), the left-hand side simplifies to \( \text{abs}(\text{repr}(x)) \). Therefore, the goal becomes:

\[
\text{abs}(\text{repr}(x)) = \text{abs}(\text{repr}(x))
\]

This equality is trivially true due to the reflexive property of equality, since both sides are the same. Hence, we have shown that \( \text{id}_\alpha \cdot x = x \). This completes the proof. \(\blacksquare\)","theorem MvQPF.id_map {α : TypeVec n} (x : F α) : TypeVec.id <$$> x = x := by
/- First, we use the fact that the absolute value and representation of a multivariate polynomial functor are inverses, and the property that the absolute value of a multivariate polynomial functor preserves mapping. This allows us to rewrite the goal from \( \text{id}_\alpha \cdot x = x \) to \( \text{abs}(\text{id}_\alpha \cdot \text{repr}(x)) = \text{abs}(\text{repr}(x)) \). -/
  rw [← abs_repr x, ← abs_map]
/- The current goal \( \text{abs}(\text{id}_\alpha \cdot \text{repr}(x)) = \text{abs}(\text{repr}(x)) \) is trivially true due to the reflexive property, since both sides are definitionally equal. -/
  rfl","import Mathlib.Data.PFunctor.Multivariate.Basic
import Mathlib.Data.QPF.Multivariate.Basic

open MvQPF
open MvFunctor
variable {n : ℕ} {F : TypeVec.{u} n → Type*} [q : MvQPF F]
open MvFunctor (LiftP LiftR)
"
12241,MvQPF.mem_supp,"theorem MvQPF.mem_supp {α : TypeVec n} (x : F α) (i) (u : α i) :
    u ∈ supp x i ↔ ∀ a f, abs ⟨a, f⟩ = x → u ∈ f i '' univ := by sorry","Element in Support of Multivariate Quotient Polynomial Functor if and only if in Image of Universal Set : For any natural number \( n \), a multivariate quotient polynomial functor \( F \) of \( n \) variables, and a type vector \( \alpha \) of length \( n \), an element \( u \) of type \( \alpha_i \) is in the support of \( x \) at index \( i \) if and only if for all elements \( a \) of the type \( (MvQPF.P F).A \) and all functions \( f \) from \( (MvQPF.P F).B a \) to \( \alpha \), if \( x = MvQPF.abs \langle a, f \rangle \), then \( u \) is in the image of \( f_i \) applied to the universal set \( \text{Set.univ} \).","theorem MvQPF.mem_supp {α : TypeVec n} (x : F α) (i) (u : α i) :
    u ∈ supp x i ↔ ∀ a f, abs ⟨a, f⟩ = x → u ∈ f i '' univ := by
  rw [supp]; dsimp; constructor
  · intro h a f haf
    have : LiftP (fun i u => u ∈ f i '' univ) x := by
      rw [liftP_iff]
      refine ⟨a, f, haf.symm, ?_⟩
      intro i u
      exact mem_image_of_mem _ (mem_univ _)
    exact h this
  intro h p; rw [liftP_iff]
  rintro ⟨a, f, xeq, h'⟩
  rcases h a f xeq.symm with ⟨i, _, hi⟩
  rw [← hi]; apply h'","We start by using the definition of the support of \( x \) at index \( i \). The support of \( x \) at index \( i \) is the set of elements \( y \) in \( \alpha_i \) such that for any family of propositions \( P \) indexed by the elements of \( \alpha \), if \( x \) is in the support of \( P \), then the proposition \( P y \) holds. After simplifying the definition, we split the goal into two directions of the equivalence.

1. **(\(\Rightarrow\))** Assume \( u \in \text{supp } x i \). This means for any family of propositions \( P \) indexed by the elements of \( \alpha \), if \( x \) is in the support of \( P \), then \( P i u \) holds. We need to show that for all \( a \) and \( f \) such that \( x = \text{MvQPF.abs} \langle a, f \rangle \), \( u \in f i '' \text{Set.univ} \).

- Let \( a \) and \( f \) be such that \( x = \text{MvQPF.abs} \langle a, f \rangle \).
- We need to show \( u \in f i '' \text{Set.univ} \).
- By the definition of the image of a set under a function, \( u \in f i '' \text{Set.univ} \) if and only if there exists \( j \in \text{Set.univ} \) such that \( f i j = u \).
- Since \( j \in \text{Set.univ} \) for any \( j \), it follows that \( f i j \in f i '' \text{Set.univ} \).

2. **(\(\Leftarrow\))** Assume for all \( a \) and \( f \) such that \( x = \text{MvQPF.abs} \langle a, f \rangle \), \( u \in f i '' \text{Set.univ} \). We need to show \( u \in \text{supp } x i \).

- Let \( P \) be a family of propositions indexed by the elements of \( \alpha \).
- Assume \( x \) is in the support of \( P \).
- We need to show \( P i u \).
- By the definition of the support, there exist \( a \) and \( f \) such that \( x = \text{MvQPF.abs} \langle a, f \rangle \) and for all \( i \) and \( j \), \( P (f i j) \).
- Since \( u \in f i '' \text{Set.univ} \), there exists \( j \) such that \( f i j = u \).
- By the hypothesis, \( P (f i j) \) holds, so \( P i u \) holds.

Thus, we have shown both directions of the equivalence, completing the proof.","theorem MvQPF.mem_supp {α : TypeVec n} (x : F α) (i) (u : α i) :
    u ∈ supp x i ↔ ∀ a f, abs ⟨a, f⟩ = x → u ∈ f i '' univ := by
/- First, we use the definition of the support of \( x \) at index \( i \) to rewrite the goal. The support of \( x \) at index \( i \) is the set of elements \( y \) in \( \alpha_i \) such that for any family of propositions \( P \) indexed by the elements of \( \alpha \), if \( x \) is in the support of \( P \), then the proposition \( P y \) holds. After simplifying the definition, we split the goal into two directions of the equivalence. -/
  rw [supp]; dsimp; constructor
/- We start by assuming the hypothesis \( h \) that for any family of propositions \( P \) indexed by the elements of \( \alpha \), if \( x \) is in the support of \( P \), then \( P i u \) holds. We also introduce the variables \( a \) and \( f \) and the hypothesis \( haf \) that \( x = \text{MvQPF.abs} \langle a, f \rangle \). Our goal is to show that \( u \in f i '' \text{Set.univ} \). -/
  · intro h a f haf
/- We need to show that the predicate \( \text{LiftP} (fun i u => u \in f i '' \text{Set.univ}) x \) holds. This means we need to construct a proof that for some \( a \) and \( f \), \( x = \text{MvQPF.abs} \langle a, f \rangle \) and for all \( i \) and \( j \), \( f i j \in f i '' \text{Set.univ} \). -/
    have : LiftP (fun i u => u ∈ f i '' univ) x := by
/- We use the equivalence \( \text{LiftP} p x \leftrightarrow \exists a f, x = \text{MvQPF.abs} \langle a, f \rangle \land \forall (i : \text{Fin2 } n) (j : (MvQPF.P F).B a i), p (f i j) \) to rewrite the goal. This means we need to show that there exist \( a \) and \( f \) such that \( x = \text{MvQPF.abs} \langle a, f \rangle \) and for all \( i \) and \( j \), \( f i j \in f i '' \text{Set.univ} \). -/
      rw [liftP_iff]
/- We refine the goal by providing \( a \) and \( f \) and the symmetry of \( haf \) (i.e., \( x = \text{MvQPF.abs} \langle a, f \rangle \)). This leaves us to show that for all \( i \) and \( j \), \( f i j \in f i '' \text{Set.univ} \). -/
      refine ⟨a, f, haf.symm, ?_⟩
/- We introduce the variables \( i \) and \( u \) and need to show that \( f i u \in f i '' \text{Set.univ} \). -/
      intro i u
/- Since \( u \in \text{Set.univ} \) (by the definition of the universal set), it follows that \( f i u \in f i '' \text{Set.univ} \) by the definition of the image of a set under a function. -/
      exact mem_image_of_mem _ (mem_univ _)
/- We use the hypothesis \( h \) and the fact that \( \text{LiftP} (fun i u => u \in f i '' \text{Set.univ}) x \) holds to conclude that \( u \in f i '' \text{Set.univ} \). -/
    exact h this
/- We introduce the hypothesis \( h \) and the predicate \( p \). We use the equivalence \( \text{LiftP} p x \leftrightarrow \exists a f, x = \text{MvQPF.abs} \langle a, f \rangle \land \forall (i : \text{Fin2 } n) (j : (MvQPF.P F).B a i), p (f i j) \) to rewrite the goal. This means we need to show that if there exist \( a \) and \( f \) such that \( x = \text{MvQPF.abs} \langle a, f \rangle \) and for all \( i \) and \( j \), \( p (f i j) \), then \( p i u \). -/
  intro h p; rw [liftP_iff]
/- We introduce the variables \( a \) and \( f \), the equality \( xeq \) (i.e., \( x = \text{MvQPF.abs} \langle a, f \rangle \)), and the hypothesis \( h' \) (i.e., \( \forall (i : \text{Fin2 } n) (j : (MvQPF.P F).B a i), p (f i j) \)). Our goal is to show that \( p i u \). -/
  rintro ⟨a, f, xeq, h'⟩
/- We use the hypothesis \( h \) and the symmetry of \( xeq \) to find an element \( i \) in \( (MvQPF.P F).B a i \) such that \( f i i = u \). This means we have \( f i i = u \) and \( i \in \text{Set.univ} \). -/
  rcases h a f xeq.symm with ⟨i, _, hi⟩
/- We use the equality \( f i i = u \) to rewrite the goal. Since \( h' \) states that \( p (f i i) \) holds, it follows that \( p i u \) holds. -/
  rw [← hi]; apply h'","import Mathlib.Data.PFunctor.Multivariate.Basic
import Mathlib.Data.QPF.Multivariate.Basic

open MvQPF
open MvFunctor
variable {n : ℕ} {F : TypeVec.{u} n → Type*} [q : MvQPF F]
open MvFunctor (LiftP LiftR)
open Set
open MvFunctor (LiftP LiftR)
"
13161,List.mk_add_mem_enumFrom_iff_get?,"theorem List.mk_add_mem_enumFrom_iff_get? {n i : ℕ} {x : α} {l : List α} :
    (n + i, x) ∈ enumFrom n l ↔ l.get? i = x := by sorry","Pair Membership in Enumerated List: \((n + i, x) \in \text{enumFrom } n \, l \leftrightarrow l.get? \, i = \text{some } x\) : For any type \( \alpha \), natural numbers \( n \) and \( i \), and a list \( l \) of elements of type \( \alpha \), the pair \((n + i, x)\) is a member of the list `enumFrom n l` if and only if the \( i \)-th element of the list \( l \) (zero-based) is \( x \), i.e., \( l.get? i = \text{some } x \).","theorem List.mk_add_mem_enumFrom_iff_get? {n i : ℕ} {x : α} {l : List α} :
    (n + i, x) ∈ enumFrom n l ↔ l.get? i = x := by
  simp [mem_iff_get?]","To prove the theorem, we use the equivalence that an element \((n + i, x)\) is a member of the list `enumFrom n l` if and only if the \( i \)-th element of the list \( l \) (zero-based) is \( x \). This equivalence is given by the theorem that states the membership of a pair in the enumerated list is equivalent to the optional element lookup in the list.

Formally, the theorem states:
\[
(n + i, x) \in \text{enumFrom } n \, l \leftrightarrow l.get? \, i = \text{some } x
\]

By using this equivalence, we can directly conclude that the pair \((n + i, x)\) is a member of the list `enumFrom n l` if and only if the \( i \)-th element of the list \( l \) is \( x \). This completes the proof. \(\blacksquare\)","theorem List.mk_add_mem_enumFrom_iff_get? {n i : ℕ} {x : α} {l : List α} :
    (n + i, x) ∈ enumFrom n l ↔ l.get? i = x := by
/- Using the equivalence that an element \((n + i, x)\) is a member of the list `enumFrom n l` if and only if the \( i \)-th element of the list \( l \) (zero-based) is \( x \), we can simplify the proposition we want to show. This equivalence is given by the theorem that states the membership of a pair in the enumerated list is equivalent to the optional element lookup in the list. -/
  simp [mem_iff_get?]","import Batteries.Tactic.Alias
import Mathlib.Tactic.TypeStar
import Mathlib.Data.Nat.Notation
import Mathlib.Data.List.Enum

open List
variable {α β : Type*}
"
15529,Nat.mod_mul_left_div_self,theorem Nat.mod_mul_left_div_self (m n k : Nat) : m % (k * n) / n = m / n % k := by sorry,"Modulo and Division Identity: \(\frac{m \mod (k \cdot n)}{n} = \left( \frac{m}{n} \right) \mod k\) : For any natural numbers \( m \), \( n \), and \( k \), the following identity holds:
\[ \frac{m \mod (k \cdot n)}{n} = \left( \frac{m}{n} \right) \mod k. \]","theorem Nat.mod_mul_left_div_self (m n k : Nat) : m % (k * n) / n = m / n % k := by
  rw [Nat.mul_comm k n, mod_mul_right_div_self]","To prove the identity \(\frac{m \mod (k \cdot n)}{n} = \left( \frac{m}{n} \right) \mod k\), we start by using the commutativity of multiplication. This allows us to rewrite \( k \cdot n \) as \( n \cdot k \). Therefore, the left-hand side of the identity becomes \(\frac{m \mod (n \cdot k)}{n}\).

Next, we apply the known identity \(\frac{m \mod (n \cdot k)}{n} = \left( \frac{m}{n} \right) \mod k\). This identity directly simplifies our goal to \(\left( \frac{m}{n} \right) \mod k\), which is exactly the right-hand side of the original identity.

Thus, we have shown that:
\[ \frac{m \mod (k \cdot n)}{n} = \left( \frac{m}{n} \right) \mod k. \]

This completes the proof.","theorem Nat.mod_mul_left_div_self (m n k : Nat) : m % (k * n) / n = m / n % k := by
/- First, we use the commutativity of multiplication to rewrite \( k \cdot n \) as \( n \cdot k \). Then, we apply the identity \(\frac{m \mod (n \cdot k)}{n} = \left( \frac{m}{n} \right) \mod k\) to the goal. This simplifies the goal to \(\frac{m \mod (n \cdot k)}{n} = \left( \frac{m}{n} \right) \mod k\), which is exactly what we need to prove. -/
  rw [Nat.mul_comm k n, mod_mul_right_div_self]","import Init.Omega
import Init.Data.Nat.Mod

open Nat

"
15535,Nat.mul_lt_mul_left,theorem Nat.mul_lt_mul_left (a0 : 0 < a) : a * b < a * c ↔ b < c := by sorry,"Multiplication Preserves Strict Inequality on the Left in Natural Numbers : For any natural numbers \(a\), \(b\), and \(c\) such that \(a > 0\), the inequality \(a \cdot b < a \cdot c\) holds if and only if \(b < c\).","theorem Nat.mul_lt_mul_left (a0 : 0 < a) : a * b < a * c ↔ b < c := by
  induction a with
  | zero => simp_all
  | succ a ih =>
    cases a
    · simp
    · simp_all [succ_eq_add_one, Nat.right_distrib]
      omega","We will prove the theorem by induction on \(a\).

**Base Case:**
For \(a = 0\), the assumption \(0 < 0\) is false. Therefore, the implication \(0 \cdot b < 0 \cdot c \leftrightarrow b < c\) is trivially true.

**Inductive Step:**
Assume \(a = n + 1\) and that the inductive hypothesis \(0 < n \rightarrow (n \cdot b < n \cdot c \leftrightarrow b < c)\) holds. We need to show that \((n + 1) \cdot b < (n + 1) \cdot c \leftrightarrow b < c\).

First, we use the definitions of successor and the right distributive property to simplify the goal:
\[
(n + 1) \cdot b = n \cdot b + b \quad \text{and} \quad (n + 1) \cdot c = n \cdot c + c
\]
Thus, the goal becomes:
\[
n \cdot b + b + b < n \cdot c + c + c \leftrightarrow b < c
\]

We use the inductive hypothesis \(n \cdot b < n \cdot c \leftrightarrow b < c\) and the fact that adding the same positive number to both sides of an inequality preserves the inequality. Therefore, the goal is trivially true by the properties of natural numbers and the inductive hypothesis.

By induction, the theorem holds for all natural numbers \(a > 0\). This completes the proof. \(\blacksquare\)","theorem Nat.mul_lt_mul_left (a0 : 0 < a) : a * b < a * c ↔ b < c := by
  induction a with
/- For the base case where \(a = 0\), we simplify the goal using the fact that \(0 < 0\) is false, and thus the implication \(0 \cdot b < 0 \cdot c \leftrightarrow b < c\) is trivially true. -/
  | zero => simp_all
/- We perform induction on \(a\). For the inductive step, assume \(a = n + 1\) and that the inductive hypothesis \(0 < n \rightarrow (n \cdot b < n \cdot c \leftrightarrow b < c)\) holds. We need to show that \((n + 1) \cdot b < (n + 1) \cdot c \leftrightarrow b < c\). -/
  | succ a ih =>
/- We consider the two cases for \(a\): \(a = 0\) and \(a = n + 1\). -/
    cases a
/- For the case \(a = 0\), we simplify the goal using the fact that \(0 < 0\) is false, and thus the implication \(0 \cdot b < 0 \cdot c \leftrightarrow b < c\) is trivially true. -/
    · simp
/- For the case \(a = n + 1\), we use the definitions of successor and the right distributive property to simplify the goal. This transforms the goal into \(n \cdot b + b + b < n \cdot c + c + c \leftrightarrow b < c\). -/
    · simp_all [succ_eq_add_one, Nat.right_distrib]
/- We use the omega tactic to automatically handle the arithmetic reasoning for the base case, which is trivial. -/
/- We use the omega tactic to automatically handle the arithmetic reasoning for the inductive step, which is trivial. -/
      omega","import Init.Omega
import Init.Data.Nat.Mod

open Nat

"
15541,Nat.mod_mul,theorem Nat.mod_mul {a b x : Nat} : x % (a * b) = x % a + a * (x / a % b) := by sorry,"Modulo Identity: \(x \mod (a \cdot b) = x \mod a + a \cdot (x \div a \mod b)\) : For any natural numbers \(a\), \(b\), and \(x\), the modulo operation \(x \mod (a \cdot b)\) is equal to \(x \mod a + a \cdot (x \div a \mod b)\). In other words, the remainder when \(x\) is divided by \(a \cdot b\) can be expressed as the sum of the remainder when \(x\) is divided by \(a\) and the product of \(a\) and the remainder when \(x \div a\) is divided by \(b\).","theorem Nat.mod_mul {a b x : Nat} : x % (a * b) = x % a + a * (x / a % b) := by
  rw [Nat.add_comm, ← Nat.div_add_mod (x % (a*b)) a, Nat.mod_mul_right_mod,
    Nat.mod_mul_right_div_self]","We start by using the commutativity of addition to rewrite the goal:
\[ x \% (a \cdot b) = a \cdot (x / a \% b) + x \% a. \]
Next, we apply the division and modulo identity:
\[ a \cdot (x \% (a \cdot b) / a) + x \% (a \cdot b) \% a = a \cdot (x / a \% b) + x \% a. \]
Then, we use the modulo of product modulo right argument identity to simplify the modulo operation:
\[ a \cdot (x \% (a \cdot b) / a) + x \% a = a \cdot (x / a \% b) + x \% a. \]
Finally, we apply the modulo and division identity to simplify the division operation:
\[ a \cdot (x / a \% b) + x \% a = a \cdot (x / a \% b) + x \% a. \]
Since the left-hand side and the right-hand side are now identical, the goal is trivially true. This completes the proof.","theorem Nat.mod_mul {a b x : Nat} : x % (a * b) = x % a + a * (x / a % b) := by
/- First, we use the commutativity of addition to rewrite the goal as:
\[ x \% (a \cdot b) = a \cdot (x / a \% b) + x \% a. \]
Next, we apply the division and modulo identity to rewrite the left-hand side:
\[ a \cdot (x \% (a \cdot b) / a) + x \% (a \cdot b) \% a = a \cdot (x / a \% b) + x \% a. \]
Then, we use the modulo of product modulo right argument identity to simplify the modulo operation:
\[ a \cdot (x \% (a \cdot b) / a) + x \% a = a \cdot (x / a \% b) + x \% a. \]
Finally, we apply the modulo and division identity to simplify the division operation:
\[ a \cdot (x / a \% b) + x \% a = a \cdot (x / a \% b) + x \% a. \]
Since the left-hand side and the right-hand side are now identical, the goal is trivially true. -/
  rw [Nat.add_comm, ← Nat.div_add_mod (x % (a*b)) a, Nat.mod_mul_right_mod,
    Nat.mod_mul_right_div_self]","import Init.Omega
import Init.Data.Nat.Mod

open Nat

"
16357,Bimon_.mul_counit,"theorem Bimon_.mul_counit (M : Bimon_ C) :
    M.X.mul ≫ M.counit.hom = (M.counit.hom ⊗ M.counit.hom) ≫ (λ_ _).hom := by sorry","Multiplication and Counit Compatibility in Bimonoids: \( M.\text{mul} \circ (M.\text{counit} \otimes M.\text{counit}) = \lambda_I \) : For any bimonoid \( M \) in a braided monoidal category \( C \), the following diagram commutes:
\[
\begin{tikzcd}
M.X \otimes M.X \arrow[r, ""M.\text{mul}""] \arrow[d, ""M.\text{counit} \otimes M.\text{counit}""'] & M.X \arrow[d, ""M.\text{counit}""] \\
I \otimes I \arrow[r, ""\lambda_I""'] & I
\end{tikzcd}
\]
where:
- \( M.\text{mul} \) is the multiplication morphism of the monoid \( M \).
- \( M.\text{counit} \) is the counit morphism of the comonoid \( M \).
- \( I \) is the tensor unit object in the monoidal category \( C \).
- \( \lambda_I \) is the left unitor isomorphism in \( C \).","theorem Bimon_.mul_counit (M : Bimon_ C) :
    M.X.mul ≫ M.counit.hom = (M.counit.hom ⊗ M.counit.hom) ≫ (λ_ _).hom := by
  simp","To prove that the given diagram commutes, we need to show that:
\[
M.\text{mul} \circ (M.\text{counit} \otimes M.\text{counit}) = \lambda_I
\]

We start by using the properties of the monoidal category and the definitions of the morphisms involved. Specifically, we use the following properties and definitions:
1. The left unitor \( \lambda_X : I \otimes X \cong X \) for any object \( X \) in \( C \).
2. The multiplication morphism \( M.\text{mul} : M.X \otimes M.X \to M.X \).
3. The counit morphism \( M.\text{counit} : M.X \to I \).

By simplifying the left-hand side of the equation using these properties, we get:
\[
M.\text{mul} \circ (M.\text{counit} \otimes M.\text{counit}) : M.X \otimes M.X \to I
\]

Since \( M.\text{counit} \) maps \( M.X \) to the tensor unit \( I \), the tensor product \( M.\text{counit} \otimes M.\text{counit} \) maps \( M.X \otimes M.X \) to \( I \otimes I \). The left unitor \( \lambda_I \) then maps \( I \otimes I \) to \( I \). Therefore, we have:
\[
M.\text{mul} \circ (M.\text{counit} \otimes M.\text{counit}) = \lambda_I
\]

This shows that the given diagram commutes, completing the proof.","theorem Bimon_.mul_counit (M : Bimon_ C) :
    M.X.mul ≫ M.counit.hom = (M.counit.hom ⊗ M.counit.hom) ≫ (λ_ _).hom := by
/- Using the properties of the monoidal category and the definitions of the morphisms involved, we simplify the proposition we want to show. Specifically, we use the properties of the left unitor and the definitions of the multiplication and counit morphisms to show that the given diagram commutes. -/
  simp","import Mathlib.CategoryTheory.Monoidal.Comon_
import Mathlib.CategoryTheory.Monoidal.Bimon_

open Bimon_
open CategoryTheory MonoidalCategory
variable (C : Type u₁) [Category.{v₁} C] [MonoidalCategory.{v₁} C] [BraidedCategory C]
variable {C}
"
16796,HasDerivAtFilter.add,"theorem HasDerivAtFilter.add (hf : HasDerivAtFilter f f' x L)
    (hg : HasDerivAtFilter g g' x L) : HasDerivAtFilter (fun y => f y + g y) (f' + g') x L := by sorry","Derivative of Sum of Functions along a Filter: \((f + g)' = f' + g'\) : For any functions \( f, g : \mathbb{𝕜} \to F \) from a nontrivially normed field \(\mathbb{𝕜}\) to a normed space \( F \) over \(\mathbb{𝕜}\), and for any point \( x \in \mathbb{𝕜} \) and filter \( L \) on \(\mathbb{𝕜}\), if \( f \) has the derivative \( f' \) at \( x \) along the filter \( L \) and \( g \) has the derivative \( g' \) at \( x \) along the filter \( L \), then the function \( f + g \) has the derivative \( f' + g' \) at \( x \) along the filter \( L \).","theorem HasDerivAtFilter.add (hf : HasDerivAtFilter f f' x L)
    (hg : HasDerivAtFilter g g' x L) : HasDerivAtFilter (fun y => f y + g y) (f' + g') x L := by
  simpa using (hf.add hg).hasDerivAtFilter","We start with the given conditions that \( f \) has the derivative \( f' \) at \( x \) along the filter \( L \) and \( g \) has the derivative \( g' \) at \( x \) along the filter \( L \). By the properties of Fréchet derivatives, if \( f \) and \( g \) have Fréchet derivatives at \( x \) along the filter \( L \), then the function \( f + g \) also has a Fréchet derivative at \( x \) along the filter \( L \). Specifically, the Fréchet derivative of \( f + g \) at \( x \) is the sum of the Fréchet derivatives of \( f \) and \( g \) at \( x \).

Using the fact that the Fréchet derivative of \( f + g \) at \( x \) is \( f' + g' \), we can simplify the goal. Since \( f \) has the derivative \( f' \) at \( x \) along the filter \( L \) and \( g \) has the derivative \( g' \) at \( x \) along the filter \( L \), the function \( f + g \) has the derivative \( f' + g' \) at \( x \) along the filter \( L \). This simplification shows that the goal is exactly what we need to prove, thus finishing the proof.

\(\blacksquare\)","theorem HasDerivAtFilter.add (hf : HasDerivAtFilter f f' x L)
    (hg : HasDerivAtFilter g g' x L) : HasDerivAtFilter (fun y => f y + g y) (f' + g') x L := by
/- Using the fact that the sum of two functions with Fréchet derivatives at a point along a filter also has a Fréchet derivative, we can simplify the goal. Specifically, since \( f \) has the derivative \( f' \) at \( x \) along the filter \( L \) and \( g \) has the derivative \( g' \) at \( x \) along the filter \( L \), the function \( f + g \) has the derivative \( f' + g' \) at \( x \) along the filter \( L \). This simplification shows that the goal is exactly what we need to prove, thus finishing the proof. -/
  simpa using (hf.add hg).hasDerivAtFilter","import Mathlib.Analysis.Calculus.Deriv.Basic
import Mathlib.Analysis.Calculus.FDeriv.Add
import Mathlib.Analysis.Calculus.Deriv.Add

open HasDerivAtFilter
open scoped Classical
open scoped Topology Filter ENNReal
open Asymptotics Set
variable {𝕜 : Type u} [NontriviallyNormedField 𝕜]
variable {F : Type v} [NormedAddCommGroup F] [NormedSpace 𝕜 F]
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L : Filter 𝕜}
variable {F : Type v} [NormedAddCommGroup F] [NormedSpace 𝕜 F]
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L : Filter 𝕜}
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L : Filter 𝕜}
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L : Filter 𝕜}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L : Filter 𝕜}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L : Filter 𝕜}
variable {s t : Set 𝕜}
variable {L : Filter 𝕜}
variable {L : Filter 𝕜}
"
17015,HasStrictDerivAt.comp,"theorem HasStrictDerivAt.comp (hh₂ : HasStrictDerivAt h₂ h₂' (h x)) (hh : HasStrictDerivAt h h' x) :
    HasStrictDerivAt (h₂ ∘ h) (h₂' * h') x := by sorry","Chain Rule for Strict Derivatives in Normed Algebras: \((h_2 \circ h)'(x) = h_2'(h(x)) \cdot h'(x)\) : For any nontrivially normed fields \(\mathbb{K}\) and \(\mathbb{K}'\) with \(\mathbb{K}'\) being a normed algebra over \(\mathbb{K}\), and for any functions \( h : \mathbb{K} \to \mathbb{K}' \) and \( h_2 : \mathbb{K}' \to \mathbb{K}' \), if \( h_2 \) has a strict derivative \( h_2' \) at \( h(x) \) and \( h \) has a strict derivative \( h' \) at \( x \), then the composition \( h_2 \circ h \) has a strict derivative \( h_2' \cdot h' \) at \( x \).","theorem HasStrictDerivAt.comp (hh₂ : HasStrictDerivAt h₂ h₂' (h x)) (hh : HasStrictDerivAt h h' x) :
    HasStrictDerivAt (h₂ ∘ h) (h₂' * h') x := by
  rw [mul_comm]
  exact hh₂.scomp x hh","We start by noting that the multiplication in a commutative magma is commutative. Therefore, we can rewrite the goal to show that the strict derivative of \( h_2 \circ h \) at \( x \) is \( h' \cdot h_2' \) instead of \( h_2' \cdot h' \).

Next, we apply the chain rule for strict derivatives. According to the chain rule, if \( h_2 \) has a strict derivative \( h_2' \) at \( h(x) \) and \( h \) has a strict derivative \( h' \) at \( x \), then the composition \( h_2 \circ h \) has a strict derivative \( h' \cdot h_2' \) at \( x \).

Thus, the strict derivative of \( h_2 \circ h \) at \( x \) is indeed \( h' \cdot h_2' \). This completes the proof. \(\blacksquare\)","theorem HasStrictDerivAt.comp (hh₂ : HasStrictDerivAt h₂ h₂' (h x)) (hh : HasStrictDerivAt h h' x) :
    HasStrictDerivAt (h₂ ∘ h) (h₂' * h') x := by
/- Since the multiplication in a commutative magma is commutative, we can rewrite the goal to show that the strict derivative of \( h_2 \circ h \) at \( x \) is \( h' \cdot h_2' \) instead of \( h_2' \cdot h' \). -/
  rw [mul_comm]
/- By the chain rule for strict derivatives, if \( h_2 \) has a strict derivative \( h_2' \) at \( h(x) \) and \( h \) has a strict derivative \( h' \) at \( x \), then the composition \( h_2 \circ h \) has a strict derivative \( h' \cdot h_2' \) at \( x \). This completes the proof. -/
  exact hh₂.scomp x hh","import Mathlib.Analysis.Calculus.Deriv.Basic
import Mathlib.Analysis.Calculus.FDeriv.Comp
import Mathlib.Analysis.Calculus.FDeriv.RestrictScalars
import Mathlib.Analysis.Calculus.Deriv.Comp

open HasStrictDerivAt
open scoped Classical Topology Filter ENNReal
open Filter Asymptotics Set
open ContinuousLinearMap (smulRight smulRight_one_eq_iff)
variable {𝕜 : Type u} [NontriviallyNormedField 𝕜]
variable {F : Type v} [NormedAddCommGroup F] [NormedSpace 𝕜 F]
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {F : Type v} [NormedAddCommGroup F] [NormedSpace 𝕜 F]
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {𝕜' : Type*} [NontriviallyNormedField 𝕜'] [NormedAlgebra 𝕜 𝕜'] [NormedSpace 𝕜' F]
  [IsScalarTower 𝕜 𝕜' F] {s' t' : Set 𝕜'} {h : 𝕜 → 𝕜'} {h₁ : 𝕜 → 𝕜} {h₂ : 𝕜' → 𝕜'} {h' h₂' : 𝕜'}
  {h₁' : 𝕜} {g₁ : 𝕜' → F} {g₁' : F} {L' : Filter 𝕜'} {y : 𝕜'} (x)
"
17029,HasDerivWithinAt.comp,"theorem HasDerivWithinAt.comp (hh₂ : HasDerivWithinAt h₂ h₂' s' (h x))
    (hh : HasDerivWithinAt h h' s x) (hst : MapsTo h s s') :
    HasDerivWithinAt (h₂ ∘ h) (h₂' * h') s x := by sorry","Chain Rule for Derivatives within a Subset: \( (h_2 \circ h)'(x) = h_2'(h(x)) \cdot h'(x) \) : For any normed field \(\mathbb{K}\) and any normed algebra \(\mathbb{K}'\) over \(\mathbb{K}\), if \( h : \mathbb{K} \to \mathbb{K}' \) and \( h_2 : \mathbb{K}' \to \mathbb{K}' \) are functions, and \( h \) has a derivative \( h' \) at \( x \) within a subset \( s \) of \(\mathbb{K}\), and \( h_2 \) has a derivative \( h_2' \) at \( h(x) \) within a subset \( s' \) of \(\mathbb{K}'\), and \( h \) maps \( s \) to \( s' \), then the composition \( h_2 \circ h \) has a derivative \( h_2' \cdot h' \) at \( x \) within \( s \).","theorem HasDerivWithinAt.comp (hh₂ : HasDerivWithinAt h₂ h₂' s' (h x))
    (hh : HasDerivWithinAt h h' s x) (hst : MapsTo h s s') :
    HasDerivWithinAt (h₂ ∘ h) (h₂' * h') s x := by
  rw [mul_comm]
  exact hh₂.scomp x hh hst","We start by noting that the multiplication in a commutative magma is commutative, so we can rewrite the goal from \( h_2' \cdot h' \) to \( h' \cdot h_2' \).

Next, we apply the chain rule for the composition of functions with derivatives within a subset. Specifically, since \( h_2 \) has a derivative \( h_2' \) at \( h(x) \) within \( s' \), and \( h \) has a derivative \( h' \) at \( x \) within \( s \), and \( h \) maps \( s \) to \( s' \), the composition \( h_2 \circ h \) has a derivative \( h' \cdot h_2' \) at \( x \) within \( s \).

Thus, the composition \( h_2 \circ h \) has a derivative \( h_2' \cdot h' \) at \( x \) within \( s \), which completes the proof. \(\blacksquare\)","theorem HasDerivWithinAt.comp (hh₂ : HasDerivWithinAt h₂ h₂' s' (h x))
    (hh : HasDerivWithinAt h h' s x) (hst : MapsTo h s s') :
    HasDerivWithinAt (h₂ ∘ h) (h₂' * h') s x := by
/- Since the multiplication in a commutative magma is commutative, we can rewrite the goal from \( h_2' \cdot h' \) to \( h' \cdot h_2' \). -/
  rw [mul_comm]
/- The current goal is exactly proved by the chain rule for the composition of functions with derivatives within a subset. Specifically, since \( h_2 \) has a derivative \( h_2' \) at \( h(x) \) within \( s' \), and \( h \) has a derivative \( h' \) at \( x \) within \( s \), and \( h \) maps \( s \) to \( s' \), the composition \( h_2 \circ h \) has a derivative \( h' \cdot h_2' \) at \( x \) within \( s \). -/
  exact hh₂.scomp x hh hst","import Mathlib.Analysis.Calculus.Deriv.Basic
import Mathlib.Analysis.Calculus.FDeriv.Comp
import Mathlib.Analysis.Calculus.FDeriv.RestrictScalars
import Mathlib.Analysis.Calculus.Deriv.Comp

open HasDerivWithinAt
open scoped Classical Topology Filter ENNReal
open Filter Asymptotics Set
open ContinuousLinearMap (smulRight smulRight_one_eq_iff)
variable {𝕜 : Type u} [NontriviallyNormedField 𝕜]
variable {F : Type v} [NormedAddCommGroup F] [NormedSpace 𝕜 F]
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {F : Type v} [NormedAddCommGroup F] [NormedSpace 𝕜 F]
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {𝕜' : Type*} [NontriviallyNormedField 𝕜'] [NormedAlgebra 𝕜 𝕜'] [NormedSpace 𝕜' F]
  [IsScalarTower 𝕜 𝕜' F] {s' t' : Set 𝕜'} {h : 𝕜 → 𝕜'} {h₁ : 𝕜 → 𝕜} {h₂ : 𝕜' → 𝕜'} {h' h₂' : 𝕜'}
  {h₁' : 𝕜} {g₁ : 𝕜' → F} {g₁' : F} {L' : Filter 𝕜'} {y : 𝕜'} (x)
"
17030,HasDerivAtFilter.comp,"theorem HasDerivAtFilter.comp (hh₂ : HasDerivAtFilter h₂ h₂' (h x) L')
    (hh : HasDerivAtFilter h h' x L) (hL : Tendsto h L L') :
    HasDerivAtFilter (h₂ ∘ h) (h₂' * h') x L := by sorry","Chain Rule for Derivatives along Filters: \((h₂ \circ h)'(x) = h₂'(h(x)) \cdot h'(x)\) : For any normed fields \(\mathbb{𝕜}\) and \(\mathbb{𝕜}'\) with \(\mathbb{𝕜}'\) being a normed algebra over \(\mathbb{𝕜}\), and for any functions \( h : \mathbb{𝕜} \to \mathbb{𝕜}' \) and \( h₂ : \mathbb{𝕜}' \to \mathbb{𝕜}' \), if \( h \) has a derivative \( h' \) at \( x \) along the filter \( L \), and \( h₂ \) has a derivative \( h₂' \) at \( h(x) \) along the filter \( L' \), and \( h \) tends to \( h(x) \) along the filter \( L \) to \( L' \), then the composition \( h₂ \circ h \) has a derivative \( h₂' \cdot h' \) at \( x \) along the filter \( L \).","theorem HasDerivAtFilter.comp (hh₂ : HasDerivAtFilter h₂ h₂' (h x) L')
    (hh : HasDerivAtFilter h h' x L) (hL : Tendsto h L L') :
    HasDerivAtFilter (h₂ ∘ h) (h₂' * h') x L := by
  rw [mul_comm]
  exact hh₂.scomp x hh hL","We start by noting that the multiplication in a commutative magma is commutative, so we can rewrite the goal to show that the derivative of the composition \( h₂ \circ h \) at \( x \) along the filter \( L \) is \( h' \cdot h₂' \) instead of \( h₂' \cdot h' \).

Next, we apply the chain rule for scalar composition of functions with derivatives along filters. Specifically, since \( h \) has a derivative \( h' \) at \( x \) along the filter \( L \), and \( h₂ \) has a derivative \( h₂' \) at \( h(x) \) along the filter \( L' \), and \( h \) tends to \( h(x) \) along the filter \( L \) to \( L' \), the composition \( h₂ \circ h \) has a derivative \( h' \cdot h₂' \) at \( x \) along the filter \( L \).

Thus, the theorem is proved. \(\blacksquare\)","theorem HasDerivAtFilter.comp (hh₂ : HasDerivAtFilter h₂ h₂' (h x) L')
    (hh : HasDerivAtFilter h h' x L) (hL : Tendsto h L L') :
    HasDerivAtFilter (h₂ ∘ h) (h₂' * h') x L := by
/- Since the multiplication in a commutative magma is commutative, we can rewrite the goal to show that the derivative of the composition \( h₂ \circ h \) at \( x \) along the filter \( L \) is \( h' \cdot h₂' \) instead of \( h₂' \cdot h' \). -/
  rw [mul_comm]
/- The current goal is exactly proved by the chain rule for scalar composition of functions with derivatives along filters. Specifically, since \( h \) has a derivative \( h' \) at \( x \) along the filter \( L \), and \( h₂ \) has a derivative \( h₂' \) at \( h(x) \) along the filter \( L' \), and \( h \) tends to \( h(x) \) along the filter \( L \) to \( L' \), the composition \( h₂ \circ h \) has a derivative \( h' \cdot h₂' \) at \( x \) along the filter \( L \). -/
  exact hh₂.scomp x hh hL","import Mathlib.Analysis.Calculus.Deriv.Basic
import Mathlib.Analysis.Calculus.FDeriv.Comp
import Mathlib.Analysis.Calculus.FDeriv.RestrictScalars
import Mathlib.Analysis.Calculus.Deriv.Comp

open HasDerivAtFilter
open scoped Classical Topology Filter ENNReal
open Filter Asymptotics Set
open ContinuousLinearMap (smulRight smulRight_one_eq_iff)
variable {𝕜 : Type u} [NontriviallyNormedField 𝕜]
variable {F : Type v} [NormedAddCommGroup F] [NormedSpace 𝕜 F]
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {F : Type v} [NormedAddCommGroup F] [NormedSpace 𝕜 F]
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {𝕜' : Type*} [NontriviallyNormedField 𝕜'] [NormedAlgebra 𝕜 𝕜'] [NormedSpace 𝕜' F]
  [IsScalarTower 𝕜 𝕜' F] {s' t' : Set 𝕜'} {h : 𝕜 → 𝕜'} {h₁ : 𝕜 → 𝕜} {h₂ : 𝕜' → 𝕜'} {h' h₂' : 𝕜'}
  {h₁' : 𝕜} {g₁ : 𝕜' → F} {g₁' : F} {L' : Filter 𝕜'} {y : 𝕜'} (x)
"
17503,Subgroup.mem_centralizer_iff_commutator_eq_one,"theorem Subgroup.mem_centralizer_iff_commutator_eq_one {g : G} {s : Set G} :
    g ∈ centralizer s ↔ ∀ h ∈ s, h * g * h⁻¹ * g⁻¹ = 1 := by sorry","Element in Centralizer if and only if Commutator Equals Identity: \( g \in \text{centralizer}(S) \leftrightarrow \forall h \in S, h * g * h^{-1} * g^{-1} = 1 \) : For any group \( G \) and any subset \( S \subseteq G \), an element \( g \in G \) is in the centralizer of \( S \) if and only if for all \( h \in S \), the commutator \( h * g * h^{-1} * g^{-1} \) is equal to the identity element \( 1 \). Formally, \( g \in \text{centralizer}(S) \) if and only if \( \forall h \in S, h * g * h^{-1} * g^{-1} = 1 \).","theorem Subgroup.mem_centralizer_iff_commutator_eq_one {g : G} {s : Set G} :
    g ∈ centralizer s ↔ ∀ h ∈ s, h * g * h⁻¹ * g⁻¹ = 1 := by
  simp only [mem_centralizer_iff, mul_inv_eq_iff_eq_mul, one_mul]","To prove the theorem, we start by using the definition of the centralizer and the properties of group elements. Specifically, we use the following steps:

1. **Simplification:**
- We use the definition of the centralizer, which states that \( g \in \text{centralizer}(S) \) if and only if for all \( h \in S \), \( h \cdot g \cdot h^{-1} = g \).
- We also use the property that \( a \cdot b^{-1} = c \) if and only if \( a = c \cdot b \).
- Additionally, we use the property that \( 1 \cdot a = a \).

2. **Equivalence:**
- Using these properties, we can simplify the condition \( h \cdot g \cdot h^{-1} = g \) to \( h \cdot g \cdot h^{-1} \cdot g^{-1} = 1 \).

3. **Conclusion:**
- Therefore, \( g \in \text{centralizer}(S) \) if and only if for all \( h \in S \), \( h \cdot g \cdot h^{-1} \cdot g^{-1} = 1 \).

This completes the proof.","theorem Subgroup.mem_centralizer_iff_commutator_eq_one {g : G} {s : Set G} :
    g ∈ centralizer s ↔ ∀ h ∈ s, h * g * h⁻¹ * g⁻¹ = 1 := by
/- Using the definition of the centralizer and the properties of group elements, we can simplify the proposition we want to show. Specifically, we use the fact that an element \( g \) is in the centralizer of a set \( S \) if and only if for all \( h \in S \), \( h \cdot g \cdot h^{-1} = g \). Additionally, we use the property that \( a \cdot b^{-1} = c \) if and only if \( a = c \cdot b \), and the property that \( 1 \cdot a = a \). After simplification, we get the equivalent statement that \( g \in \text{centralizer}(S) \) if and only if for all \( h \in S \), \( h \cdot g \cdot h^{-1} \cdot g^{-1} = 1 \). -/
  simp only [mem_centralizer_iff, mul_inv_eq_iff_eq_mul, one_mul]","import Mathlib.GroupTheory.Subgroup.Center
import Mathlib.GroupTheory.Submonoid.Centralizer
import Mathlib.GroupTheory.Subgroup.Centralizer

open Subgroup
variable {G : Type*} [Group G]
variable {H K : Subgroup G}
"
17927,AlgebraicTopology.DoldKan.PInftyToNormalizedMooreComplex_comp_inclusionOfMooreComplexMap,"theorem AlgebraicTopology.DoldKan.PInftyToNormalizedMooreComplex_comp_inclusionOfMooreComplexMap (X : SimplicialObject A) :
    PInftyToNormalizedMooreComplex X ≫ inclusionOfMooreComplexMap X = PInfty := by sorry","For any simplicial object $X$ in an abelian category $A$, the composition of the map from the infinite projective space associated with $X$ to its normalized Moore complex, followed by the inclusion map from the Moore complex to the infinite projective space, is equal to the map from the infinite projective space to itself. In short, we have $(PInftyToNormalizedMooreComplex X) \circ (inclusionOfMooreComplexMap X) = PInfty$.","theorem AlgebraicTopology.DoldKan.PInftyToNormalizedMooreComplex_comp_inclusionOfMooreComplexMap (X : SimplicialObject A) :
    PInftyToNormalizedMooreComplex X ≫ inclusionOfMooreComplexMap X = PInfty := by
  aesop_cat","By category theory facts, we have that the composition of the map from the infinite projective space associated with $X$ to its normalized Moore complex, followed by the inclusion map from the Moore complex to the infinite projective space, is equal to the map from the infinite projective space to itself. In other words, $(PInftyToNormalizedMooreComplex X) \circ (inclusionOfMooreComplexMap X) = PInfty$. This completes the proof.","theorem AlgebraicTopology.DoldKan.PInftyToNormalizedMooreComplex_comp_inclusionOfMooreComplexMap (X : SimplicialObject A) :
    PInftyToNormalizedMooreComplex X ≫ inclusionOfMooreComplexMap X = PInfty := by
  aesop_cat","import Mathlib.AlgebraicTopology.DoldKan.FunctorN
import Mathlib.AlgebraicTopology.DoldKan.Normalized

open AlgebraicTopology
open DoldKan
open CategoryTheory CategoryTheory.Category CategoryTheory.Limits
variable {A : Type*} [Category A] [Abelian A] {X : SimplicialObject A}
"
17931,AlgebraicTopology.DoldKan.PInfty_comp_PInftyToNormalizedMooreComplex,"theorem AlgebraicTopology.DoldKan.PInfty_comp_PInftyToNormalizedMooreComplex (X : SimplicialObject A) :
    PInfty ≫ PInftyToNormalizedMooreComplex X = PInftyToNormalizedMooreComplex X := by sorry","For any simplicial object $X$ in an abelian category $A$, show that the composition of the infinite product functor $P^\infty$ with the functor $P^{\infty}_{\text{ToNormalizedMooreComplex}}$ applied to $X$ is equal to the functor $P^{\infty}_{\text{ToNormalizedMooreComplex}}$ applied directly to $X$, i.e., $P^\infty \circ P^{\infty}_{\text{ToNormalizedMooreComplex}} X = P^{\infty}_{\text{ToNormalizedMooreComplex}} X$.","theorem AlgebraicTopology.DoldKan.PInfty_comp_PInftyToNormalizedMooreComplex (X : SimplicialObject A) :
    PInfty ≫ PInftyToNormalizedMooreComplex X = PInftyToNormalizedMooreComplex X := by
  aesop_cat","To prove the theorem, we use category theory facts. Specifically, we show that the composition of the infinite product functor $P^\infty$ with the functor $P^{\infty}_{\text{ToNormalizedMooreComplex}}$ applied to $X$ is equal to the functor $P^{\infty}_{\text{ToNormalizedMooreComplex}}$ applied directly to $X$. By the properties of functors and the structure of the abelian category $A$, we can conclude that:
\[
P^\infty \circ P^{\infty}_{\text{ToNormalizedMooreComplex}} X = P^{\infty}_{\text{ToNormalizedMooreComplex}} X.
\]
This completes the proof.","theorem AlgebraicTopology.DoldKan.PInfty_comp_PInftyToNormalizedMooreComplex (X : SimplicialObject A) :
    PInfty ≫ PInftyToNormalizedMooreComplex X = PInftyToNormalizedMooreComplex X := by
  aesop_cat","import Mathlib.AlgebraicTopology.DoldKan.FunctorN
import Mathlib.AlgebraicTopology.DoldKan.Normalized

open AlgebraicTopology
open DoldKan
open CategoryTheory CategoryTheory.Category CategoryTheory.Limits
variable {A : Type*} [Category A] [Abelian A] {X : SimplicialObject A}
"
17939,AlgebraicTopology.DoldKan.inclusionOfMooreComplexMap_comp_PInfty,"theorem AlgebraicTopology.DoldKan.inclusionOfMooreComplexMap_comp_PInfty (X : SimplicialObject A) :
    inclusionOfMooreComplexMap X ≫ PInfty = inclusionOfMooreComplexMap X := by sorry","For any simplicial object $X$ in an abelian category $A$, the composition of the inclusion map of the Moore complex of $X$ with the $P^{\infty}$ map is equal to the inclusion map of the Moore complex of $X$.","theorem AlgebraicTopology.DoldKan.inclusionOfMooreComplexMap_comp_PInfty (X : SimplicialObject A) :
    inclusionOfMooreComplexMap X ≫ PInfty = inclusionOfMooreComplexMap X := by
  ext (_|n)
  · dsimp
    simp only [comp_id]
  · exact (HigherFacesVanish.inclusionOfMooreComplexMap n).comp_P_eq_self","To prove the equality of the natural transformations, we need to show that the components at each degree \( n \) are equal. Specifically, we need to show that for \( n = 0 \) and for any \( n \in \mathbb{N} \), the components of the natural transformations are equal.

1. **Component at degree \( 0 \):**
- We simplify the expression for the component at degree \( 0 \). This simplification shows that the component of the composition of the inclusion map of the Moore complex with \( P^{\infty} \) at degree \( 0 \) is equal to the component of the inclusion map of the Moore complex at degree \( 0 \).
- Using the property that the composition of a morphism with the identity morphism is the morphism itself, we simplify the expression to show that the component of the inclusion map of the Moore complex at degree \( 0 \) is indeed equal to itself. This completes the proof for the component at degree \( 0 \).

2. **Component at degree \( n + 1 \):**
- For the component at degree \( n + 1 \), we use the fact that the higher faces vanish for the inclusion map of the Moore complex at degree \( n + 1 \). This implies that the composition of the inclusion map of the Moore complex with \( P^{\infty} \) at degree \( n + 1 \) is equal to the inclusion map of the Moore complex at degree \( n + 1 \).
- This completes the proof for the component at degree \( n + 1 \).

Since we have shown that the components at all degrees are equal, the natural transformations are equal. Therefore, the composition of the inclusion map of the Moore complex with \( P^{\infty} \) is equal to the inclusion map of the Moore complex. This completes the proof. \(\blacksquare\)","theorem AlgebraicTopology.DoldKan.inclusionOfMooreComplexMap_comp_PInfty (X : SimplicialObject A) :
    inclusionOfMooreComplexMap X ≫ PInfty = inclusionOfMooreComplexMap X := by
/- To prove the equality of the natural transformations, we need to show that the components at each degree \( n \) are equal. Specifically, we need to show that for \( n = 0 \) and for any \( n \in \mathbb{N} \), the components of the natural transformations are equal. -/
  ext (_|n)
/- First, we simplify the expression for the component at degree \( 0 \). This simplification shows that the component of the composition of the inclusion map of the Moore complex with \( P^{\infty} \) at degree \( 0 \) is equal to the component of the inclusion map of the Moore complex at degree \( 0 \). -/
  · dsimp
/- Using the property that the composition of a morphism with the identity morphism is the morphism itself, we simplify the expression to show that the component of the inclusion map of the Moore complex at degree \( 0 \) is indeed equal to itself. This completes the proof for the component at degree \( 0 \). -/
    simp only [comp_id]
/- For the component at degree \( n + 1 \), we use the fact that the higher faces vanish for the inclusion map of the Moore complex at degree \( n + 1 \). This implies that the composition of the inclusion map of the Moore complex with \( P^{\infty} \) at degree \( n + 1 \) is equal to the inclusion map of the Moore complex at degree \( n + 1 \). This completes the proof for the component at degree \( n + 1 \). -/
  · exact (HigherFacesVanish.inclusionOfMooreComplexMap n).comp_P_eq_self","import Mathlib.AlgebraicTopology.DoldKan.FunctorN
import Mathlib.AlgebraicTopology.DoldKan.Normalized

open AlgebraicTopology
open DoldKan
open CategoryTheory CategoryTheory.Category CategoryTheory.Limits
variable {A : Type*} [Category A] [Abelian A] {X : SimplicialObject A}
"
18503,UniformConcaveOn.sub,"theorem UniformConcaveOn.sub (hf : UniformConcaveOn s φ f) (hg : UniformConvexOn s ψ g) :
    UniformConcaveOn s (φ + ψ) (f - g) := by sorry","Uniform Concavity of Difference of Uniformly Concave and Uniformly Convex Functions: \( \text{UniformConcaveOn.sub} \) : For a real normed space \( E \), if \( f : E \to \mathbb{R} \) is uniformly concave on a set \( S \subseteq E \) with modulus \( \varphi : \mathbb{R} \to \mathbb{R} \) and \( g : E \to \mathbb{R} \) is uniformly convex on \( S \) with modulus \( \psi : \mathbb{R} \to \mathbb{R} \), then the function \( f - g \) is uniformly concave on \( S \) with modulus \( \varphi + \psi \).","theorem UniformConcaveOn.sub (hf : UniformConcaveOn s φ f) (hg : UniformConvexOn s ψ g) :
    UniformConcaveOn s (φ + ψ) (f - g) := by
  simpa using hf.add hg.neg","We need to show that \( f - g \) is uniformly concave on \( S \) with modulus \( \varphi + \psi \).

1. **Step 1:**
- We know that \( f \) is uniformly concave on \( S \) with modulus \( \varphi \).
- We also know that \( g \) is uniformly convex on \( S \) with modulus \( \psi \).
- By the property of uniformly convex functions, the negation of a uniformly convex function is uniformly concave with the same modulus. Therefore, \( -g \) is uniformly concave on \( S \) with modulus \( \psi \).

2. **Step 2:**
- Using the property that the sum of uniformly concave functions is uniformly concave with the sum of their moduli, we conclude that \( f + (-g) = f - g \) is uniformly concave on \( S \) with modulus \( \varphi + \psi \).

Thus, the function \( f - g \) is uniformly concave on \( S \) with modulus \( \varphi + \psi \). This completes the proof.","theorem UniformConcaveOn.sub (hf : UniformConcaveOn s φ f) (hg : UniformConvexOn s ψ g) :
    UniformConcaveOn s (φ + ψ) (f - g) := by
  simpa using hf.add hg.neg","import Mathlib.Analysis.InnerProductSpace.Basic
import Mathlib.Analysis.Convex.Strong

open UniformConcaveOn
open Real
variable {E : Type*} [NormedAddCommGroup E]
variable [NormedSpace ℝ E] {φ ψ : ℝ → ℝ} {s : Set E} {a b m : ℝ} {x y : E} {f g : E → ℝ}
"
18511,UniformConcaveOn.add,"theorem UniformConcaveOn.add (hf : UniformConcaveOn s φ f) (hg : UniformConcaveOn s ψ g) :
    UniformConcaveOn s (φ + ψ) (f + g) := by sorry","Sum of Uniformly Concave Functions is Uniformly Concave with Sum of Moduli : For any real normed space \( E \), if \( f \) and \( g \) are uniformly concave functions on a set \( S \subseteq E \) with moduli \( \varphi \) and \( \psi \) respectively, then the function \( f + g \) is uniformly concave on \( S \) with modulus \( \varphi + \psi \).","theorem UniformConcaveOn.add (hf : UniformConcaveOn s φ f) (hg : UniformConcaveOn s ψ g) :
    UniformConcaveOn s (φ + ψ) (f + g) := by
  refine ⟨hf.1, fun x hx y hy a b ha hb hab ↦ ?_⟩
  simpa [mul_add, add_add_add_comm] using add_le_add (hf.2 hx hy ha hb hab) (hg.2 hx hy ha hb hab)","To prove that \( f + g \) is uniformly concave on \( S \) with modulus \( \varphi + \psi \), we need to show that for any \( x, y \in S \) and any \( a, b \in \mathbb{R} \) such that \( 0 \leq a \), \( 0 \leq b \), and \( a + b = 1 \), the following inequality holds:
\[ a \cdot (f + g)(x) + b \cdot (f + g)(y) + a \cdot b \cdot (\varphi + \psi)(\|x - y\|) \leq (f + g)(a \cdot x + b \cdot y). \]

Since \( f \) is uniformly concave with modulus \( \varphi \), we have:
\[ a \cdot f(x) + b \cdot f(y) + a \cdot b \cdot \varphi(\|x - y\|) \leq f(a \cdot x + b \cdot y). \]

Similarly, since \( g \) is uniformly concave with modulus \( \psi \), we have:
\[ a \cdot g(x) + b \cdot g(y) + a \cdot b \cdot \psi(\|x - y\|) \leq g(a \cdot x + b \cdot y). \]

By adding these two inequalities, we get:
\[ a \cdot f(x) + b \cdot f(y) + a \cdot b \cdot \varphi(\|x - y\|) + a \cdot g(x) + b \cdot g(y) + a \cdot b \cdot \psi(\|x - y\|) \leq f(a \cdot x + b \cdot y) + g(a \cdot x + b \cdot y). \]

Using the distributive property of multiplication over addition and the commutativity of addition, we can rewrite the left-hand side as:
\[ a \cdot (f(x) + g(x)) + b \cdot (f(y) + g(y)) + a \cdot b \cdot (\varphi(\|x - y\|) + \psi(\|x - y\|)). \]

Thus, we have:
\[ a \cdot (f + g)(x) + b \cdot (f + g)(y) + a \cdot b \cdot (\varphi + \psi)(\|x - y\|) \leq (f + g)(a \cdot x + b \cdot y). \]

This completes the proof. \(\blacksquare\)","theorem UniformConcaveOn.add (hf : UniformConcaveOn s φ f) (hg : UniformConcaveOn s ψ g) :
    UniformConcaveOn s (φ + ψ) (f + g) := by
/- To prove that \( f + g \) is uniformly concave on \( S \) with modulus \( \varphi + \psi \), we need to show that for any \( x, y \in S \) and any \( a, b \in \mathbb{R} \) such that \( 0 \leq a \), \( 0 \leq b \), and \( a + b = 1 \), the following inequality holds:
\[ a \cdot (f + g)(x) + b \cdot (f + g)(y) + a \cdot b \cdot (\varphi + \psi)(\|x - y\|) \leq (f + g)(a \cdot x + b \cdot y). \]
We will prove this inequality by using the uniform concavity of \( f \) and \( g \) with moduli \( \varphi \) and \( \psi \) respectively. -/
  refine ⟨hf.1, fun x hx y hy a b ha hb hab ↦ ?_⟩
/- Using the uniform concavity of \( f \) and \( g \), we have:
\[ a \cdot f(x) + b \cdot f(y) + a \cdot b \cdot \varphi(\|x - y\|) \leq f(a \cdot x + b \cdot y) \]
and
\[ a \cdot g(x) + b \cdot g(y) + a \cdot b \cdot \psi(\|x - y\|) \leq g(a \cdot x + b \cdot y). \]
By adding these two inequalities, we get:
\[ a \cdot f(x) + b \cdot f(y) + a \cdot b \cdot \varphi(\|x - y\|) + a \cdot g(x) + b \cdot g(y) + a \cdot b \cdot \psi(\|x - y\|) \leq f(a \cdot x + b \cdot y) + g(a \cdot x + b \cdot y). \]
Using the distributive property of multiplication over addition and the commutativity of addition, we can rewrite the left-hand side as:
\[ a \cdot (f(x) + g(x)) + b \cdot (f(y) + g(y)) + a \cdot b \cdot (\varphi(\|x - y\|) + \psi(\|x - y\|)). \]
Thus, we have:
\[ a \cdot (f + g)(x) + b \cdot (f + g)(y) + a \cdot b \cdot (\varphi + \psi)(\|x - y\|) \leq (f + g)(a \cdot x + b \cdot y). \]
This completes the proof. -/
  simpa [mul_add, add_add_add_comm] using add_le_add (hf.2 hx hy ha hb hab) (hg.2 hx hy ha hb hab)","import Mathlib.Analysis.InnerProductSpace.Basic
import Mathlib.Analysis.Convex.Strong

open UniformConcaveOn
open Real
variable {E : Type*} [NormedAddCommGroup E]
variable [NormedSpace ℝ E] {φ ψ : ℝ → ℝ} {s : Set E} {a b m : ℝ} {x y : E} {f g : E → ℝ}
"
18526,UniformConvexOn.add,"theorem UniformConvexOn.add (hf : UniformConvexOn s φ f) (hg : UniformConvexOn s ψ g) :
    UniformConvexOn s (φ + ψ) (f + g) := by sorry","Sum of Uniformly Convex Functions is Uniformly Convex with Sum of Moduli : For any real normed space \( E \), if \( f \) and \( g \) are uniformly convex functions on a set \( S \subseteq E \) with moduli \( \varphi \) and \( \psi \) respectively, then the function \( f + g \) is uniformly convex on \( S \) with modulus \( \varphi + \psi \).","theorem UniformConvexOn.add (hf : UniformConvexOn s φ f) (hg : UniformConvexOn s ψ g) :
    UniformConvexOn s (φ + ψ) (f + g) := by
  refine ⟨hf.1, fun x hx y hy a b ha hb hab ↦ ?_⟩
  simpa [mul_add, add_add_add_comm, sub_add_sub_comm]
    using add_le_add (hf.2 hx hy ha hb hab) (hg.2 hx hy ha hb hab)","To prove that \( f + g \) is uniformly convex on \( S \) with modulus \( \varphi + \psi \), we need to show that for any \( x, y \in S \) and any \( a, b \in \mathbb{R} \) such that \( 0 \leq a \), \( 0 \leq b \), and \( a + b = 1 \), the following inequality holds:
\[ (f + g)(a \cdot x + b \cdot y) \leq a \cdot (f + g)(x) + b \cdot (f + g)(y) - a \cdot b \cdot (\varphi + \psi)(\|x - y\|). \]

Since \( f \) is uniformly convex with modulus \( \varphi \), we have:
\[ f(a \cdot x + b \cdot y) \leq a \cdot f(x) + b \cdot f(y) - a \cdot b \cdot \varphi(\|x - y\|). \]

Similarly, since \( g \) is uniformly convex with modulus \( \psi \), we have:
\[ g(a \cdot x + b \cdot y) \leq a \cdot g(x) + b \cdot g(y) - a \cdot b \cdot \psi(\|x - y\|). \]

Adding these two inequalities, we get:
\[ f(a \cdot x + b \cdot y) + g(a \cdot x + b \cdot y) \leq a \cdot f(x) + b \cdot f(y) - a \cdot b \cdot \varphi(\|x - y\|) + a \cdot g(x) + b \cdot g(y) - a \cdot b \cdot \psi(\|x - y\|). \]

Using the distributive property of multiplication over addition and the commutativity of addition, we can rewrite the right-hand side as:
\[ (f + g)(a \cdot x + b \cdot y) \leq a \cdot (f(x) + g(x)) + b \cdot (f(y) + g(y)) - a \cdot b \cdot (\varphi(\|x - y\|) + \psi(\|x - y\|)). \]

This simplifies to:
\[ (f + g)(a \cdot x + b \cdot y) \leq a \cdot (f + g)(x) + b \cdot (f + g)(y) - a \cdot b \cdot (\varphi + \psi)(\|x - y\|). \]

Thus, \( f + g \) is uniformly convex on \( S \) with modulus \( \varphi + \psi \). This completes the proof. \(\blacksquare\)","theorem UniformConvexOn.add (hf : UniformConvexOn s φ f) (hg : UniformConvexOn s ψ g) :
    UniformConvexOn s (φ + ψ) (f + g) := by
/- To prove that \( f + g \) is uniformly convex on \( S \) with modulus \( \varphi + \psi \), we need to show that for any \( x, y \in S \) and any \( a, b \in \mathbb{R} \) such that \( 0 \leq a \), \( 0 \leq b \), and \( a + b = 1 \), the following inequality holds:
\[ (f + g)(a \cdot x + b \cdot y) \leq a \cdot (f + g)(x) + b \cdot (f + g)(y) - a \cdot b \cdot (\varphi + \psi)(\|x - y\|). \]
We assume this inequality and leave the proof of it to be shown later. -/
  refine ⟨hf.1, fun x hx y hy a b ha hb hab ↦ ?_⟩
/- Using the properties of addition and the fact that \( f \) and \( g \) are uniformly convex with moduli \( \varphi \) and \( \psi \) respectively, we can simplify the inequality we need to prove. Specifically, we use the distributive property of multiplication over addition, the commutativity of addition, and the commutativity of subtraction and addition to rewrite the inequality. We then use the fact that the sum of two inequalities is also an inequality to combine the inequalities for \( f \) and \( g \). This gives us:
\[ (f + g)(a \cdot x + b \cdot y) \leq a \cdot (f + g)(x) + b \cdot (f + g)(y) - a \cdot b \cdot (\varphi + \psi)(\|x - y\|). \]
This completes the proof. -/
  simpa [mul_add, add_add_add_comm, sub_add_sub_comm]
    using add_le_add (hf.2 hx hy ha hb hab) (hg.2 hx hy ha hb hab)","import Mathlib.Analysis.InnerProductSpace.Basic
import Mathlib.Analysis.Convex.Strong

open UniformConvexOn
open Real
variable {E : Type*} [NormedAddCommGroup E]
variable [NormedSpace ℝ E] {φ ψ : ℝ → ℝ} {s : Set E} {a b m : ℝ} {x y : E} {f g : E → ℝ}
"
18966,nhdsSet_le,"theorem nhdsSet_le : 𝓝ˢ s ≤ f ↔ ∀ x ∈ s, 𝓝 x ≤ f := by sorry","Neighborhood Filter of a Set is Less Than or Equal to a Filter if and only if Neighborhood Filters at Each Point are Less Than or Equal to the Filter : For any topological space \( X \), a filter \( f \) on \( X \), and a set \( s \subseteq X \), the neighborhood filter of \( s \), denoted by \( \mathcal{N}^s \), is less than or equal to \( f \) if and only if for every point \( x \in s \), the neighborhood filter at \( x \), denoted by \( \mathcal{N}(x) \), is less than or equal to \( f \). Formally, \( \mathcal{N}^s \leq f \) if and only if \( \forall x \in s, \mathcal{N}(x) \leq f \).","theorem nhdsSet_le : 𝓝ˢ s ≤ f ↔ ∀ x ∈ s, 𝓝 x ≤ f := by
  simp [nhdsSet]","To prove the theorem, we start by using the definition of the neighborhood filter of a set. The neighborhood filter of a set \( s \), denoted by \( \mathcal{N}^s \), is defined as the supremum of the neighborhood filters of all points in \( s \). This means that \( \mathcal{N}^s \) is the smallest filter that contains all the neighborhood filters \( \mathcal{N}(x) \) for \( x \in s \).

Given this definition, the statement \( \mathcal{N}^s \leq f \) is equivalent to saying that for every point \( x \in s \), the neighborhood filter \( \mathcal{N}(x) \) is less than or equal to \( f \). This is because the supremum of a collection of filters is less than or equal to a filter \( f \) if and only if each filter in the collection is less than or equal to \( f \).

Therefore, we have:
\[ \mathcal{N}^s \leq f \iff \forall x \in s, \mathcal{N}(x) \leq f \]

This completes the proof.","theorem nhdsSet_le : 𝓝ˢ s ≤ f ↔ ∀ x ∈ s, 𝓝 x ≤ f := by
  simp [nhdsSet]","import Mathlib.Topology.Basic
import Mathlib.Topology.NhdsSet


open Set Filter Topology
variable {X Y : Type*} [TopologicalSpace X] [TopologicalSpace Y] {f : Filter X}
  {s t s₁ s₂ t₁ t₂ : Set X} {x : X}
"
19311,conformalAt_iff',"theorem conformalAt_iff' {f : E → F} {x : E} : ConformalAt f x ↔
    ∃ c : ℝ, 0 < c ∧ ∀ u v : E, ⟪fderiv ℝ f x u, fderiv ℝ f x v⟫ = c * ⟪u, v⟫ := by sorry","For a real differentiable map \( f: E \to F \) and a point \( x \in E \), \( f \) is conformal at \( x \) if and only if there exists a positive scalar \( c \in \mathbb{R} \) such that for all \( u, v \in E \), the inner product of the differentials \( \langle f'(x)u, f'(x)v \rangle \) is equal to \( c \) times the inner product \( \langle u, v \rangle \).","theorem conformalAt_iff' {f : E → F} {x : E} : ConformalAt f x ↔
    ∃ c : ℝ, 0 < c ∧ ∀ u v : E, ⟪fderiv ℝ f x u, fderiv ℝ f x v⟫ = c * ⟪u, v⟫ := by
  rw [conformalAt_iff_isConformalMap_fderiv, isConformalMap_iff]","To prove the theorem, we start by using the equivalence that a function \( f \) is conformal at a point \( x \) if and only if its differential at \( x \), denoted \( f'(x) \), is a conformal linear map. This equivalence is given by the theorem \( \text{conformalAt\_iff\_isConformalMap\_fderiv} \).

Next, we use the characterization of a conformal linear map, which states that a linear map \( f \) is conformal if and only if there exists a positive scalar \( c \) such that for all vectors \( u, v \in E \), the inner product of \( f(u) \) and \( f(v) \) is equal to \( c \) times the inner product of \( u \) and \( v \). This characterization is given by the theorem \( \text{isConformalMap\_iff} \).

Combining these two equivalences, we conclude that \( f \) is conformal at \( x \) if and only if there exists a positive scalar \( c \) such that for all \( u, v \in E \), the inner product of \( f'(x)u \) and \( f'(x)v \) is equal to \( c \) times the inner product of \( u \) and \( v \). This completes the proof.","theorem conformalAt_iff' {f : E → F} {x : E} : ConformalAt f x ↔
    ∃ c : ℝ, 0 < c ∧ ∀ u v : E, ⟪fderiv ℝ f x u, fderiv ℝ f x v⟫ = c * ⟪u, v⟫ := by
/- First, we use the equivalence that a function \( f \) is conformal at a point \( x \) if and only if its differential at \( x \), denoted \( f'(x) \), is a conformal linear map. Then, we use the characterization of a conformal linear map, which states that a linear map \( f \) is conformal if and only if there exists a positive scalar \( c \) such that for all vectors \( u, v \in E \), the inner product of \( f(u) \) and \( f(v) \) is equal to \( c \) times the inner product of \( u \) and \( v \). Therefore, the goal is to show that \( f \) is conformal at \( x \) if and only if there exists a positive scalar \( c \) such that for all \( u, v \in E \), the inner product of \( f'(x)u \) and \( f'(x)v \) is equal to \( c \) times the inner product of \( u \) and \( v \). -/
  rw [conformalAt_iff_isConformalMap_fderiv, isConformalMap_iff]","import Mathlib.Analysis.Calculus.Conformal.NormedSpace
import Mathlib.Analysis.InnerProductSpace.ConformalLinearMap
import Mathlib.Analysis.Calculus.Conformal.InnerProduct


variable {E F : Type*}
variable [NormedAddCommGroup E] [NormedAddCommGroup F]
variable [InnerProductSpace ℝ E] [InnerProductSpace ℝ F]
variable [NormedAddCommGroup E] [NormedAddCommGroup F]
variable [InnerProductSpace ℝ E] [InnerProductSpace ℝ F]
variable [InnerProductSpace ℝ E] [InnerProductSpace ℝ F]
open RealInnerProductSpace
"
22178,padicNorm.zero,theorem padicNorm.zero : padicNorm p 0 = 0 := by sorry,"\( p \)-adic Norm of Zero is Zero : For any prime number \( p \), the \( p \)-adic norm of \( 0 \) is \( 0 \), i.e., \( \lvert 0 \rvert_p = 0 \).","theorem padicNorm.zero : padicNorm p 0 = 0 := by
  simp [padicNorm]","To prove that the \( p \)-adic norm of \( 0 \) is \( 0 \), we use the definition of the \( p \)-adic norm. The \( p \)-adic norm of a rational number \( q \) is defined as follows:
- If \( q \neq 0 \), the \( p \)-adic norm of \( q \) is \( p^{-\text{padicValRat}(p, q)} \).
- If \( q = 0 \), the \( p \)-adic norm of \( q \) is \( 0 \).

Since \( 0 \) is the rational number in question, we directly apply the second part of the definition. Therefore, the \( p \)-adic norm of \( 0 \) is \( 0 \). This completes the proof. \(\blacksquare\)","theorem padicNorm.zero : padicNorm p 0 = 0 := by
  simp [padicNorm]","import Mathlib.Algebra.Order.Field.Power
import Mathlib.NumberTheory.Padics.PadicVal.Basic
import Mathlib.NumberTheory.Padics.PadicNorm

open padicNorm
open padicValRat
variable {p : ℕ}
"
22469,BitVec.add_eq_adc,theorem BitVec.add_eq_adc (w : Nat) (x y : BitVec w) : x + y = (adc x y false).snd := by sorry,"Bitvector Addition Equals Second Component of Bitwise Addition with False Carry: \( x + y = \text{snd}(\text{adc}(x, y, \text{false})) \) : For any natural number \( w \) and any bitvectors \( x \) and \( y \) of width \( w \), the sum of \( x \) and \( y \) is equal to the second component of the result of the bitwise addition with carry operation `adc` applied to \( x \), \( y \), and the boolean value `false`. In other words, \( x + y = \text{snd}(\text{adc}(x, y, \text{false})) \).","theorem BitVec.add_eq_adc (w : Nat) (x y : BitVec w) : x + y = (adc x y false).snd := by
  simp [adc_spec]","To prove the theorem, we use the specification of the bitwise addition with carry operation \( \text{adc} \). According to the specification, for any natural number \( w \) and any bitvectors \( x \) and \( y \) of width \( w \), and for the boolean value `false`, the result of the bitwise addition with carry operation \( \text{adc}(x, y, \text{false}) \) is a pair consisting of the carry bit and the sum of the bitvectors. Specifically, we have:
\[
\text{adc}(x, y, \text{false}) = (\text{carry}(w, x, y, \text{false}), x + y + \text{zeroExtend}(w, \text{ofBool}(\text{false})))
\]
Since the boolean value `false` is converted to the bitvector `0` of length 1, and the zero extension of `0` to any width \( w \) is still `0`, we have:
\[
\text{zeroExtend}(w, \text{ofBool}(\text{false})) = 0
\]
Thus, the second component of \( \text{adc}(x, y, \text{false}) \) is:
\[
x + y + 0 = x + y
\]
Therefore, the proposition \( x + y = \text{snd}(\text{adc}(x, y, \text{false})) \) holds. This completes the proof. \(\blacksquare\)","theorem BitVec.add_eq_adc (w : Nat) (x y : BitVec w) : x + y = (adc x y false).snd := by
/- Using the specification of the bitwise addition with carry operation, we can simplify the proposition we want to show. Specifically, the specification states that for any natural number \( w \) and any bitvectors \( x \) and \( y \) of width \( w \), and for the boolean value `false`, the result of the bitwise addition with carry operation \( \text{adc}(x, y, \text{false}) \) is a pair consisting of the carry bit and the sum of the bitvectors. Since the boolean value `false` is converted to the bitvector `0` of length 1, and the zero extension of `0` to any width \( w \) is still `0`, the second component of \( \text{adc}(x, y, \text{false}) \) is \( x + y + 0 \), which simplifies to \( x + y \). Therefore, the proposition \( x + y = \text{snd}(\text{adc}(x, y, \text{false})) \) holds. -/
  simp [adc_spec]","import Init.Data.BitVec.Folds
import Init.Data.Nat.Mod
import Init.Data.BitVec.Bitblast

open BitVec
open Nat Bool
"
23893,aemeasurable_of_re_im,"theorem aemeasurable_of_re_im (hre : AEMeasurable (fun x => RCLike.re (f x)) μ)
    (him : AEMeasurable (fun x => RCLike.im (f x)) μ) : AEMeasurable f μ := by sorry","Almost Everywhere Measurability of Function from Real and Imaginary Parts : For any type \(\alpha\) and a real or complex-like field \(\mathbb{𝕜}\), if a function \( f : \alpha \to \mathbb{𝕜} \) is almost everywhere measurable with respect to a measure \(\mu\) on \(\alpha\) in both its real and imaginary parts, then \( f \) itself is almost everywhere measurable with respect to \(\mu\). Formally, if \( \text{AEMeasurable}(\text{Re}(f), \mu) \) and \( \text{AEMeasurable}(\text{Im}(f), \mu) \), then \( \text{AEMeasurable}(f, \mu) \).","theorem aemeasurable_of_re_im (hre : AEMeasurable (fun x => RCLike.re (f x)) μ)
    (him : AEMeasurable (fun x => RCLike.im (f x)) μ) : AEMeasurable f μ := by
  convert AEMeasurable.add (M := 𝕜) (RCLike.measurable_ofReal.comp_aemeasurable hre)
      ((RCLike.measurable_ofReal.comp_aemeasurable him).mul_const RCLike.I)
  exact (RCLike.re_add_im _).symm","To prove that \( f \) is almost everywhere measurable, we start by noting that the real part \( \text{Re}(f) \) and the imaginary part \( \text{Im}(f) \) of \( f \) are both almost everywhere measurable with respect to the measure \(\mu\).

1. **Step 1:**
We use the fact that the sum of two almost everywhere measurable functions is almost everywhere measurable. Specifically, we consider the real part of \( f \) and the imaginary part of \( f \) multiplied by the imaginary unit \( i \). Since the real part \( \text{Re}(f) \) and the imaginary part \( \text{Im}(f) \) are both almost everywhere measurable, and the canonical coercion from real numbers to the real or complex-like field is measurable, the functions \( \text{Re}(f) \) and \( \text{Im}(f) \cdot i \) are almost everywhere measurable. Therefore, their sum \( \text{Re}(f) + \text{Im}(f) \cdot i \) is almost everywhere measurable. We need to show that this sum is equal to \( f \).

2. **Step 2:**
We use the property that for any element \( z \) in a real or complex-like field, the sum of its real part and its imaginary part multiplied by the imaginary unit \( i \) is equal to \( z \). This property is symmetric, so we can write \( f(x) = \text{Re}(f(x)) + \text{Im}(f(x)) \cdot i \). This completes the proof that \( f \) is almost everywhere measurable.

Thus, we have shown that if \( \text{AEMeasurable}(\text{Re}(f), \mu) \) and \( \text{AEMeasurable}(\text{Im}(f), \mu) \), then \( \text{AEMeasurable}(f, \mu) \). This completes the proof. \(\blacksquare\)","theorem aemeasurable_of_re_im (hre : AEMeasurable (fun x => RCLike.re (f x)) μ)
    (him : AEMeasurable (fun x => RCLike.im (f x)) μ) : AEMeasurable f μ := by
/- To prove that \( f \) is almost everywhere measurable, we use the fact that the sum of two almost everywhere measurable functions is almost everywhere measurable. Specifically, we consider the real part of \( f \) and the imaginary part of \( f \) multiplied by the imaginary unit \( i \). Since the real part \( \text{Re}(f) \) and the imaginary part \( \text{Im}(f) \) are both almost everywhere measurable, and the canonical coercion from real numbers to the real or complex-like field is measurable, the functions \( \text{Re}(f) \) and \( \text{Im}(f) \cdot i \) are almost everywhere measurable. Therefore, their sum \( \text{Re}(f) + \text{Im}(f) \cdot i \) is almost everywhere measurable. We need to show that this sum is equal to \( f \). -/
  convert AEMeasurable.add (M := 𝕜) (RCLike.measurable_ofReal.comp_aemeasurable hre)
      ((RCLike.measurable_ofReal.comp_aemeasurable him).mul_const RCLike.I)
/- We use the property that for any element \( z \) in a real or complex-like field, the sum of its real part and its imaginary part multiplied by the imaginary unit \( i \) is equal to \( z \). This property is symmetric, so we can write \( f(x) = \text{Re}(f(x)) + \text{Im}(f(x)) \cdot i \). This completes the proof that \( f \) is almost everywhere measurable. -/
  exact (RCLike.re_add_im _).symm","import Mathlib.Analysis.RCLike.Lemmas
import Mathlib.MeasureTheory.Constructions.BorelSpace.Complex
import Mathlib.MeasureTheory.Function.SpecialFunctions.RCLike


open NNReal ENNReal
variable {𝕜 : Type*} [RCLike 𝕜]
variable {α 𝕜 : Type*} [RCLike 𝕜] {m : MeasurableSpace α} {f : α → 𝕜}
  {μ : MeasureTheory.Measure α}
variable {α 𝕜 : Type*} [RCLike 𝕜] [MeasurableSpace α] {f : α → 𝕜} {μ : MeasureTheory.Measure α}
"
24215,Complex.cot_eq_exp_ratio,"theorem Complex.cot_eq_exp_ratio (z : ℂ) :
    cot z = (Complex.exp (2 * I * z) + 1) / (I * (1 - Complex.exp (2 * I * z))) := by sorry","Cotangent Identity: \(\cot(z) = \frac{\exp(2i z) + 1}{i (1 - \exp(2i z))}\) : For any complex number \( z \), the cotangent of \( z \) is equal to the ratio \(\frac{\exp(2i z) + 1}{i (1 - \exp(2i z))}\), i.e., \(\cot(z) = \frac{\exp(2i z) + 1}{i (1 - \exp(2i z))}\).","theorem Complex.cot_eq_exp_ratio (z : ℂ) :
    cot z = (Complex.exp (2 * I * z) + 1) / (I * (1 - Complex.exp (2 * I * z))) := by
  rw [Complex.cot, Complex.sin, Complex.cos]
  field_simp
  have h1 : exp (z * I) + exp (-(z * I)) = exp (-(z * I)) * (exp (2 * I * z) + 1) := by
    rw [mul_add, ← Complex.exp_add]
    simp only [mul_one, add_left_inj]
    ring_nf
  have h2 : (exp (-(z * I)) - exp (z * I)) * I = exp (-(z * I)) * (I * (1 - exp (2 * I * z))) := by
    ring_nf
    rw [mul_assoc, ← Complex.exp_add]
    ring_nf
  rw [h1, h2, mul_div_mul_left _ _ (Complex.exp_ne_zero _)]","We start by using the definitions of the complex cotangent, sine, and cosine functions. The cotangent of \( z \) is defined as the ratio of the cosine of \( z \) to the sine of \( z \):
\[
\cot z = \frac{\cos z}{\sin z}
\]
The sine of \( z \) is given by:
\[
\sin z = \frac{\exp(-z \cdot i) - \exp(z \cdot i)}{2i}
\]
and the cosine of \( z \) is given by:
\[
\cos z = \frac{\exp(z \cdot i) + \exp(-z \cdot i)}{2}
\]
Thus, the goal is to show:
\[
\frac{\exp(z \cdot i) + \exp(-z \cdot i)}{2} / \left( \frac{\exp(-z \cdot i) - \exp(z \cdot i)}{2i} \right) = \frac{\exp(2i z) + 1}{i (1 - \exp(2i z))}
\]

By simplifying the expression using field arithmetic, we can rewrite the goal as:
\[
\frac{\exp(z \cdot i) + \exp(-z \cdot i)}{\exp(-z \cdot i) - \exp(z \cdot i) \cdot i} = \frac{\exp(2i z) + 1}{i (1 - \exp(2i z))}
\]

We now construct a lemma \( h1 \) that states:
\[
\exp(z \cdot i) + \exp(-z \cdot i) = \exp(-z \cdot i) \cdot (\exp(2i z) + 1)
\]
Using the distributive property of multiplication over addition and the property of the exponential function, we rewrite the goal as:
\[
\exp(z \cdot i) + \exp(-z \cdot i) = \exp(-z \cdot i + 2i z) + \exp(-z \cdot i) \cdot 1
\]
Simplifying the expression using the properties of multiplication by one and the cancellation property of addition, we get:
\[
\exp(z \cdot i) = \exp(-z \cdot i + 2i z)
\]
By applying ring normalization, we conclude that the equation holds.

We now construct a lemma \( h2 \) that states:
\[
(\exp(-z \cdot i) - \exp(z \cdot i)) \cdot i = \exp(-z \cdot i) \cdot (i \cdot (1 - \exp(2i z)))
\]
By applying ring normalization, we rewrite the goal as:
\[
i \cdot \exp(-z \cdot i) - i \cdot \exp(z \cdot i) = i \cdot \exp(-z \cdot i) - i \cdot \exp(-z \cdot i) \cdot \exp(z \cdot i \cdot 2)
\]
Using the associative property of multiplication and the property of the exponential function, we rewrite the goal as:
\[
i \cdot \exp(-z \cdot i) - i \cdot \exp(z \cdot i) = i \cdot \exp(-z \cdot i) - i \cdot \exp(-z \cdot i + z \cdot i \cdot 2)
\]
By applying ring normalization, we conclude that the equation holds.

Using the lemmas \( h1 \) and \( h2 \), and the fact that the exponential function is never zero, we rewrite the goal as:
\[
\frac{\exp(-z \cdot i) \cdot (\exp(2i z) + 1)}{\exp(-z \cdot i) \cdot (i \cdot (1 - \exp(2i z)))} = \frac{\exp(2i z) + 1}{i (1 - \exp(2i z))}
\]
By the cancellation property of multiplication and division, we conclude that the equation holds.

Thus, we have shown that:
\[
\cot(z) = \frac{\exp(2i z) + 1}{i (1 - \exp(2i z))}
\]
This completes the proof. \(\blacksquare\)","theorem Complex.cot_eq_exp_ratio (z : ℂ) :
    cot z = (Complex.exp (2 * I * z) + 1) / (I * (1 - Complex.exp (2 * I * z))) := by
/- We start by using the definitions of the complex cotangent, sine, and cosine functions. The cotangent of \( z \) is defined as the ratio of the cosine of \( z \) to the sine of \( z \). The sine of \( z \) is given by:
\[
\sin z = \frac{\exp(-z \cdot i) - \exp(z \cdot i)}{2i}
\]
and the cosine of \( z \) is given by:
\[
\cos z = \frac{\exp(z \cdot i) + \exp(-z \cdot i)}{2}
\]
Thus, the goal is to show:
\[
\frac{\exp(z \cdot i) + \exp(-z \cdot i)}{2} / \left( \frac{\exp(-z \cdot i) - \exp(z \cdot i)}{2i} \right) = \frac{\exp(2i z) + 1}{i (1 - \exp(2i z))}
\] -/
  rw [Complex.cot, Complex.sin, Complex.cos]
/- By simplifying the expression using field arithmetic, we can rewrite the goal as:
\[
\frac{\exp(z \cdot i) + \exp(-z \cdot i)}{\exp(-z \cdot i) - \exp(z \cdot i) \cdot i} = \frac{\exp(2i z) + 1}{i (1 - \exp(2i z))}
\] -/
  field_simp
/- We now construct a lemma \( h1 \) that states:
\[
\exp(z \cdot i) + \exp(-z \cdot i) = \exp(-z \cdot i) \cdot (\exp(2i z) + 1)
\] -/
  have h1 : exp (z * I) + exp (-(z * I)) = exp (-(z * I)) * (exp (2 * I * z) + 1) := by
/- Using the distributive property of multiplication over addition and the property of the exponential function, we rewrite the goal as:
\[
\exp(z \cdot i) + \exp(-z \cdot i) = \exp(-z \cdot i + 2i z) + \exp(-z \cdot i) \cdot 1
\] -/
    rw [mul_add, ← Complex.exp_add]
/- Simplifying the expression using the properties of multiplication by one and the cancellation property of addition, we get:
\[
\exp(z \cdot i) = \exp(-z \cdot i + 2i z)
\] -/
    simp only [mul_one, add_left_inj]
/- By applying ring normalization, we conclude that the equation holds. -/
    ring_nf
/- We now construct a lemma \( h2 \) that states:
\[
(\exp(-z \cdot i) - \exp(z \cdot i)) \cdot i = \exp(-z \cdot i) \cdot (i \cdot (1 - \exp(2i z)))
\] -/
  have h2 : (exp (-(z * I)) - exp (z * I)) * I = exp (-(z * I)) * (I * (1 - exp (2 * I * z))) := by
/- By applying ring normalization, we rewrite the goal as:
\[
i \cdot \exp(-z \cdot i) - i \cdot \exp(z \cdot i) = i \cdot \exp(-z \cdot i) - i \cdot \exp(-z \cdot i) \cdot \exp(z \cdot i \cdot 2)
\] -/
    ring_nf
/- Using the associative property of multiplication and the property of the exponential function, we rewrite the goal as:
\[
i \cdot \exp(-z \cdot i) - i \cdot \exp(z \cdot i) = i \cdot \exp(-z \cdot i) - i \cdot \exp(-z \cdot i + z \cdot i \cdot 2)
\] -/
    rw [mul_assoc, ← Complex.exp_add]
/- By applying ring normalization, we conclude that the equation holds. -/
    ring_nf
/- Using the lemmas \( h1 \) and \( h2 \), and the fact that the exponential function is never zero, we rewrite the goal as:
\[
\frac{\exp(-z \cdot i) \cdot (\exp(2i z) + 1)}{\exp(-z \cdot i) \cdot (i \cdot (1 - \exp(2i z)))} = \frac{\exp(2i z) + 1}{i (1 - \exp(2i z))}
\]
By the cancellation property of multiplication and division, we conclude that the equation holds. -/
  rw [h1, h2, mul_div_mul_left _ _ (Complex.exp_ne_zero _)]","import Mathlib.Analysis.Complex.UpperHalfPlane.Exp
import Mathlib.Analysis.SpecialFunctions.Trigonometric.Cotangent

open Complex
open Real Complex BigOperators Filter
open scoped UpperHalfPlane Topology
"
25274,Bool.eq_false_iff,theorem Bool.eq_false_iff : {b : Bool} → b = false ↔ b ≠ true := by sorry,"Boolean Value is False if and only if it is not True: \( b = \text{false} \leftrightarrow b \neq \text{true} \) : For any boolean value \( b \), \( b = \text{false} \) if and only if \( b \neq \text{true} \).","theorem Bool.eq_false_iff : {b : Bool} → b = false ↔ b ≠ true := by
  decide","We need to show that for any boolean value \( b \), the statement \( b = \text{false} \) is equivalent to \( b \neq \text{true} \).

1. **(\( \Rightarrow \) direction):** Assume \( b = \text{false} \). We need to show that \( b \neq \text{true} \). Since \( b = \text{false} \), it is clear that \( b \) cannot be equal to \(\text{true}\). Therefore, \( b \neq \text{true} \).

2. **(\( \Leftarrow \) direction):** Assume \( b \neq \text{true} \). We need to show that \( b = \text{false} \). Since \( b \) is a boolean value, it can only be either \(\text{true}\) or \(\text{false}\). Given that \( b \neq \text{true} \), the only remaining possibility is \( b = \text{false} \).

Thus, we have shown that \( b = \text{false} \) if and only if \( b \neq \text{true} \). This completes the proof.","theorem Bool.eq_false_iff : {b : Bool} → b = false ↔ b ≠ true := by
  decide","import Init.BinderPredicates
import Init.Data.Bool

open Bool

"
25275,Bool.ne_false_iff,theorem Bool.ne_false_iff : {b : Bool} → b ≠ false ↔ b = true := by sorry,"Boolean Not Equal to False is Equivalent to True: \( b \neq \text{false} \leftrightarrow b = \text{true} \) : For any boolean value \( b \), \( b \neq \text{false} \) if and only if \( b = \text{true} \).","theorem Bool.ne_false_iff : {b : Bool} → b ≠ false ↔ b = true := by
  decide","We need to show that for any boolean value \( b \), the statement \( b \neq \text{false} \) is equivalent to \( b = \text{true} \).

1. **(\( \Rightarrow \)) Assume \( b \neq \text{false} \):**
- Since \( b \) is a boolean value, it can only be either \( \text{true} \) or \( \text{false} \).
- If \( b \neq \text{false} \), then \( b \) must be \( \text{true} \).

2. **(\( \Leftarrow \)) Assume \( b = \text{true} \):**
- If \( b = \text{true} \), then \( b \) is not equal to \( \text{false} \), i.e., \( b \neq \text{false} \).

Since both directions are proven, we conclude that for any boolean value \( b \), \( b \neq \text{false} \) if and only if \( b = \text{true} \). This completes the proof. \(\blacksquare\)","theorem Bool.ne_false_iff : {b : Bool} → b ≠ false ↔ b = true := by
  decide","import Init.BinderPredicates
import Init.Data.Bool

open Bool

"
25287,Bool.not_or_self,"theorem Bool.not_or_self : ∀ (x : Bool), (!x || x) = true := by sorry","Negation or Self is True: $\neg x \lor x = \text{true}$ : For any boolean value $x$, the expression $\neg x \lor x$ is equal to `true`, i.e., $\neg x \lor x = \text{true}$.","theorem Bool.not_or_self : ∀ (x : Bool), (!x || x) = true := by
  decide","To prove that for any boolean value $x$, the expression $\neg x \lor x$ is equal to `true`, we consider the two possible values of $x$:

1. **Case 1: $x = \text{true}$**
- If $x = \text{true}$, then $\neg x = \text{false}$.
- Therefore, $\neg x \lor x = \text{false} \lor \text{true} = \text{true}$.

2. **Case 2: $x = \text{false}$**
- If $x = \text{false}$, then $\neg x = \text{true}$.
- Therefore, $\neg x \lor x = \text{true} \lor \text{false} = \text{true}$.

In both cases, the expression $\neg x \lor x$ evaluates to `true`. Hence, for any boolean value $x$, $\neg x \lor x = \text{true}$. This completes the proof.","theorem Bool.not_or_self : ∀ (x : Bool), (!x || x) = true := by
  decide","import Init.BinderPredicates
import Init.Data.Bool

open Bool

"
25297,Bool.and_comm,"theorem Bool.and_comm : ∀ (x y : Bool), (x && y) = (y && x) := by sorry","Boolean AND is Commutative: \( x \land y = y \land x \) : For any boolean values \( x \) and \( y \), the boolean ""and"" operation is commutative, i.e., \( x \land y = y \land x \).","theorem Bool.and_comm : ∀ (x y : Bool), (x && y) = (y && x) := by
  decide","To prove that the boolean ""and"" operation is commutative, we need to show that for any boolean values \( x \) and \( y \), the equality \( x \land y = y \land x \) holds. We can do this by checking all possible combinations of \( x \) and \( y \):

1. If \( x = \text{true} \) and \( y = \text{true} \), then \( x \land y = \text{true} \) and \( y \land x = \text{true} \), so \( x \land y = y \land x \).
2. If \( x = \text{true} \) and \( y = \text{false} \), then \( x \land y = \text{false} \) and \( y \land x = \text{false} \), so \( x \land y = y \land x \).
3. If \( x = \text{false} \) and \( y = \text{true} \), then \( x \land y = \text{false} \) and \( y \land x = \text{false} \), so \( x \land y = y \land x \).
4. If \( x = \text{false} \) and \( y = \text{false} \), then \( x \land y = \text{false} \) and \( y \land x = \text{false} \), so \( x \land y = y \land x \).

Since the equality \( x \land y = y \land x \) holds for all possible values of \( x \) and \( y \), we conclude that the boolean ""and"" operation is commutative. This completes the proof.","theorem Bool.and_comm : ∀ (x y : Bool), (x && y) = (y && x) := by
  decide","import Init.BinderPredicates
import Init.Data.Bool

open Bool

"
25312,Bool.and_iff_right_iff_imp,"theorem Bool.and_iff_right_iff_imp : ∀(a b : Bool), ((a && b) = b) ↔ (b → a) := by sorry","Boolean AND Identity: \( (a \land b) = b \leftrightarrow (b = \text{true} \to a = \text{true}) \) : For any boolean values \( a \) and \( b \), the boolean ""and"" operation \( a \land b \) is equal to \( b \) if and only if \( b \) is `true` implies \( a \) is `true`.","theorem Bool.and_iff_right_iff_imp : ∀(a b : Bool), ((a && b) = b) ↔ (b → a) := by
  decide","To prove the theorem, we need to show that for any boolean values \( a \) and \( b \), the following equivalence holds:
\[ (a \land b) = b \leftrightarrow (b = \text{true} \to a = \text{true}) \]

We will consider all possible values of \( a \) and \( b \) and verify the equivalence directly.

1. **Case 1: \( a = \text{true} \) and \( b = \text{true} \)**
- \( (a \land b) = (\text{true} \land \text{true}) = \text{true} \)
- \( b = \text{true} \)
- \( b = \text{true} \to a = \text{true} \) is true because both \( b \) and \( a \) are true.
- Therefore, \( (a \land b) = b \) and \( b = \text{true} \to a = \text{true} \) are both true.

2. **Case 2: \( a = \text{true} \) and \( b = \text{false} \)**
- \( (a \land b) = (\text{true} \land \text{false}) = \text{false} \)
- \( b = \text{false} \)
- \( b = \text{true} \to a = \text{true} \) is vacuously true because \( b \) is false.
- Therefore, \( (a \land b) = b \) is false and \( b = \text{true} \to a = \text{true} \) is true, so the equivalence holds.

3. **Case 3: \( a = \text{false} \) and \( b = \text{true} \)**
- \( (a \land b) = (\text{false} \land \text{true}) = \text{false} \)
- \( b = \text{true} \)
- \( b = \text{true} \to a = \text{true} \) is false because \( b \) is true and \( a \) is false.
- Therefore, \( (a \land b) = b \) is false and \( b = \text{true} \to a = \text{true} \) is false, so the equivalence holds.

4. **Case 4: \( a = \text{false} \) and \( b = \text{false} \)**
- \( (a \land b) = (\text{false} \land \text{false}) = \text{false} \)
- \( b = \text{false} \)
- \( b = \text{true} \to a = \text{true} \) is vacuously true because \( b \) is false.
- Therefore, \( (a \land b) = b \) is false and \( b = \text{true} \to a = \text{true} \) is true, so the equivalence holds.

Since the equivalence holds in all possible cases, we conclude that for any boolean values \( a \) and \( b \),
\[ (a \land b) = b \leftrightarrow (b = \text{true} \to a = \text{true}) \]
This completes the proof.","theorem Bool.and_iff_right_iff_imp : ∀(a b : Bool), ((a && b) = b) ↔ (b → a) := by
  decide","import Init.BinderPredicates
import Init.Data.Bool

open Bool

"
25327,Bool.eq_not_self,"theorem Bool.eq_not_self : ∀(b : Bool), (b = (!b)) ↔ False := by sorry","Boolean Equality with Negation is False: \( b = \neg b \leftrightarrow \text{False} \) : For any boolean value \( b \), the equation \( b = \neg b \) is equivalent to the false proposition, i.e., \( b = \neg b \leftrightarrow \text{False} \).","theorem Bool.eq_not_self : ∀(b : Bool), (b = (!b)) ↔ False := by
  decide","To prove that for any boolean value \( b \), the equation \( b = \neg b \) is equivalent to the false proposition, we consider the possible values of \( b \):

1. If \( b = \text{true} \), then \( \neg b = \text{false} \). Clearly, \( \text{true} \neq \text{false} \), so \( b = \neg b \) is false.
2. If \( b = \text{false} \), then \( \neg b = \text{true} \). Clearly, \( \text{false} \neq \text{true} \), so \( b = \neg b \) is false.

Since \( b \) can only be either \( \text{true} \) or \( \text{false} \), and in both cases \( b = \neg b \) is false, we conclude that \( b = \neg b \) is equivalent to the false proposition. Therefore, \( b = \neg b \leftrightarrow \text{False} \). This completes the proof.","theorem Bool.eq_not_self : ∀(b : Bool), (b = (!b)) ↔ False := by
  decide","import Init.BinderPredicates
import Init.Data.Bool

open Bool

"
25330,Bool.false_eq,theorem Bool.false_eq (b : Bool) : (false = b) = (b = false) := by sorry,"Equivalence of False Equality in Boolean Type: $\text{false} = b \leftrightarrow b = \text{false}$ : For any boolean value $b$, the statement $\text{false} = b$ is equivalent to $b = \text{false}$, i.e., $\text{false} = b \leftrightarrow b = \text{false}$.","theorem Bool.false_eq (b : Bool) : (false = b) = (b = false) := by
  cases b <;> simp","To prove the equivalence $\text{false} = b \leftrightarrow b = \text{false}$ for any boolean value $b$, we will consider every possible case of $b$.

1. **Case 1: $b = \text{false}$**
- We need to show that $(\text{false} = \text{false}) = (\text{false} = \text{false})$.
- This is trivially true because both sides of the equality are the same.

2. **Case 2: $b = \text{true}$**
- We need to show that $(\text{false} = \text{true}) = (\text{true} = \text{false})$.
- Using the fact that $\text{true} \neq \text{false}$, both sides of the equality are false, so the proposition is true.

Since both cases are true, we have shown that $\text{false} = b \leftrightarrow b = \text{false}$ for any boolean value $b$. This completes the proof. $\blacksquare$","theorem Bool.false_eq (b : Bool) : (false = b) = (b = false) := by
/- We will consider every possible case of the boolean value $b$. There are two cases: $b = \text{false}$ and $b = \text{true}$. For each case, we will simplify the proposition we want to show. -/
/- We will consider every possible case of the boolean value $b$. There are two cases: $b = \text{false}$ and $b = \text{true}$. -/
/- First, we apply the case analysis to the current goal, which generates two subgoals. Then, for each of these new subgoals, we will simplify the proposition we want to show. -/
/- For the case $b = \text{false}$, we simplify the proposition $(\text{false} = \text{false}) = (\text{false} = \text{false})$. This is trivially true because both sides of the equality are the same. -/
/- For the case $b = \text{true}$, we simplify the proposition $(\text{false} = \text{true}) = (\text{true} = \text{false})$. Using the fact that $\text{true} \neq \text{false}$, both sides of the equality are false, so the proposition is true. -/
  cases b <;> simp","import Init.BinderPredicates
import Init.Data.Bool

open Bool

"
26071,Nat.WithBot.add_one_le_of_lt,theorem Nat.WithBot.add_one_le_of_lt {n m : WithBot ℕ} (h : n < m) : n + 1 ≤ m := by sorry,"Addition of One Preserves Inequality in Natural Numbers with Bottom Element : For any elements \( n \) and \( m \) in the type \( \mathbb{N} \sqcup \{\bot\} \) (natural numbers with a bottom element), if \( n < m \), then \( n + 1 \leq m \).","theorem Nat.WithBot.add_one_le_of_lt {n m : WithBot ℕ} (h : n < m) : n + 1 ≤ m := by
  cases n
  · simp only [WithBot.bot_add, bot_le]
  cases m
  · exact (not_lt_bot h).elim
  · rwa [WithBot.coe_lt_coe, ← Nat.add_one_le_iff, ← WithBot.coe_le_coe, WithBot.coe_add,
      WithBot.coe_one] at h","We need to show that for any \( n, m \in \mathbb{N} \sqcup \{\bot\} \), if \( n < m \), then \( n + 1 \leq m \).

1. **Case 1: \( n = \bot \)**
- We use the fact that the addition of the bottom element with any element is the bottom element itself, and the bottom element is less than or equal to any element. Therefore, \( \bot + 1 \leq m \) holds trivially.

2. **Case 2: \( n = a \) for some \( a \in \mathbb{N} \)**
- We will consider every possible case of \( m \):
- **Subcase 2.1: \( m = \bot \)**
- We use the fact that no element in \( \mathbb{N} \) is less than the bottom element. Therefore, the assumption \( a < \bot \) leads to a contradiction, and the goal is vacuously true.
- **Subcase 2.2: \( m = b \) for some \( b \in \mathbb{N} \)**
- We use the following equivalences:
1. The canonical map preserves strict inequality: \( a < b \) if and only if \( \text{WithBot.some}(a) < \text{WithBot.some}(b) \).
2. The inequality \( a + 1 \leq b \) is equivalent to \( a < b \).
3. The canonical map preserves the less-than-or-equal relation: \( a + 1 \leq b \) if and only if \( \text{WithBot.some}(a + 1) \leq \text{WithBot.some}(b) \).
4. The canonical map preserves addition: \( \text{WithBot.some}(a + 1) = \text{WithBot.some}(a) + \text{WithBot.some}(1) \).
5. The canonical map preserves the multiplicative identity: \( \text{WithBot.some}(1) = 1 \).

Using these equivalences, we can rewrite the assumption \( a < b \) to \( a + 1 \leq b \), which is exactly what we need to show.

Therefore, in all cases, we have shown that if \( n < m \), then \( n + 1 \leq m \). This completes the proof. \(\blacksquare\)","theorem Nat.WithBot.add_one_le_of_lt {n m : WithBot ℕ} (h : n < m) : n + 1 ≤ m := by
/- We will consider every possible case of \( n \). Case 1: \( n = \bot \). Case 2: \( n = a \) for some \( a \in \mathbb{N} \). -/
  cases n
/- For the case \( n = \bot \), we use the fact that the addition of the bottom element with any element is the bottom element itself, and the bottom element is less than or equal to any element. Therefore, \( \bot + 1 \leq m \) holds trivially. -/
  · simp only [WithBot.bot_add, bot_le]
/- We will consider every possible case of \( m \). Case 1: \( m = \bot \). Case 2: \( m = b \) for some \( b \in \mathbb{N} \). -/
  cases m
/- For the case \( m = \bot \), we use the fact that no element in \( \mathbb{N} \) is less than the bottom element. Therefore, the assumption \( a < \bot \) leads to a contradiction, and the goal is vacuously true. -/
  · exact (not_lt_bot h).elim
/- For the case \( m = b \) for some \( b \in \mathbb{N} \), we use the following equivalences:
1. The canonical map preserves strict inequality: \( a < b \) if and only if \( \text{WithBot.some}(a) < \text{WithBot.some}(b) \).
2. The inequality \( a + 1 \leq b \) is equivalent to \( a < b \).
3. The canonical map preserves the less-than-or-equal relation: \( a + 1 \leq b \) if and only if \( \text{WithBot.some}(a + 1) \leq \text{WithBot.some}(b) \).
4. The canonical map preserves addition: \( \text{WithBot.some}(a + 1) = \text{WithBot.some}(a) + \text{WithBot.some}(1) \).
5. The canonical map preserves the multiplicative identity: \( \text{WithBot.some}(1) = 1 \).

Using these equivalences, we can rewrite the assumption \( a < b \) to \( a + 1 \leq b \), which is exactly what we need to show. -/
  · rwa [WithBot.coe_lt_coe, ← Nat.add_one_le_iff, ← WithBot.coe_le_coe, WithBot.coe_add,
      WithBot.coe_one] at h","import Mathlib.Algebra.Order.Ring.Nat
import Mathlib.Algebra.Order.Monoid.WithTop
import Mathlib.Data.Nat.Cast.WithTop
import Mathlib.Data.Nat.WithBot

open Nat
open WithBot

"
26079,Nat.WithBot.add_eq_zero_iff,theorem Nat.WithBot.add_eq_zero_iff {n m : WithBot ℕ} : n + m = 0 ↔ n = 0 ∧ m = 0 := by sorry,"Sum Equals Zero in Natural Numbers with Bottom Element: \( n + m = 0 \leftrightarrow n = 0 \land m = 0 \) : For any elements \( n \) and \( m \) in the type \( \mathbb{N} \cup \{\bot\} \) (natural numbers with a bottom element adjoined), the sum \( n + m \) is equal to 0 if and only if both \( n \) and \( m \) are equal to 0.","theorem Nat.WithBot.add_eq_zero_iff {n m : WithBot ℕ} : n + m = 0 ↔ n = 0 ∧ m = 0 := by
  cases n
  · simp [WithBot.bot_add]
  cases m
  · simp [WithBot.add_bot]
  simp [← WithBot.coe_add, add_eq_zero_iff_of_nonneg]","We need to show that \( n + m = 0 \) if and only if \( n = 0 \) and \( m = 0 \) for any \( n, m \in \mathbb{N} \cup \{\bot\} \).

1. **Case 1: \( n = \bot \)**
- We need to show that \( \bot + m = 0 \) if and only if \( \bot = 0 \) and \( m = 0 \).
- Using the property that adding the bottom element to any element results in the bottom element, we have \( \bot + m = \bot \).
- Since \( \bot \neq 0 \), the only way \( \bot + m = 0 \) is if \( m = 0 \). However, \( \bot \neq 0 \), so this case is vacuously true.

2. **Case 2: \( n = a \) for some \( a \in \mathbb{N} \)**
- We need to consider the cases for \( m \).

- **Subcase 2.1: \( m = \bot \)**
- We need to show that \( a + \bot = 0 \) if and only if \( a = 0 \) and \( \bot = 0 \).
- Using the property that adding the bottom element to any element results in the bottom element, we have \( a + \bot = \bot \).
- Since \( \bot \neq 0 \), the only way \( a + \bot = 0 \) is if \( a = 0 \). However, \( \bot \neq 0 \), so this subcase is vacuously true.

- **Subcase 2.2: \( m = b \) for some \( b \in \mathbb{N} \)**
- We need to show that \( a + b = 0 \) if and only if \( a = 0 \) and \( b = 0 \).
- Using the property that the sum of two nonnegative elements is zero if and only if both elements are zero, we have \( a + b = 0 \) if and only if \( a = 0 \) and \( b = 0 \).

Since all cases have been considered and the conditions are satisfied, we conclude that \( n + m = 0 \) if and only if \( n = 0 \) and \( m = 0 \). This completes the proof. \(\blacksquare\)","theorem Nat.WithBot.add_eq_zero_iff {n m : WithBot ℕ} : n + m = 0 ↔ n = 0 ∧ m = 0 := by
/- We will consider every possible case of \( n \). Case 1: \( n = \bot \). Case 2: \( n = a \) for some \( a \in \mathbb{N} \). -/
  cases n
/- First, we show that if \( n = \bot \), then \( \bot + m = 0 \) if and only if \( \bot = 0 \) and \( m = 0 \). Using the property that adding the bottom element to any element results in the bottom element, we simplify the goal to \( \bot = 0 \) and \( m = 0 \). Since \( \bot \neq 0 \), this case is vacuously true. -/
  · simp [WithBot.bot_add]
/- We will consider every possible case of \( m \). Case 1: \( m = \bot \). Case 2: \( m = b \) for some \( b \in \mathbb{N} \). -/
  cases m
/- First, we show that if \( m = \bot \), then \( a + \bot = 0 \) if and only if \( a = 0 \) and \( \bot = 0 \). Using the property that adding the bottom element to any element results in the bottom element, we simplify the goal to \( a = 0 \) and \( \bot = 0 \). Since \( \bot \neq 0 \), this case is vacuously true. -/
  · simp [WithBot.add_bot]
/- Finally, we show that if \( n = a \) and \( m = b \) for some \( a, b \in \mathbb{N} \), then \( a + b = 0 \) if and only if \( a = 0 \) and \( b = 0 \). Using the property that the sum of two nonnegative elements is zero if and only if both elements are zero, we simplify the goal to \( a = 0 \) and \( b = 0 \). This completes the proof. -/
  simp [← WithBot.coe_add, add_eq_zero_iff_of_nonneg]","import Mathlib.Algebra.Order.Ring.Nat
import Mathlib.Algebra.Order.Monoid.WithTop
import Mathlib.Data.Nat.Cast.WithTop
import Mathlib.Data.Nat.WithBot

open Nat
open WithBot

"
26083,Nat.WithBot.add_eq_three_iff,"theorem Nat.WithBot.add_eq_three_iff {n m : WithBot ℕ} :
    n + m = 3 ↔ n = 0 ∧ m = 3 ∨ n = 1 ∧ m = 2 ∨ n = 2 ∧ m = 1 ∨ n = 3 ∧ m = 0 := by sorry","Sum Equals Three in `WithBot ℕ` if and only if Specific Pairs of Elements : For any elements \( n \) and \( m \) in the type `WithBot ℕ`, the sum \( n + m \) is equal to 3 if and only if one of the following conditions holds:
- \( n = 0 \) and \( m = 3 \)
- \( n = 1 \) and \( m = 2 \)
- \( n = 2 \) and \( m = 1 \)
- \( n = 3 \) and \( m = 0 \)","theorem Nat.WithBot.add_eq_three_iff {n m : WithBot ℕ} :
    n + m = 3 ↔ n = 0 ∧ m = 3 ∨ n = 1 ∧ m = 2 ∨ n = 2 ∧ m = 1 ∨ n = 3 ∧ m = 0 := by
  cases n
  · simp [WithBot.bot_add]
  cases m
  · simp [WithBot.add_bot]
  simp [← WithBot.coe_add, Nat.add_eq_three_iff]","We need to show that for any elements \( n \) and \( m \) in the type `WithBot ℕ`, the sum \( n + m = 3 \) if and only if one of the specified pairs of elements holds. We will consider all possible cases for \( n \) and \( m \).

1. **Case 1: \( n = \bot \)**
- If \( n = \bot \), then \( \bot + m = 3 \). Since \( \bot \) is the bottom element, \( \bot + m = \bot \) for any \( m \). Therefore, \( \bot + m = 3 \) is false. The right-hand side of the equivalence is also false because each disjunct involves \( \bot \) being equal to a natural number, which is impossible. Hence, the equivalence holds trivially.

2. **Case 2: \( n = a \) for some \( a \in \mathbb{N} \)**
- **Subcase 2.1: \( m = \bot \)**
- If \( m = \bot \), then \( a + \bot = 3 \). Since \( \bot \) is the bottom element, \( a + \bot = \bot \) for any \( a \). Therefore, \( a + \bot = 3 \) is false. The right-hand side of the equivalence is also false because each disjunct involves \( \bot \) being equal to a natural number, which is impossible. Hence, the equivalence holds trivially.
- **Subcase 2.2: \( m = b \) for some \( b \in \mathbb{N} \)**
- If both \( n = a \) and \( m = b \) for some \( a, b \in \mathbb{N} \), the goal is to show that \( a + b = 3 \leftrightarrow a = 0 \land b = 3 \lor a = 1 \land b = 2 \lor a = 2 \land b = 1 \lor a = 3 \land b = 0 \). Using the fact that the canonical inclusion of natural numbers into `WithBot ℕ` preserves addition, we can simplify the goal to the known result that the sum of two natural numbers equals 3 if and only if one of the pairs \((0, 3)\), \((1, 2)\), \((2, 1)\), or \((3, 0)\) holds. This completes the proof.

Thus, we have shown that the sum \( n + m = 3 \) in `WithBot ℕ` if and only if one of the specified pairs of elements holds.","theorem Nat.WithBot.add_eq_three_iff {n m : WithBot ℕ} :
    n + m = 3 ↔ n = 0 ∧ m = 3 ∨ n = 1 ∧ m = 2 ∨ n = 2 ∧ m = 1 ∨ n = 3 ∧ m = 0 := by
/- We will consider every possible case for \( n \). Case 1: \( n = \bot \). Case 2: \( n = a \) for some \( a \in \mathbb{N} \). -/
  cases n
/- First, we show that if \( n = \bot \), then the goal \( \bot + m = 3 \leftrightarrow \bot = 0 \land m = 3 \lor \bot = 1 \land m = 2 \lor \bot = 2 \land m = 1 \lor \bot = 3 \land m = 0 \) simplifies to a contradiction. Since \( \bot \) is the bottom element, \( \bot + m = \bot \) for any \( m \). Therefore, \( \bot + m = 3 \) is false, and the right-hand side of the equivalence is also false because each disjunct involves \( \bot \) being equal to a natural number, which is impossible. Hence, the equivalence holds trivially. -/
  · simp [WithBot.bot_add]
/- We will consider every possible case for \( m \). Case 1: \( m = \bot \). Case 2: \( m = b \) for some \( b \in \mathbb{N} \). -/
  cases m
/- First, we show that if \( m = \bot \), then the goal \( a + \bot = 3 \leftrightarrow a = 0 \land \bot = 3 \lor a = 1 \land \bot = 2 \lor a = 2 \land \bot = 1 \lor a = 3 \land \bot = 0 \) simplifies to a contradiction. Since \( \bot \) is the bottom element, \( a + \bot = \bot \) for any \( a \). Therefore, \( a + \bot = 3 \) is false, and the right-hand side of the equivalence is also false because each disjunct involves \( \bot \) being equal to a natural number, which is impossible. Hence, the equivalence holds trivially. -/
  · simp [WithBot.add_bot]
/- Now, we consider the case where both \( n \) and \( m \) are natural numbers, i.e., \( n = a \) and \( m = b \) for some \( a, b \in \mathbb{N} \). The goal is to show that \( a + b = 3 \leftrightarrow a = 0 \land b = 3 \lor a = 1 \land b = 2 \lor a = 2 \land b = 1 \lor a = 3 \land b = 0 \). Using the fact that the canonical inclusion of natural numbers into `WithBot ℕ` preserves addition, we can simplify the goal to the known result that the sum of two natural numbers equals 3 if and only if one of the pairs \((0, 3)\), \((1, 2)\), \((2, 1)\), or \((3, 0)\) holds. This completes the proof. -/
  simp [← WithBot.coe_add, Nat.add_eq_three_iff]","import Mathlib.Algebra.Order.Ring.Nat
import Mathlib.Algebra.Order.Monoid.WithTop
import Mathlib.Data.Nat.Cast.WithTop
import Mathlib.Data.Nat.WithBot

open Nat
open WithBot

"
26115,IsLocalHomeomorphOn.comp,"theorem IsLocalHomeomorphOn.comp (hg : IsLocalHomeomorphOn g t) (hf : IsLocalHomeomorphOn f s)
    (h : Set.MapsTo f s t) : IsLocalHomeomorphOn (g ∘ f) s := by sorry","Composition of Local Homeomorphisms on Sets Preserves Local Homeomorphism Property : For topological spaces \( X \), \( Y \), and \( Z \), if \( g : Y \to Z \) is a local homeomorphism on a set \( t \subseteq Y \) and \( f : X \to Y \) is a local homeomorphism on a set \( s \subseteq X \), and if \( f \) maps \( s \) into \( t \), then the composition \( g \circ f : X \to Z \) is a local homeomorphism on \( s \).","theorem IsLocalHomeomorphOn.comp (hg : IsLocalHomeomorphOn g t) (hf : IsLocalHomeomorphOn f s)
    (h : Set.MapsTo f s t) : IsLocalHomeomorphOn (g ∘ f) s := by
  intro x hx
  obtain ⟨eg, hxg, rfl⟩ := hg (f x) (h hx)
  obtain ⟨ef, hxf, rfl⟩ := hf x hx
  exact ⟨ef.trans eg, ⟨hxf, hxg⟩, rfl⟩","Let \( x \) be an arbitrary element of \( X \) such that \( x \in s \). We need to show that there exists a partial homeomorphism \( e \) such that \( x \in e.source \) and \( g \circ f = e \).

Since \( f \) maps \( s \) into \( t \) and \( x \in s \), we have \( f(x) \in t \). By the assumption that \( g \) is a local homeomorphism on \( t \), there exists a partial homeomorphism \( e_g \) such that \( f(x) \in e_g.source \) and \( g = e_g \) on \( t \).

Since \( f \) is a local homeomorphism on \( s \) and \( x \in s \), there exists a partial homeomorphism \( e_f \) such that \( x \in e_f.source \) and \( f = e_f \) on \( s \).

We construct the partial homeomorphism \( e \) as the composition \( e_f \circ e_g \). Since \( x \in e_f.source \) and \( e_f(x) \in e_g.source \), it follows that \( x \in (e_f \circ e_g).source \). Therefore, \( g \circ f = e_f \circ e_g \) on \( s \).

This completes the proof. \(\blacksquare\)","theorem IsLocalHomeomorphOn.comp (hg : IsLocalHomeomorphOn g t) (hf : IsLocalHomeomorphOn f s)
    (h : Set.MapsTo f s t) : IsLocalHomeomorphOn (g ∘ f) s := by
/- Let \( x \) be an arbitrary element of \( X \) such that \( x \in s \). We need to show that there exists a partial homeomorphism \( e \) such that \( x \in e.source \) and \( g \circ f = e \). -/
  intro x hx
/- Since \( f \) maps \( s \) into \( t \) and \( x \in s \), we have \( f(x) \in t \). By the assumption that \( g \) is a local homeomorphism on \( t \), there exists a partial homeomorphism \( e_g \) such that \( f(x) \in e_g.source \) and \( g = e_g \) on \( t \). -/
  obtain ⟨eg, hxg, rfl⟩ := hg (f x) (h hx)
/- Since \( f \) is a local homeomorphism on \( s \) and \( x \in s \), there exists a partial homeomorphism \( e_f \) such that \( x \in e_f.source \) and \( f = e_f \) on \( s \). -/
  obtain ⟨ef, hxf, rfl⟩ := hf x hx
/- We construct the partial homeomorphism \( e \) as the composition \( e_f \circ e_g \). Since \( x \in e_f.source \) and \( e_f(x) \in e_g.source \), it follows that \( x \in (e_f \circ e_g).source \). Therefore, \( g \circ f = e_f \circ e_g \) on \( s \). This completes the proof. -/
  exact ⟨ef.trans eg, ⟨hxf, hxg⟩, rfl⟩","import Mathlib.Topology.PartialHomeomorph
import Mathlib.Topology.SeparatedMap
import Mathlib.Topology.IsLocalHomeomorph

open IsLocalHomeomorphOn
open Topology
variable {X Y Z : Type*} [TopologicalSpace X] [TopologicalSpace Y] [TopologicalSpace Z] (g : Y → Z)
  (f : X → Y) (s : Set X) (t : Set Y)
variable {g f s t}
"
26136,Nat.min_le_right,theorem Nat.min_le_right (a b : Nat) : min a b ≤ b := by sorry,"Minimum is Less Than or Equal to Right Argument in Natural Numbers : For any natural numbers \(a\) and \(b\), the minimum of \(a\) and \(b\) is less than or equal to \(b\), i.e., \(\min(a, b) \leq b\).","theorem Nat.min_le_right (a b : Nat) : min a b ≤ b := by
  by_cases (a <= b) <;> simp [Nat.min_def, *]","To prove that \(\min(a, b) \leq b\) for any natural numbers \(a\) and \(b\), we consider two cases based on the relationship between \(a\) and \(b\):

1. **Case 1: \(a \leq b\)**

By the definition of the minimum function, \(\min(a, b) = a\) if \(a \leq b\). Since we are assuming \(a \leq b\), it follows that \(\min(a, b) = a \leq b\). Therefore, \(\min(a, b) \leq b\) holds in this case.

2. **Case 2: \(a \not\leq b\)**

By the definition of the minimum function, \(\min(a, b) = b\) if \(a \not\leq b\). Since we are assuming \(a \not\leq b\), it follows that \(\min(a, b) = b\). Clearly, \(b \leq b\) is always true. Therefore, \(\min(a, b) \leq b\) holds in this case as well.

Since both cases cover all possible relationships between \(a\) and \(b\), we conclude that \(\min(a, b) \leq b\) for any natural numbers \(a\) and \(b\). This completes the proof. \(\blacksquare\)","theorem Nat.min_le_right (a b : Nat) : min a b ≤ b := by
/- Consider two cases: (1) Assume \( a \leq b \), and (2) Assume \( a \not\leq b \). -/
/- In the first case, where \( a \leq b \), we use the definition of the minimum function. The definition states that \(\min(a, b) = a\) if \( a \leq b \). Therefore, we need to show that \( a \leq b \), which is true by our assumption. Hence, \(\min(a, b) \leq b\). -/
/- In the second case, where \( a \not\leq b \), we again use the definition of the minimum function. The definition states that \(\min(a, b) = b\) if \( a \not\leq b \). Therefore, we need to show that \( b \leq b \), which is trivially true. Hence, \(\min(a, b) \leq b\). -/
  by_cases (a <= b) <;> simp [Nat.min_def, *]","import Init.ByCases
import Init.Data.Nat.MinMax

open Nat

"
26141,Nat.le_max_left,theorem Nat.le_max_left ( a b : Nat) : a ≤ max a b := by sorry,"Left Element is Less Than or Equal to Maximum in Natural Numbers : For any natural numbers \( a \) and \( b \), it holds that \( a \leq \max(a, b) \).","theorem Nat.le_max_left ( a b : Nat) : a ≤ max a b := by
  by_cases (a <= b) <;> simp [Nat.max_def, *]","To prove that \( a \leq \max(a, b) \) for any natural numbers \( a \) and \( b \), we consider two cases based on the relationship between \( a \) and \( b \).

1. **Case 1: \( a \leq b \)**
- By the definition of the maximum function, \(\max(a, b) = b\) if \( a \leq b \).
- Therefore, \( a \leq \max(a, b) \) simplifies to \( a \leq b \), which is true by our assumption.

2. **Case 2: \( a \not\leq b \)**
- By the definition of the maximum function, \(\max(a, b) = a\) if \( a \not\leq b \).
- Therefore, \( a \leq \max(a, b) \) simplifies to \( a \leq a \), which is always true by the reflexive property of the less than or equal to relation.

Since both cases cover all possible relationships between \( a \) and \( b \), we conclude that \( a \leq \max(a, b) \) holds for any natural numbers \( a \) and \( b \). This completes the proof.","theorem Nat.le_max_left ( a b : Nat) : a ≤ max a b := by
/- Consider two cases: (1) Assume \( a \leq b \), and (2) Assume \( a \not\leq b \). -/
/- In the first case, where \( a \leq b \), we use the definition of the maximum function, which states that \(\max(a, b) = b\) if \( a \leq b \). Therefore, \( a \leq \max(a, b) \) simplifies to \( a \leq b \), which is true by our assumption. -/
/- In the second case, where \( a \not\leq b \), we again use the definition of the maximum function, which states that \(\max(a, b) = a\) if \( a \not\leq b \). Therefore, \( a \leq \max(a, b) \) simplifies to \( a \leq a \), which is always true by the reflexive property of the less than or equal to relation. -/
  by_cases (a <= b) <;> simp [Nat.max_def, *]","import Init.ByCases
import Init.Data.Nat.MinMax

open Nat

"
26145,IsLocalHomeomorphOn.mk,"theorem IsLocalHomeomorphOn.mk (h : ∀ x ∈ s, ∃ e : PartialHomeomorph X Y, x ∈ e.source ∧ Set.EqOn f e e.source) :
    IsLocalHomeomorphOn f s := by sorry","Local Homeomorphism on a Set via Partial Homeomorphisms : For a function \( f : X \to Y \) between topological spaces and a set \( s \subseteq X \), if for each \( x \in s \) there exists a partial homeomorphism \( e : \text{PartialHomeomorph} X Y \) such that \( x \in e.\text{source} \) and \( f \) agrees with \( e \) on \( e.\text{source} \), then \( f \) satisfies \( \text{IsLocalHomeomorphOn} f s \).","theorem IsLocalHomeomorphOn.mk (h : ∀ x ∈ s, ∃ e : PartialHomeomorph X Y, x ∈ e.source ∧ Set.EqOn f e e.source) :
    IsLocalHomeomorphOn f s := by
  intro x hx
  obtain ⟨e, hx, he⟩ := h x hx
  exact
    ⟨{ e with
        toFun := f
        map_source' := fun _x hx ↦ by rw [he hx]; exact e.map_source' hx
        left_inv' := fun _x hx ↦ by rw [he hx]; exact e.left_inv' hx
        right_inv' := fun _y hy ↦ by rw [he (e.map_target' hy)]; exact e.right_inv' hy
        continuousOn_toFun := (continuousOn_congr he).mpr e.continuousOn_toFun },
      hx, rfl⟩","To prove that \( f \) satisfies \( \text{IsLocalHomeomorphOn } f s \), we need to show that for every \( x \in s \), there exists a partial homeomorphism \( e \) such that \( x \in e.\text{source} \) and \( f = e \).

1. Let \( x \) be an arbitrary element in \( s \). We need to find a partial homeomorphism \( e \) such that \( x \in e.\text{source} \) and \( f = e \).

2. By the hypothesis \( h \), for the element \( x \in s \), there exists a partial homeomorphism \( e \) such that \( x \in e.\text{source} \) and \( f \) agrees with \( e \) on \( e.\text{source} \). Let \( e \) be this partial homeomorphism, and let \( hx \) and \( he \) be the corresponding proofs.

3. We construct a new partial homeomorphism \( e' \) as follows:
- The forward map \( e'.\text{toFun} \) is defined as \( f \).
- For any \( x \in e.\text{source} \), we show that \( f(x) \in e.\text{target} \) by using the fact that \( f \) agrees with \( e \) on \( e.\text{source} \) and the property of \( e \) that maps elements of \( e.\text{source} \) to \( e.\text{target} \).
- For any \( x \in e.\text{source} \), we show that \( e'.\text{invFun}(f(x)) = x \) by using the fact that \( f \) agrees with \( e \) on \( e.\text{source} \) and the left inverse property of \( e \).
- For any \( y \in e.\text{target} \), we show that \( f(e'.\text{invFun}(y)) = y \) by using the fact that \( f \) agrees with \( e \) on \( e.\text{source} \) and the right inverse property of \( e \).
- The forward map \( f \) is continuous on \( e.\text{source} \) because \( f \) agrees with \( e \) on \( e.\text{source} \) and \( e \) is continuous on \( e.\text{source} \).

4. Thus, we have constructed a partial homeomorphism \( e' \) such that \( x \in e'.\text{source} \) and \( f = e' \). This completes the proof that \( f \) satisfies \( \text{IsLocalHomeomorphOn } f s \).","theorem IsLocalHomeomorphOn.mk (h : ∀ x ∈ s, ∃ e : PartialHomeomorph X Y, x ∈ e.source ∧ Set.EqOn f e e.source) :
    IsLocalHomeomorphOn f s := by
/- Let \( x \) be an arbitrary element in \( s \). We need to show that there exists a partial homeomorphism \( e \) such that \( x \in e.\text{source} \) and \( f = e \). -/
  intro x hx
/- By the hypothesis \( h \), for the element \( x \in s \), there exists a partial homeomorphism \( e \) such that \( x \in e.\text{source} \) and \( f \) agrees with \( e \) on \( e.\text{source} \). Let \( e \) be this partial homeomorphism, and let \( hx \) and \( he \) be the corresponding proofs. -/
  obtain ⟨e, hx, he⟩ := h x hx
/- We construct a new partial homeomorphism \( e' \) as follows:
- The forward map \( e'.\text{toFun} \) is defined as \( f \).
- For any \( x \in e.\text{source} \), we show that \( f(x) \in e.\text{target} \) by using the fact that \( f \) agrees with \( e \) on \( e.\text{source} \) and the property of \( e \) that maps elements of \( e.\text{source} \) to \( e.\text{target} \).
- For any \( x \in e.\text{source} \), we show that \( e'.\text{invFun}(f(x)) = x \) by using the fact that \( f \) agrees with \( e \) on \( e.\text{source} \) and the left inverse property of \( e \).
- For any \( y \in e.\text{target} \), we show that \( f(e'.\text{invFun}(y)) = y \) by using the fact that \( f \) agrees with \( e \) on \( e.\text{source} \) and the right inverse property of \( e \).
- The forward map \( f \) is continuous on \( e.\text{source} \) because \( f \) agrees with \( e \) on \( e.\text{source} \) and \( e \) is continuous on \( e.\text{source} \).

Thus, we have constructed a partial homeomorphism \( e' \) such that \( x \in e'.\text{source} \) and \( f = e' \). This completes the proof. -/
  exact
    ⟨{ e with
        toFun := f
        map_source' := fun _x hx ↦ by rw [he hx]; exact e.map_source' hx
        left_inv' := fun _x hx ↦ by rw [he hx]; exact e.left_inv' hx
        right_inv' := fun _y hy ↦ by rw [he (e.map_target' hy)]; exact e.right_inv' hy
        continuousOn_toFun := (continuousOn_congr he).mpr e.continuousOn_toFun },
      hx, rfl⟩","import Mathlib.Topology.PartialHomeomorph
import Mathlib.Topology.SeparatedMap
import Mathlib.Topology.IsLocalHomeomorph

open IsLocalHomeomorphOn
open Topology
variable {X Y Z : Type*} [TopologicalSpace X] [TopologicalSpace Y] [TopologicalSpace Z] (g : Y → Z)
  (f : X → Y) (s : Set X) (t : Set Y)
"
26168,Nat.max_comm,theorem Nat.max_comm (a b : Nat) : max a b = max b a := by sorry,"Commutativity of Maximum Operation in Natural Numbers: \(\max(a, b) = \max(b, a)\) : For any natural numbers \(a\) and \(b\), the maximum of \(a\) and \(b\) is equal to the maximum of \(b\) and \(a\), i.e., \(\max(a, b) = \max(b, a)\).","theorem Nat.max_comm (a b : Nat) : max a b = max b a := by
  simp only [Nat.max_def]
  by_cases h₁ : a ≤ b <;> by_cases h₂ : b ≤ a <;> simp [h₁, h₂]
  · exact Nat.le_antisymm h₂ h₁
  · cases not_or_intro h₁ h₂ <| Nat.le_total ..","We start by using the definition of the maximum function in a linear order. The maximum of \(a\) and \(b\) is defined as:
\[
\max(a, b) = \begin{cases}
b & \text{if } a \leq b \\
a & \text{if } a > b
\end{cases}
\]
Similarly, the maximum of \(b\) and \(a\) is:
\[
\max(b, a) = \begin{cases}
a & \text{if } b \leq a \\
b & \text{if } b > a
\end{cases}
\]
Thus, the goal \(\max(a, b) = \max(b, a)\) is equivalent to:
\[
(\text{if } a \leq b \text{ then } b \text{ else } a) = (\text{if } b \leq a \text{ then } a \text{ else } b)
\]

We consider two cases for \(a \leq b\) and \(b \leq a\):

1. **Case 1:** Assume \(a \leq b\).
- **Subcase 1.1:** Assume \(b \leq a\).
- Using the antisymmetry property of the less than or equal to relation, if \(a \leq b\) and \(b \leq a\), then \(a = b\). Therefore, the goal simplifies to \(b = a\), which is true.
- **Subcase 1.2:** Assume \(\neg (b \leq a)\).
- Since \(a \leq b\) and \(\neg (b \leq a)\), the goal simplifies to \(b = b\), which is trivially true.

2. **Case 2:** Assume \(\neg (a \leq b)\).
- **Subcase 2.1:** Assume \(b \leq a\).
- Since \(\neg (a \leq b)\) and \(b \leq a\), the goal simplifies to \(a = a\), which is trivially true.
- **Subcase 2.2:** Assume \(\neg (b \leq a)\).
- Since \(\neg (a \leq b)\) and \(\neg (b \leq a)\), we have a contradiction because the totality of the less than or equal to relation implies that either \(a \leq b\) or \(b \leq a\) must hold. Therefore, this subcase is impossible, and the goal simplifies to \(a = b\).

In all cases, the goal \(\max(a, b) = \max(b, a)\) is satisfied. Therefore, the theorem is proved. \(\blacksquare\)","theorem Nat.max_comm (a b : Nat) : max a b = max b a := by
/- First, we use the definition of the maximum function in a linear order to simplify the goal. The maximum of \(a\) and \(b\) is defined as \(b\) if \(a \leq b\), and \(a\) otherwise. Therefore, the goal \(\max(a, b) = \max(b, a)\) is equivalent to \((\text{if } a \leq b \text{ then } b \text{ else } a) = (\text{if } b \leq a \text{ then } a \text{ else } b)\). -/
  simp only [Nat.max_def]
/- We consider two cases for \(a \leq b\) and \(b \leq a\):

1. **Case 1:** Assume \(a \leq b\).
- We further consider two subcases for \(b \leq a\):
- **Subcase 1.1:** Assume \(b \leq a\).
- Using the assumptions \(a \leq b\) and \(b \leq a\), we simplify the goal to \(b = a\).
- **Subcase 1.2:** Assume \(\neg (b \leq a)\).
- Using the assumption \(a \leq b\) and \(\neg (b \leq a)\), we simplify the goal to \(b = b\), which is trivially true.

2. **Case 2:** Assume \(\neg (a \leq b)\).
- We further consider two subcases for \(b \leq a\):
- **Subcase 2.1:** Assume \(b \leq a\).
- Using the assumptions \(\neg (a \leq b)\) and \(b \leq a\), we simplify the goal to \(a = a\), which is trivially true.
- **Subcase 2.2:** Assume \(\neg (b \leq a)\).
- Using the assumptions \(\neg (a \leq b)\) and \(\neg (b \leq a)\), we simplify the goal to \(a = b\). -/
  by_cases h₁ : a ≤ b <;> by_cases h₂ : b ≤ a <;> simp [h₁, h₂]
/- In the subcase where \(a \leq b\) and \(b \leq a\), we use the antisymmetry property of the less than or equal to relation to conclude that \(b = a\). -/
  · exact Nat.le_antisymm h₂ h₁
/- In the subcase where \(\neg (a \leq b)\) and \(\neg (b \leq a)\), we use the totality of the less than or equal to relation to derive a contradiction. Since \(\neg (a \leq b)\) and \(\neg (b \leq a)\) imply that neither \(a \leq b\) nor \(b \leq a\) holds, this is a contradiction, and thus \(a = b\). -/
  · cases not_or_intro h₁ h₂ <| Nat.le_total ..","import Init.ByCases
import Init.Data.Nat.MinMax

open Nat

"
26669,Multiset.mem_Ioo,theorem Multiset.mem_Ioo : x ∈ Ioo a b ↔ a < x ∧ x < b := by sorry,"Membership in Open-Open Interval Multiset: \( x \in (a, b) \leftrightarrow a < x \land x < b \) : For any type \( \alpha \) equipped with a preorder and a locally finite order, and for any elements \( a, b, x \in \alpha \), the element \( x \) belongs to the open-open interval multiset \((a, b)\) if and only if \( a < x \) and \( x < b \).","theorem Multiset.mem_Ioo : x ∈ Ioo a b ↔ a < x ∧ x < b := by
  rw [Ioo, ← Finset.mem_def, Finset.mem_Ioo]","To prove the theorem, we start by using the definition of the open-open interval multiset \( \text{Multiset.Ioo} \, a \, b \). By definition, \( \text{Multiset.Ioo} \, a \, b \) is the multiset of elements \( x \) such that \( a < x \) and \( x < b \).

Next, we use the fact that membership in a finite set is equivalent to membership in its underlying multiset. Specifically, for any finite set \( s \), \( x \in s \) if and only if \( x \in s.val \). Applying this to the finite set \( \text{Finset.Ioo} \, a \, b \), we have \( x \in \text{Finset.Ioo} \, a \, b \) if and only if \( x \in (\text{Finset.Ioo} \, a \, b).val \).

Finally, we use the theorem that states \( x \in \text{Finset.Ioo} \, a \, b \) if and only if \( a < x \) and \( x < b \). Combining these steps, we conclude that \( x \in \text{Multiset.Ioo} \, a \, b \) if and only if \( a < x \) and \( x < b \).

Thus, the theorem is proved. \(\blacksquare\)","theorem Multiset.mem_Ioo : x ∈ Ioo a b ↔ a < x ∧ x < b := by
  rw [Ioo, ← Finset.mem_def, Finset.mem_Ioo]","import Mathlib.Order.Interval.Finset.Basic
import Mathlib.Order.Interval.Multiset

open Multiset
variable {α : Type*}
variable [Preorder α] [LocallyFiniteOrder α] {a b x : α}
"
26686,jacobiSym.zero_right,theorem jacobiSym.zero_right (a : ℤ) : J(a | 0) = 1 := by sorry,"Jacobi Symbol of Any Integer with Zero is One: \( J(a \mid 0) = 1 \) : For any integer \( a \), the Jacobi symbol \( J(a \mid 0) \) is equal to 1, i.e., \( J(a \mid 0) = 1 \).","theorem jacobiSym.zero_right (a : ℤ) : J(a | 0) = 1 := by
  simp only [jacobiSym, primeFactorsList_zero, List.prod_nil, List.pmap]","To prove that \( J(a \mid 0) = 1 \), we start by recalling the definition of the Jacobi symbol \( J(a \mid b) \). The Jacobi symbol is defined as the product of the Legendre symbols over the prime factors of \( b \). Specifically, for a natural number \( b \), the Jacobi symbol \( J(a \mid b) \) is given by:
\[
J(a \mid b) = \prod_{p \in \text{prime factors of } b} \left( \frac{a}{p} \right)
\]
where \(\left( \frac{a}{p} \right)\) is the Legendre symbol.

When \( b = 0 \), the prime factorization of \( 0 \) is the empty list. Therefore, the product of the Legendre symbols over the prime factors of \( 0 \) is the product of an empty list. By the definition of the product of an empty list, this product is \( 1 \). Hence, we have:
\[
J(a \mid 0) = \prod_{p \in \text{prime factors of } 0} \left( \frac{a}{p} \right) = 1
\]
This completes the proof.","theorem jacobiSym.zero_right (a : ℤ) : J(a | 0) = 1 := by
/- To prove that \( J(a \mid 0) = 1 \), we use the definition of the Jacobi symbol \( J(a \mid b) \) as the product of the Legendre symbols over the prime factors of \( b \). Since the prime factorization of \( 0 \) is the empty list, the product of an empty list is \( 1 \). Therefore, \( J(a \mid 0) = 1 \). -/
  simp only [jacobiSym, primeFactorsList_zero, List.prod_nil, List.pmap]","import Mathlib.NumberTheory.LegendreSymbol.QuadraticReciprocity
import Mathlib.NumberTheory.LegendreSymbol.JacobiSymbol

open jacobiSym
open Nat ZMod
open NumberTheorySymbols
"
26722,qrSign.mul_left,theorem qrSign.mul_left (m₁ m₂ n : ℕ) : qrSign (m₁ * m₂) n = qrSign m₁ n * qrSign m₂ n := by sorry,"Multiplicativity of Quadratic Reciprocity Sign in First Argument: \(\text{qrSign}(m_1 \cdot m_2, n) = \text{qrSign}(m_1, n) \cdot \text{qrSign}(m_2, n)\) : For any natural numbers \( m_1 \), \( m_2 \), and \( n \), the quadratic reciprocity sign function \( \text{qrSign} \) is multiplicative in the first argument, i.e., \( \text{qrSign}(m_1 \cdot m_2, n) = \text{qrSign}(m_1, n) \cdot \text{qrSign}(m_2, n) \).","theorem qrSign.mul_left (m₁ m₂ n : ℕ) : qrSign (m₁ * m₂) n = qrSign m₁ n * qrSign m₂ n := by
  simp_rw [qrSign, Nat.cast_mul, map_mul, jacobiSym.mul_left]","To prove the multiplicativity of the quadratic reciprocity sign function \( \text{qrSign} \) in the first argument, we start by using the definition of \( \text{qrSign} \):
\[
\text{qrSign}(m, n) = J(\chi_4(m) \mid n),
\]
where \( \chi_4(m) \) is the residue class of \( m \) modulo 4, and \( J(a \mid b) \) is the Jacobi symbol.

We need to show that:
\[
\text{qrSign}(m_1 \cdot m_2, n) = \text{qrSign}(m_1, n) \cdot \text{qrSign}(m_2, n).
\]

Using the definition of \( \text{qrSign} \), this is equivalent to:
\[
J(\chi_4(m_1 \cdot m_2) \mid n) = J(\chi_4(m_1) \mid n) \cdot J(\chi_4(m_2) \mid n).
\]

Next, we use the property that the canonical map from natural numbers to integers preserves multiplication:
\[
\chi_4(m_1 \cdot m_2) = \chi_4(m_1) \cdot \chi_4(m_2).
\]

Thus, the left-hand side becomes:
\[
J(\chi_4(m_1) \cdot \chi_4(m_2) \mid n).
\]

By the multiplicativity of the Jacobi symbol in the first argument, we have:
\[
J(\chi_4(m_1) \cdot \chi_4(m_2) \mid n) = J(\chi_4(m_1) \mid n) \cdot J(\chi_4(m_2) \mid n).
\]

Therefore, we have shown that:
\[
\text{qrSign}(m_1 \cdot m_2, n) = \text{qrSign}(m_1, n) \cdot \text{qrSign}(m_2, n).
\]

This completes the proof. \(\blacksquare\)","theorem qrSign.mul_left (m₁ m₂ n : ℕ) : qrSign (m₁ * m₂) n = qrSign m₁ n * qrSign m₂ n := by
/- First, we simplify the expression using the definition of the quadratic reciprocity sign function \( \text{qrSign} \), the property that the canonical map from natural numbers to integers preserves multiplication, the property that a multiplicative homomorphism preserves multiplication, and the multiplicativity of the Jacobi symbol in the first argument. This simplification shows that:
\[
\text{qrSign}(m_1 \cdot m_2, n) = J(\chi_4(m_1 \cdot m_2) \mid n)
\]
is equivalent to:
\[
J(\chi_4(m_1) \cdot \chi_4(m_2) \mid n) = J(\chi_4(m_1) \mid n) \cdot J(\chi_4(m_2) \mid n)
\]
which is true by the multiplicativity of the Jacobi symbol in the first argument. -/
  simp_rw [qrSign, Nat.cast_mul, map_mul, jacobiSym.mul_left]","import Mathlib.NumberTheory.LegendreSymbol.QuadraticReciprocity
import Mathlib.NumberTheory.LegendreSymbol.JacobiSymbol

open qrSign
open Nat ZMod
open NumberTheorySymbols
open jacobiSym
"
26723,qrSign.symm,theorem qrSign.symm {m n : ℕ} (hm : Odd m) (hn : Odd n) : qrSign m n = qrSign n m := by sorry,"Symmetry of Quadratic Reciprocity Sign for Odd Natural Numbers: \(\text{qrSign}(m, n) = \text{qrSign}(n, m)\) : For any odd natural numbers \( m \) and \( n \), the quadratic reciprocity sign function \( \text{qrSign} \) is symmetric, i.e., \( \text{qrSign}(m, n) = \text{qrSign}(n, m) \).","theorem qrSign.symm {m n : ℕ} (hm : Odd m) (hn : Odd n) : qrSign m n = qrSign n m := by
  rw [neg_one_pow hm hn, neg_one_pow hn hm, mul_comm (m / 2)]","To prove that \( \text{qrSign}(m, n) = \text{qrSign}(n, m) \) for any odd natural numbers \( m \) and \( n \), we start by using the property of the quadratic reciprocity sign function. Specifically, for any odd natural numbers \( m \) and \( n \), we have:
\[
\text{qrSign}(m, n) = (-1)^{\frac{m}{2} \cdot \frac{n}{2}}
\]
Similarly,
\[
\text{qrSign}(n, m) = (-1)^{\frac{n}{2} \cdot \frac{m}{2}}
\]
Since multiplication is commutative, we have:
\[
\frac{m}{2} \cdot \frac{n}{2} = \frac{n}{2} \cdot \frac{m}{2}
\]
Therefore,
\[
(-1)^{\frac{m}{2} \cdot \frac{n}{2}} = (-1)^{\frac{n}{2} \cdot \frac{m}{2}}
\]
This implies:
\[
\text{qrSign}(m, n) = \text{qrSign}(n, m)
\]
Thus, the quadratic reciprocity sign function is symmetric for any odd natural numbers \( m \) and \( n \). This completes the proof. \(\blacksquare\)","theorem qrSign.symm {m n : ℕ} (hm : Odd m) (hn : Odd n) : qrSign m n = qrSign n m := by
/- First, we use the fact that for any odd natural numbers \( m \) and \( n \), the quadratic reciprocity sign function \( \text{qrSign}(m, n) \) is equal to \((-1)^{\frac{m}{2} \cdot \frac{n}{2}}\). Applying this to both \( \text{qrSign}(m, n) \) and \( \text{qrSign}(n, m) \), we get:
\[
\text{qrSign}(m, n) = (-1)^{\frac{m}{2} \cdot \frac{n}{2}}
\]
and
\[
\text{qrSign}(n, m) = (-1)^{\frac{n}{2} \cdot \frac{m}{2}}
\]
Since multiplication is commutative, \(\frac{m}{2} \cdot \frac{n}{2} = \frac{n}{2} \cdot \frac{m}{2}\). Therefore, we have:
\[
(-1)^{\frac{m}{2} \cdot \frac{n}{2}} = (-1)^{\frac{n}{2} \cdot \frac{m}{2}}
\]
Thus, it suffices to show that:
\[
\text{qrSign}(m, n) = \text{qrSign}(n, m)
\] -/
  rw [neg_one_pow hm hn, neg_one_pow hn hm, mul_comm (m / 2)]","import Mathlib.NumberTheory.LegendreSymbol.QuadraticReciprocity
import Mathlib.NumberTheory.LegendreSymbol.JacobiSymbol

open qrSign
open Nat ZMod
open NumberTheorySymbols
open jacobiSym
"
26726,jacobiSym.quadratic_reciprocity_one_mod_four,"theorem jacobiSym.quadratic_reciprocity_one_mod_four {a b : ℕ} (ha : a % 4 = 1) (hb : Odd b) :
    J(a | b) = J(b | a) := by sorry","Quadratic Reciprocity for Jacobi Symbol: \( J(a \mid b) = J(b \mid a) \) when \( a \equiv 1 \pmod{4} \) and \( b \) is odd : For any natural numbers \( a \) and \( b \) such that \( a \equiv 1 \pmod{4} \) and \( b \) is odd, the Jacobi symbol \( J(a \mid b) \) is equal to \( J(b \mid a) \).","theorem jacobiSym.quadratic_reciprocity_one_mod_four {a b : ℕ} (ha : a % 4 = 1) (hb : Odd b) :
    J(a | b) = J(b | a) := by
  rw [quadratic_reciprocity (odd_iff.mpr (odd_of_mod_four_eq_one ha)) hb, pow_mul,
    neg_one_pow_div_two_of_one_mod_four ha, one_pow, one_mul]","We start by noting that the quadratic reciprocity law for the Jacobi symbol states that for any odd natural numbers \( a \) and \( b \):
\[ J(a \mid b) = (-1)^{\frac{a}{2} \cdot \frac{b}{2}} \cdot J(b \mid a). \]
Given that \( a \equiv 1 \pmod{4} \), we know that \( a \) is odd. Therefore, we can apply the quadratic reciprocity law to get:
\[ J(a \mid b) = (-1)^{\frac{a}{2} \cdot \frac{b}{2}} \cdot J(b \mid a). \]
Next, we use the property that for any natural number \( n \equiv 1 \pmod{4} \), \( (-1)^{\frac{n}{2}} = 1 \). Applying this to \( a \), we get:
\[ (-1)^{\frac{a}{2}} = 1. \]
Thus, the expression simplifies to:
\[ (-1)^{\frac{a}{2} \cdot \frac{b}{2}} = 1^{\frac{b}{2}} = 1. \]
Finally, we use the property that \( 1 \cdot J(b \mid a) = J(b \mid a) \). Therefore, we have:
\[ J(a \mid b) = J(b \mid a). \]
This completes the proof. \(\blacksquare\)","theorem jacobiSym.quadratic_reciprocity_one_mod_four {a b : ℕ} (ha : a % 4 = 1) (hb : Odd b) :
    J(a | b) = J(b | a) := by
/- First, we use the quadratic reciprocity law for the Jacobi symbol, which states that for any odd natural numbers \( a \) and \( b \), \( J(a \mid b) = (-1)^{\frac{a}{2} \cdot \frac{b}{2}} \cdot J(b \mid a) \). Since \( a \equiv 1 \pmod{4} \), we know that \( a \) is odd. Therefore, we can apply the quadratic reciprocity law to get:
\[ J(a \mid b) = (-1)^{\frac{a}{2} \cdot \frac{b}{2}} \cdot J(b \mid a). \]
Next, we use the property that for any natural number \( n \equiv 1 \pmod{4} \), \( (-1)^{\frac{n}{2}} = 1 \). Applying this to \( a \), we get:
\[ (-1)^{\frac{a}{2}} = 1. \]
Thus, the expression simplifies to:
\[ (-1)^{\frac{a}{2} \cdot \frac{b}{2}} = 1^{\frac{b}{2}} = 1. \]
Finally, we use the property that \( 1 \cdot J(b \mid a) = J(b \mid a) \). Therefore, we have:
\[ J(a \mid b) = J(b \mid a). \] -/
  rw [quadratic_reciprocity (odd_iff.mpr (odd_of_mod_four_eq_one ha)) hb, pow_mul,
    neg_one_pow_div_two_of_one_mod_four ha, one_pow, one_mul]","import Mathlib.NumberTheory.LegendreSymbol.QuadraticReciprocity
import Mathlib.NumberTheory.LegendreSymbol.JacobiSymbol

open jacobiSym
open Nat ZMod
open NumberTheorySymbols
open jacobiSym
"
26748,jacobiSym.mod_right,theorem jacobiSym.mod_right (a : ℤ) {b : ℕ} (hb : Odd b) : J(a | b) = J(a | b % (4 * a.natAbs)) := by sorry,"Jacobi Symbol Modulo \( 4|a| \) for Odd \( b \) : For any integer \( a \) and any odd natural number \( b \), the Jacobi symbol \( J(a \mid b) \) is equal to \( J(a \mid b \mod 4|a|) \), i.e., \( J(a \mid b) = J(a \mid b \mod 4|a|) \).","theorem jacobiSym.mod_right (a : ℤ) {b : ℕ} (hb : Odd b) : J(a | b) = J(a | b % (4 * a.natAbs)) := by
  cases' Int.natAbs_eq a with ha ha <;> nth_rw 2 [ha] <;> nth_rw 1 [ha]
  · exact mod_right' a.natAbs hb
  · have hb' : Odd (b % (4 * a.natAbs)) := hb.mod_even (Even.mul_right (by decide) _)
    rw [jacobiSym.neg _ hb, jacobiSym.neg _ hb', mod_right' _ hb, χ₄_nat_mod_four,
      χ₄_nat_mod_four (b % (4 * _)), mod_mod_of_dvd b (dvd_mul_right 4 _)]","We start by considering the two cases for the integer \( a \):
1. **Case 1: \( a = \text{natAbs}(a) \)**
- In this case, we need to show that \( J(\text{natAbs}(a) \mid b) = J(\text{natAbs}(a) \mid b \mod 4 \cdot \text{natAbs}(a)) \).
- By the theorem that for any natural number \( a \) and any odd natural number \( b \), \( J(a \mid b) = J(a \mid b \mod 4a) \), the goal is directly proved.

2. **Case 2: \( a = -\text{natAbs}(a) \)**
- In this case, we need to show that \( J(-\text{natAbs}(a) \mid b) = J(-\text{natAbs}(a) \mid b \mod 4 \cdot \text{natAbs}(a)) \).
- First, we show that \( b \mod (4 \cdot \text{natAbs}(a)) \) is odd. Since \( b \) is odd and the modulo of an odd number by an even number is odd, \( b \mod (4 \cdot \text{natAbs}(a)) \) is odd.
- Using the theorem that for any integer \( a \) and any odd natural number \( b \), \( J(-a \mid b) = \chi_4(b) \cdot J(a \mid b) \), we have:
\[
J(-\text{natAbs}(a) \mid b) = \chi_4(b) \cdot J(\text{natAbs}(a) \mid b)
\]
- Similarly, for \( b \mod (4 \cdot \text{natAbs}(a)) \):
\[
J(-\text{natAbs}(a) \mid b \mod (4 \cdot \text{natAbs}(a))) = \chi_4(b \mod (4 \cdot \text{natAbs}(a))) \cdot J(\text{natAbs}(a) \mid b \mod (4 \cdot \text{natAbs}(a)))
\]
- By the theorem that for any natural number \( a \) and any odd natural number \( b \), \( J(a \mid b) = J(a \mid b \mod 4a) \), we have:
\[
J(\text{natAbs}(a) \mid b) = J(\text{natAbs}(a) \mid b \mod (4 \cdot \text{natAbs}(a)))
\]
- Using the property that \( \chi_4(n) = \chi_4(n \mod 4) \) for any natural number \( n \), we have:
\[
\chi_4(b) = \chi_4(b \mod 4)
\]
- Since \( b \mod (4 \cdot \text{natAbs}(a)) \mod 4 = b \mod 4 \), we have:
\[
\chi_4(b \mod (4 \cdot \text{natAbs}(a))) = \chi_4(b)
\]
- Therefore:
\[
J(-\text{natAbs}(a) \mid b) = \chi_4(b) \cdot J(\text{natAbs}(a) \mid b) = \chi_4(b) \cdot J(\text{natAbs}(a) \mid b \mod (4 \cdot \text{natAbs}(a))) = J(-\text{natAbs}(a) \mid b \mod (4 \cdot \text{natAbs}(a)))
\]

Thus, in both cases, the goal is proved. Therefore, the theorem is true. \(\blacksquare\)","theorem jacobiSym.mod_right (a : ℤ) {b : ℕ} (hb : Odd b) : J(a | b) = J(a | b % (4 * a.natAbs)) := by
/- We consider two cases for the integer \( a \): either \( a = \text{natAbs}(a) \) or \( a = -\text{natAbs}(a) \). In the first case, we replace the second occurrence of \( a \) in the goal with \( \text{natAbs}(a) \), and in the second case, we replace the first occurrence of \( a \) in the goal with \( -\text{natAbs}(a) \). -/
  cases' Int.natAbs_eq a with ha ha <;> nth_rw 2 [ha] <;> nth_rw 1 [ha]
/- In the first case where \( a = \text{natAbs}(a) \), we use the theorem that for any natural number \( a \) and any odd natural number \( b \), the Jacobi symbol \( J(a \mid b) \) is equal to \( J(a \mid b \mod 4a) \). This theorem directly proves the goal. -/
  · exact mod_right' a.natAbs hb
/- In the second case where \( a = -\text{natAbs}(a) \), we first show that \( b \mod (4 \cdot \text{natAbs}(a)) \) is odd. This is done by using the fact that \( b \) is odd and the property that the modulo of an odd number by an even number is odd. -/
  · have hb' : Odd (b % (4 * a.natAbs)) := hb.mod_even (Even.mul_right (by decide) _)
/- We use the properties of the Jacobi symbol and the quadratic character \( \chi_4 \) to simplify the goal. Specifically:
1. We use the theorem that for any integer \( a \) and any odd natural number \( b \), \( J(-a \mid b) = \chi_4(b) \cdot J(a \mid b) \).
2. We apply this theorem to both \( b \) and \( b \mod (4 \cdot \text{natAbs}(a)) \).
3. We use the theorem that for any natural number \( a \) and any odd natural number \( b \), \( J(a \mid b) = J(a \mid b \mod 4a) \).
4. We use the property that \( \chi_4(n) = \chi_4(n \mod 4) \) for any natural number \( n \).
5. We use the modulo identity \( a \% b \% c = a \% c \) when \( c \mid b \) to simplify the expression. -/
    rw [jacobiSym.neg _ hb, jacobiSym.neg _ hb', mod_right' _ hb, χ₄_nat_mod_four,
      χ₄_nat_mod_four (b % (4 * _)), mod_mod_of_dvd b (dvd_mul_right 4 _)]","import Mathlib.NumberTheory.LegendreSymbol.QuadraticReciprocity
import Mathlib.NumberTheory.LegendreSymbol.JacobiSymbol

open jacobiSym
open Nat ZMod
open NumberTheorySymbols
open jacobiSym
"
28037,Option.elim'_eq_elim,"theorem Option.elim'_eq_elim {α β : Type*} (b : β) (f : α → β) (a : Option α) :
    Option.elim' b f a = Option.elim a b f := by sorry","Equivalence of Option Elimination Principles: $\text{Option.elim'}(b, f, a) = a.\text{elim}(b, f)$ : For any types $\alpha$ and $\beta$, a default value $b \in \beta$, and a function $f : \alpha \to \beta$, the elimination principle `Option.elim'` applied to an `Option` value $a$ of type `Option α` is equal to the elimination principle `Option.elim` applied to the same `Option` value $a$, the default value $b$, and the function $f$. In other words, for any `Option` value $a$, the following holds:
\[ \text{Option.elim'}(b, f, a) = a.\text{elim}(b, f) \]","theorem Option.elim'_eq_elim {α β : Type*} (b : β) (f : α → β) (a : Option α) :
    Option.elim' b f a = Option.elim a b f := by
  cases a <;> rfl","We will consider every possible case of the `Option` value $a$.

1. **Case 1: $a = \text{none}$**
- The goal is to show that $\text{Option.elim'}(b, f, \text{none}) = \text{none}.\text{elim}(b, f)$.
- By the definition of `Option.elim'` and `Option.elim`, both sides of the equation are equal to $b$.
- Therefore, the equality holds trivially due to the reflexive property.

2. **Case 2: $a = \text{some val}$**
- The goal is to show that $\text{Option.elim'}(b, f, \text{some val}) = (\text{some val}).\text{elim}(b, f)$.
- By the definition of `Option.elim'` and `Option.elim`, both sides of the equation are equal to $f(val)$.
- Therefore, the equality holds trivially due to the reflexive property.

Since both cases are covered and the equality holds in each case, the theorem is proved. $\blacksquare$","theorem Option.elim'_eq_elim {α β : Type*} (b : β) (f : α → β) (a : Option α) :
    Option.elim' b f a = Option.elim a b f := by
/- We will consider every possible case of the `Option` value $a$. There are two cases: $a$ is `none` or $a$ is `some val` for some value `val` in $\alpha$.

1. **Case 1: $a = \text{none}$**
- The goal is to show that $\text{Option.elim'}(b, f, \text{none}) = \text{none}.\text{elim}(b, f)$.
- By the definition of `Option.elim'` and `Option.elim`, both sides of the equation are equal to $b$.
- Therefore, the equality holds trivially due to the reflexive property.

2. **Case 2: $a = \text{some val}$**
- The goal is to show that $\text{Option.elim'}(b, f, \text{some val}) = (\text{some val}).\text{elim}(b, f)$.
- By the definition of `Option.elim'` and `Option.elim`, both sides of the equation are equal to $f(val)$.
- Therefore, the equality holds trivially due to the reflexive property. -/
  cases a <;> rfl","import Mathlib.Tactic.Lemma
import Mathlib.Tactic.TypeStar
import Mathlib.Data.Option.Defs

open Option
variable {α : Type*} {β : Type*}
"
28495,NNReal.rpow_eq_zero_iff,theorem NNReal.rpow_eq_zero_iff {x : ℝ≥0} {y : ℝ} : x ^ y = 0 ↔ x = 0 ∧ y ≠ 0 := by sorry,"Power of Non-negative Real Number is Zero if and only if Base is Zero and Exponent is Nonzero: \( x^y = 0 \leftrightarrow x = 0 \land y \neq 0 \) : For any non-negative real number \( x \in \mathbb{R}_{\geq 0} \) and any real number \( y \in \mathbb{R} \), the expression \( x^y = 0 \) holds if and only if \( x = 0 \) and \( y \neq 0 \).","theorem NNReal.rpow_eq_zero_iff {x : ℝ≥0} {y : ℝ} : x ^ y = 0 ↔ x = 0 ∧ y ≠ 0 := by
  rw [← NNReal.coe_inj, coe_rpow, ← NNReal.coe_eq_zero]
  exact Real.rpow_eq_zero_iff_of_nonneg x.2","We start by using the injectivity of the canonical map from nonnegative reals to reals. Specifically, for any nonnegative real numbers \( r_1 \) and \( r_2 \), \( \text{toReal}(r_1) = \text{toReal}(r_2) \) if and only if \( r_1 = r_2 \). This allows us to rewrite the goal \( x^y = 0 \) as \( \text{toReal}(x^y) = 0 \).

Next, we use the property that the canonical map preserves the power operation. For any non-negative real number \( x \in \mathbb{R}_{\geq 0} \) and any real number \( y \in \mathbb{R} \), \( \text{toReal}(x^y) = (\text{toReal}(x))^y \). Therefore, \( \text{toReal}(x^y) = 0 \) is equivalent to \( (\text{toReal}(x))^y = 0 \).

Finally, we use the fact that \( \text{toReal}(r) = 0 \) if and only if \( r = 0 \). This means that \( (\text{toReal}(x))^y = 0 \) is equivalent to \( \text{toReal}(x) = 0 \) and \( y \neq 0 \).

Since \( x \) is a nonnegative real number, we can apply the theorem that for any real numbers \( x \) and \( y \) such that \( 0 \leq x \), the power function \( x^y \) is equal to 0 if and only if \( x = 0 \) and \( y \neq 0 \). This theorem directly applies, and thus \( \text{toReal}(x) = 0 \) and \( y \neq 0 \) is equivalent to \( x = 0 \) and \( y \neq 0 \).

Therefore, the expression \( x^y = 0 \) holds if and only if \( x = 0 \) and \( y \neq 0 \). This completes the proof. \(\blacksquare\)","theorem NNReal.rpow_eq_zero_iff {x : ℝ≥0} {y : ℝ} : x ^ y = 0 ↔ x = 0 ∧ y ≠ 0 := by
/- First, we use the injectivity of the canonical map from nonnegative reals to reals to rewrite the goal. Specifically, we use the fact that \( \text{toReal}(r_1) = \text{toReal}(r_2) \) if and only if \( r_1 = r_2 \). Then, we use the property that the canonical map preserves the power operation, i.e., \( \text{toReal}(x^y) = (\text{toReal}(x))^y \). Finally, we use the fact that \( \text{toReal}(r) = 0 \) if and only if \( r = 0 \). Combining these, the goal \( x^y = 0 \) is equivalent to \( \text{toReal}(x^y) = 0 \), which is further equivalent to \( \text{toReal}(x) = 0 \) and \( y \neq 0 \). -/
  rw [← NNReal.coe_inj, coe_rpow, ← NNReal.coe_eq_zero]
/- The current goal \( \text{toReal}(x) = 0 \) and \( y \neq 0 \) is exactly proved by the theorem that for any real numbers \( x \) and \( y \) such that \( 0 \leq x \), the power function \( x^y \) is equal to 0 if and only if \( x = 0 \) and \( y \neq 0 \). Since \( x \) is a nonnegative real number, this theorem directly applies, and the proof is complete. -/
  exact Real.rpow_eq_zero_iff_of_nonneg x.2","import Mathlib.Analysis.SpecialFunctions.Pow.Real
import Mathlib.Analysis.SpecialFunctions.Pow.NNReal

open NNReal
open scoped Classical
open Real NNReal ENNReal ComplexConjugate
open Finset Function Set
variable {w x y z : ℝ}
"
28524,ENNReal.rpow_one,theorem ENNReal.rpow_one (x : ℝ≥0∞) : x ^ (1 : ℝ) = x := by sorry,"Power of One Identity: \( x^1 = x \) for Extended Nonnegative Real Numbers : For any extended nonnegative real number \( x \in [0, \infty] \), the power function \( x^1 \) is equal to \( x \), i.e., \( x^1 = x \).","theorem ENNReal.rpow_one (x : ℝ≥0∞) : x ^ (1 : ℝ) = x := by
  cases x
  · exact dif_pos zero_lt_one
  · change ite _ _ _ = _
    simp only [NNReal.rpow_one, some_eq_coe, ite_eq_right_iff, top_ne_coe, and_imp]
    exact fun _ => zero_le_one.not_lt","We will consider every possible case of \( x \).

1. **Case 1: \( x = \infty \)**
- We need to show that \( \infty^1 = \infty \).
- The power function for \( \infty \) is defined such that \( \infty^1 = \infty \).
- Specifically, the dependent if-then-else construct evaluates to the true branch when the condition \( 0 < 1 \) is true, which is always the case.
- Therefore, \( \infty^1 = \infty \).

2. **Case 2: \( x \) is a non-negative real number**
- We need to show that \( x^1 = x \).
- We can rewrite the goal using the if-then-else construct, which simplifies to \( ( \text{if } x = 0 \land 1 < 0 \text{ then } \infty \text{ else } x^1 ) = x \).
- Using the properties of the power function for non-negative real numbers, the if-then-else construct, and logical equivalences, we can simplify the goal.
- Specifically, we use the fact that \( x^1 = x \) for non-negative real numbers, the identity function, and the property that \( \infty \) is not equal to any non-negative real number.
- This simplifies the goal to \( x = 0 \to 1 < 0 \to \text{False} \).
- Since \( 0 \leq 1 \) is always true, the negation of \( 1 < 0 \) is true, i.e., \( 1 \leq 0 \) is false.
- Therefore, the implication \( 1 < 0 \to \text{False} \) is true, and the entire goal is satisfied.

Thus, we have shown that for any extended nonnegative real number \( x \), \( x^1 = x \). This completes the proof.","theorem ENNReal.rpow_one (x : ℝ≥0∞) : x ^ (1 : ℝ) = x := by
/- We will consider every possible case of \( x \). Case 1: \( x = \infty \). Case 2: \( x \) is a non-negative real number. -/
  cases x
/- For the case \( x = \infty \), we need to show that \( \infty^1 = \infty \). This is true because the power function for \( \infty \) is defined such that \( \infty^1 = \infty \). Specifically, the dependent if-then-else construct evaluates to the true branch when the condition \( 0 < 1 \) is true, which is always the case. Therefore, \( \infty^1 = \infty \). -/
  · exact dif_pos zero_lt_one
/- For the case \( x \) is a non-negative real number, we need to show that \( x^1 = x \). We can rewrite the goal using the if-then-else construct, which simplifies to \( ( \text{if } x = 0 \land 1 < 0 \text{ then } \infty \text{ else } x^1 ) = x \). -/
  · change ite _ _ _ = _
/- Using the properties of the power function for non-negative real numbers, the if-then-else construct, and logical equivalences, we can simplify the goal. Specifically, we use the fact that \( x^1 = x \) for non-negative real numbers, the identity function, and the property that \( \infty \) is not equal to any non-negative real number. This simplifies the goal to \( x = 0 \to 1 < 0 \to \text{False} \). -/
    simp only [NNReal.rpow_one, some_eq_coe, ite_eq_right_iff, top_ne_coe, and_imp]
/- The current goal is to show that \( x = 0 \to 1 < 0 \to \text{False} \). Since \( 0 \leq 1 \) is always true, the negation of \( 1 < 0 \) is true, i.e., \( 1 \leq 0 \) is false. Therefore, the implication \( 1 < 0 \to \text{False} \) is true, and the entire goal is satisfied. -/
    exact fun _ => zero_le_one.not_lt","import Mathlib.Analysis.SpecialFunctions.Pow.Real
import Mathlib.Analysis.SpecialFunctions.Pow.NNReal

open ENNReal
open scoped Classical
open Real NNReal ENNReal ComplexConjugate
open Finset Function Set
variable {w x y z : ℝ}
"
28595,LSeriesHasSum.neg,"theorem LSeriesHasSum.neg {f : ℕ → ℂ} {s a : ℂ} (hf : LSeriesHasSum f s a) :
    LSeriesHasSum (-f) s (-a) := by sorry","Negation of L-series Convergence: \(\sum_{n=1}^{\infty} \text{term}(-f, s, n) = -a\) : For any function \( f : \mathbb{N} \to \mathbb{C} \), any complex number \( s \in \mathbb{C} \), and any complex number \( a \in \mathbb{C} \), if the L-series of \( f \) converges at \( s \) to \( a \), then the L-series of \(-f\) converges at \( s \) to \(-a\). Mathematically, this is expressed as:
\[ \text{If } \sum_{n=1}^{\infty} \text{term}(f, s, n) = a, \text{ then } \sum_{n=1}^{\infty} \text{term}(-f, s, n) = -a. \]","theorem LSeriesHasSum.neg {f : ℕ → ℂ} {s a : ℂ} (hf : LSeriesHasSum f s a) :
    LSeriesHasSum (-f) s (-a) := by
  simpa only [LSeriesHasSum, term_neg] using HasSum.neg hf","Assume that the L-series of \( f \) converges at \( s \) to \( a \). This means:
\[ \sum_{n=1}^{\infty} \text{term}(f, s, n) = a. \]
We need to show that the L-series of \(-f\) converges at \( s \) to \(-a\). By the definition of the L-series, the \( n \)-th term of the L-series of \(-f\) evaluated at \( s \) is:
\[ \text{term}(-f, s, n) = -\text{term}(f, s, n). \]
Since the L-series of \( f \) converges to \( a \), the L-series of \(-f\) converges to \(-a\) by the property of the negation of an unconditional sum in a commutative topological additive group. Therefore, we have:
\[ \sum_{n=1}^{\infty} \text{term}(-f, s, n) = -a. \]
This completes the proof.","theorem LSeriesHasSum.neg {f : ℕ → ℂ} {s a : ℂ} (hf : LSeriesHasSum f s a) :
    LSeriesHasSum (-f) s (-a) := by
/- To prove that the L-series of \(-f\) converges at \( s \) to \(-a\), we use the fact that the L-series of \( f \) converges at \( s \) to \( a \). Specifically, we use the property that the \( n \)-th term of the L-series of \(-f\) evaluated at \( s \) is the negation of the \( n \)-th term of the L-series of \( f \) evaluated at \( s \). This means:
\[ \text{term}(-f, s, n) = -\text{term}(f, s, n) \]
for all \( n \in \mathbb{N} \). Since the L-series of \( f \) converges to \( a \), the L-series of \(-f\) converges to \(-a\) by the property of the negation of an unconditional sum in a commutative topological additive group. Therefore, the L-series of \(-f\) converges at \( s \) to \(-a\). -/
  simpa only [LSeriesHasSum, term_neg] using HasSum.neg hf","import Mathlib.NumberTheory.LSeries.Basic
import Mathlib.NumberTheory.LSeries.Linearity

open LSeriesHasSum
open LSeries
"
29622,ZMod.exists_sq_eq_neg_two_iff,theorem ZMod.exists_sq_eq_neg_two_iff (hp : p ≠ 2) : IsSquare (-2 : ZMod p) ↔ p % 8 = 1 ∨ p % 8 = 3 := by sorry,"Quadratic Residue of \(-2\) Modulo Prime: \(-2\) is a Square Modulo \( p \) if and only if \( p \equiv 1 \pmod{8} \) or \( p \equiv 3 \pmod{8} \) : For any prime number \( p \) such that \( p \neq 2 \), the number \(-2\) is a square modulo \( p \) if and only if \( p \) is congruent to 1 or 3 modulo 8. In other words, \(-2\) is a quadratic residue modulo \( p \) if and only if \( p \equiv 1 \pmod{8} \) or \( p \equiv 3 \pmod{8} \).","theorem ZMod.exists_sq_eq_neg_two_iff (hp : p ≠ 2) : IsSquare (-2 : ZMod p) ↔ p % 8 = 1 ∨ p % 8 = 3 := by
  rw [FiniteField.isSquare_neg_two_iff, card p]
  have h₁ := Prime.mod_two_eq_one_iff_ne_two.mpr hp
  rw [← mod_mod_of_dvd p (by decide : 2 ∣ 8)] at h₁
  have h₂ := mod_lt p (by norm_num : 0 < 8)
  revert h₂ h₁
  generalize p % 8 = m; clear! p
  intros; interval_cases m <;> simp_all","We start by using the equivalence that \(-2\) is a square in a finite field \( F \) if and only if the cardinality of \( F \) is not congruent to 5 or 7 modulo 8. Since the cardinality of \( \mathbb{Z}/p\mathbb{Z} \) is \( p \), the goal is to show that \( p \mod 8 \neq 5 \) and \( p \mod 8 \neq 7 \) if and only if \( p \mod 8 = 1 \) or \( p \mod 8 = 3 \).

Given that \( p \neq 2 \), we know that \( p \mod 2 = 1 \). This is because a prime number \( p \) is congruent to 1 modulo 2 if and only if \( p \neq 2 \).

Next, we use the property that \( p \mod 8 \mod 2 = p \mod 2 \) because 2 divides 8. Since \( p \mod 2 = 1 \), it follows that \( p \mod 8 \mod 2 = 1 \).

We also know that \( p \mod 8 < 8 \) because the remainder of any number when divided by 8 is always less than 8.

We strengthen the proposition we are proving. We show that for every \( m \) such that \( m \mod 2 = 1 \) and \( m < 8 \), the statement \( m \neq 5 \) and \( m \neq 7 \) is equivalent to \( m = 1 \) or \( m = 3 \).

We replace \( p \mod 8 \) with a new variable \( m \) and clear the variable \( p \) from the context. Now, we need to show that \( m \mod 2 = 1 \) and \( m < 8 \) implies \( m \neq 5 \) and \( m \neq 7 \) if and only if \( m = 1 \) or \( m = 3 \).

We introduce the hypotheses \( m \mod 2 = 1 \) and \( m < 8 \). We then consider all possible values of \( m \) in the interval from 0 to 7. By simplifying, we find that the only values of \( m \) that satisfy \( m \mod 2 = 1 \) and \( m < 8 \) are 1, 3, 5, and 7. Since \( m \neq 5 \) and \( m \neq 7 \), the only remaining values are 1 and 3. Therefore, \( m = 1 \) or \( m = 3 \).

Thus, we have shown that \(-2\) is a square modulo \( p \) if and only if \( p \equiv 1 \pmod{8} \) or \( p \equiv 3 \pmod{8} \). This completes the proof. \(\blacksquare\)","theorem ZMod.exists_sq_eq_neg_two_iff (hp : p ≠ 2) : IsSquare (-2 : ZMod p) ↔ p % 8 = 1 ∨ p % 8 = 3 := by
/- First, we use the equivalence that \(-2\) is a square in a finite field \( F \) if and only if the cardinality of \( F \) is not congruent to 5 or 7 modulo 8. Since the cardinality of \( \mathbb{Z}/p\mathbb{Z} \) is \( p \), the goal is to show that \( p \mod 8 \neq 5 \) and \( p \mod 8 \neq 7 \) if and only if \( p \mod 8 = 1 \) or \( p \mod 8 = 3 \). -/
  rw [FiniteField.isSquare_neg_two_iff, card p]
/- Since \( p \neq 2 \) (given by \( hp \)), we know that \( p \mod 2 = 1 \). This is because a prime number \( p \) is congruent to 1 modulo 2 if and only if \( p \neq 2 \). -/
  have h₁ := Prime.mod_two_eq_one_iff_ne_two.mpr hp
/- We use the property that \( p \mod 8 \mod 2 = p \mod 2 \) because 2 divides 8. Since \( p \mod 2 = 1 \), it follows that \( p \mod 8 \mod 2 = 1 \). -/
  rw [← mod_mod_of_dvd p (by decide : 2 ∣ 8)] at h₁
/- We know that \( p \mod 8 < 8 \) because the remainder of any number when divided by 8 is always less than 8. -/
  have h₂ := mod_lt p (by norm_num : 0 < 8)
/- We strengthen the proposition we are proving. We show that for every \( m \) such that \( m \mod 2 = 1 \) and \( m < 8 \), the statement \( m \neq 5 \) and \( m \neq 7 \) is equivalent to \( m = 1 \) or \( m = 3 \). -/
  revert h₂ h₁
/- We replace \( p \mod 8 \) with a new variable \( m \) and clear the variable \( p \) from the context. Now, we need to show that \( m \mod 2 = 1 \) and \( m < 8 \) implies \( m \neq 5 \) and \( m \neq 7 \) if and only if \( m = 1 \) or \( m = 3 \). -/
/- We introduce the hypotheses \( m \mod 2 = 1 \) and \( m < 8 \). We then consider all possible values of \( m \) in the interval from 0 to 7. By simplifying, we find that the only values of \( m \) that satisfy \( m \mod 2 = 1 \) and \( m < 8 \) are 1, 3, 5, and 7. Since \( m \neq 5 \) and \( m \neq 7 \), the only remaining values are 1 and 3. Therefore, \( m = 1 \) or \( m = 3 \). -/
  generalize p % 8 = m; clear! p
  intros; interval_cases m <;> simp_all","import Mathlib.NumberTheory.LegendreSymbol.Basic
import Mathlib.NumberTheory.LegendreSymbol.QuadraticChar.GaussSum
import Mathlib.NumberTheory.LegendreSymbol.QuadraticReciprocity

open ZMod
open Nat
variable {p : ℕ} [Fact p.Prime]
open ZMod
"
30444,omegaLimit_subset_of_tendsto,"theorem omegaLimit_subset_of_tendsto {m : τ → τ} {f₁ f₂ : Filter τ} (hf : Tendsto m f₁ f₂) :
    ω f₁ (fun t x ↦ ϕ (m t) x) s ⊆ ω f₂ ϕ s := by sorry","ω-limit Subset under Tending Function in Filters : For any types \( \tau \), \( \alpha \), and \( \beta \) where \( \beta \) is a topological space, and for any function \( \phi : \tau \to \alpha \to \beta \) and set \( s \subseteq \alpha \), if \( m : \tau \to \tau \) is a function such that \( m \) tends to \( f_2 \) along the filter \( f_1 \) (denoted as \( m \to f_2 \) as \( t \to f_1 \)), then the ω-limit of \( s \) under the function \( \phi \circ m \) with respect to the filter \( f_1 \) is a subset of the ω-limit of \( s \) under \( \phi \) with respect to the filter \( f_2 \). Formally, this is expressed as:
\[
\text{If } m \to f_2 \text{ as } t \to f_1, \text{ then } \omega\text{-limit}(f_1, \phi \circ m, s) \subseteq \omega\text{-limit}(f_2, \phi, s).
\]","theorem omegaLimit_subset_of_tendsto {m : τ → τ} {f₁ f₂ : Filter τ} (hf : Tendsto m f₁ f₂) :
    ω f₁ (fun t x ↦ ϕ (m t) x) s ⊆ ω f₂ ϕ s := by
  refine iInter₂_mono' fun u hu ↦ ⟨m ⁻¹' u, tendsto_def.mp hf _ hu, ?_⟩
  rw [← image2_image_left]
  exact closure_mono (image2_subset (image_preimage_subset _ _) Subset.rfl)","To prove the theorem, we need to show that the ω-limit of \( s \) under \( \phi \circ m \) with respect to the filter \( f_1 \) is a subset of the ω-limit of \( s \) under \( \phi \) with respect to the filter \( f_2 \).

1. **Monotonicity of Double Indexed Intersection:**
We use the monotonicity of the double indexed intersection of sets. For any set \( u \in f_2 \), we need to show that the closure of the image of \( s \) under \( \phi \circ m \) in \( m^{-1}(u) \) is a subset of the closure of the image of \( s \) under \( \phi \) in \( u \). This reduces to showing that \( m^{-1}(u) \in f_1 \) and that the closure of the image of \( s \) under \( \phi \circ m \) in \( m^{-1}(u) \) is a subset of the closure of the image of \( s \) under \( \phi \) in \( u \).

2. **Rewriting the Goal:**
We use the property of the binary image of sets under function composition to rewrite the goal. Specifically, the binary image of \( s \) and \( m^{-1}(u) \) under \( \phi \circ m \) is equal to the binary image of \( s \) and \( m^{-1}(u) \) under the function \( (t, x) \mapsto \phi(m(t), x) \). This simplifies our goal to showing that the closure of the image of \( s \) under \( \phi \) in \( m(m^{-1}(u)) \) is a subset of the closure of the image of \( s \) under \( \phi \) in \( u \).

3. **Monotonicity of Closure and Subset Relation:**
We use the monotonicity of the closure and the subset relation to complete the proof. Specifically, since \( m(m^{-1}(u)) \subseteq u \) (by the property of the image of the preimage), the image of \( s \) under \( \phi \) in \( m(m^{-1}(u)) \) is a subset of the image of \( s \) under \( \phi \) in \( u \). Therefore, the closure of the image of \( s \) under \( \phi \) in \( m(m^{-1}(u)) \) is a subset of the closure of the image of \( s \) under \( \phi \) in \( u \).

Thus, we have shown that the ω-limit of \( s \) under \( \phi \circ m \) with respect to the filter \( f_1 \) is a subset of the ω-limit of \( s \) under \( \phi \) with respect to the filter \( f_2 \). This completes the proof. \(\blacksquare\)","theorem omegaLimit_subset_of_tendsto {m : τ → τ} {f₁ f₂ : Filter τ} (hf : Tendsto m f₁ f₂) :
    ω f₁ (fun t x ↦ ϕ (m t) x) s ⊆ ω f₂ ϕ s := by
/- To prove that the ω-limit of \( s \) under \( \phi \circ m \) with respect to the filter \( f_1 \) is a subset of the ω-limit of \( s \) under \( \phi \) with respect to the filter \( f_2 \), we use the monotonicity of the double indexed intersection of sets. Specifically, for any set \( u \in f_2 \), we need to show that the closure of the image of \( s \) under \( \phi \circ m \) in \( m^{-1}(u) \) is a subset of the closure of the image of \( s \) under \( \phi \) in \( u \). This reduces to showing that \( m^{-1}(u) \in f_1 \) and that the closure of the image of \( s \) under \( \phi \circ m \) in \( m^{-1}(u) \) is a subset of the closure of the image of \( s \) under \( \phi \) in \( u \). -/
  refine iInter₂_mono' fun u hu ↦ ⟨m ⁻¹' u, tendsto_def.mp hf _ hu, ?_⟩
/- We use the property of the binary image of sets under function composition to rewrite the goal. Specifically, the binary image of \( s \) and \( m^{-1}(u) \) under \( \phi \circ m \) is equal to the binary image of \( s \) and \( m^{-1}(u) \) under the function \( (t, x) \mapsto \phi(m(t), x) \). This simplifies our goal to showing that the closure of the image of \( s \) under \( \phi \) in \( m(m^{-1}(u)) \) is a subset of the closure of the image of \( s \) under \( \phi \) in \( u \). -/
  rw [← image2_image_left]
/- We use the monotonicity of the closure and the subset relation to complete the proof. Specifically, since \( m(m^{-1}(u)) \subseteq u \) (by the property of the image of the preimage), the image of \( s \) under \( \phi \) in \( m(m^{-1}(u)) \) is a subset of the image of \( s \) under \( \phi \) in \( u \). Therefore, the closure of the image of \( s \) under \( \phi \) in \( m(m^{-1}(u)) \) is a subset of the closure of the image of \( s \) under \( \phi \) in \( u \). This completes the proof. -/
  exact closure_mono (image2_subset (image_preimage_subset _ _) Subset.rfl)","import Mathlib.Dynamics.Flow
import Mathlib.Tactic.Monotonicity
import Mathlib.Dynamics.OmegaLimit


open Set Function Filter Topology
variable {τ : Type*} {α : Type*} {β : Type*} {ι : Type*}
variable [TopologicalSpace β]
variable (f : Filter τ) (ϕ : τ → α → β) (s s₁ s₂ : Set α)
variable (f : Filter τ) (ϕ : τ → α → β) (s s₁ s₂ : Set α)
open omegaLimit
"
30710,IsPiSystem.singleton,theorem IsPiSystem.singleton {α} (S : Set α) : IsPiSystem ({S} : Set (Set α)) := by sorry,"Singleton Set is a π-system : For any set \( S \) of a type \( \alpha \), the collection \(\{S\}\) is a π-system. This means that the collection \(\{S\}\) is closed under binary intersection of non-disjoint sets. Specifically, if \( S \cap S \) is nonempty, then \( S \cap S \) is also in \(\{S\}\).","theorem IsPiSystem.singleton {α} (S : Set α) : IsPiSystem ({S} : Set (Set α)) := by
  intro s h_s t h_t _
  rw [Set.mem_singleton_iff.1 h_s, Set.mem_singleton_iff.1 h_t, Set.inter_self,
    Set.mem_singleton_iff]","To prove that \(\{S\}\) is a π-system, we need to show that for any sets \( s \) and \( t \) in \(\{S\}\), if \( s \cap t \) is nonempty, then \( s \cap t \in \{S\} \).

1. Let \( s \) and \( t \) be arbitrary sets in \( \alpha \), and assume \( s \in \{S\} \) and \( t \in \{S\} \).
2. By the definition of membership in a singleton set, \( s = S \) and \( t = S \).
3. Therefore, \( s \cap t = S \cap S \).
4. By the property of intersection, \( S \cap S = S \).
5. Hence, \( s \cap t = S \).
6. Since \( S \in \{S\} \), it follows that \( s \cap t \in \{S\} \).

This completes the proof that \(\{S\}\) is a π-system.","theorem IsPiSystem.singleton {α} (S : Set α) : IsPiSystem ({S} : Set (Set α)) := by
/- Let \( s \) and \( t \) be arbitrary sets in \( \alpha \), and assume \( s \in \{S\} \) and \( t \in \{S\} \). We need to show that if \( s \cap t \) is nonempty, then \( s \cap t \in \{S\} \). -/
  intro s h_s t h_t _
/- Since \( s \in \{S\} \) and \( t \in \{S\} \), by the definition of membership in a singleton set, we have \( s = S \) and \( t = S \). Therefore, \( s \cap t = S \cap S \). By the property of intersection, \( S \cap S = S \). Hence, \( s \cap t = S \). Since \( S \in \{S\} \), it follows that \( s \cap t \in \{S\} \). -/
  rw [Set.mem_singleton_iff.1 h_s, Set.mem_singleton_iff.1 h_t, Set.inter_self,
    Set.mem_singleton_iff]","import Mathlib.Logic.Encodable.Lattice
import Mathlib.MeasureTheory.MeasurableSpace.Defs
import Mathlib.MeasureTheory.PiSystem

open IsPiSystem
open MeasurableSpace Set
open MeasureTheory
"
33860,Invertible.congr,"theorem Invertible.congr [Monoid α] (a b : α) [Invertible a] [Invertible b] (h : a = b) :
    ⅟a = ⅟b := by sorry","Invertible Elements are Equal if Their Inverses are Equal: \( a = b \to ⅟a = ⅟b \) : For any type \( \alpha \) that is a monoid, if \( a \) and \( b \) are elements of \( \alpha \) and both are invertible, then \( a = b \) implies \( ⅟a = ⅟b \).","theorem Invertible.congr [Monoid α] (a b : α) [Invertible a] [Invertible b] (h : a = b) :
    ⅟a = ⅟b := by
  subst h; congr; apply Subsingleton.allEq","Given that \( a \) and \( b \) are elements of a monoid \( \alpha \) and both are invertible, we need to show that if \( a = b \), then \( ⅟a = ⅟b \).

1. **Substitution:**
Since we have the hypothesis that \( a = b \), we can substitute \( b \) with \( a \) in the goal. This simplifies the goal to \( ⅟a = ⅟a \).

2. **Conclusion:**
The equation \( ⅟a = ⅟a \) is trivially true because any element is equal to itself.

Therefore, we have shown that if \( a = b \), then \( ⅟a = ⅟b \). This completes the proof. \(\blacksquare\)","theorem Invertible.congr [Monoid α] (a b : α) [Invertible a] [Invertible b] (h : a = b) :
    ⅟a = ⅟b := by
  subst h; congr; apply Subsingleton.allEq","import Mathlib.Algebra.Group.Defs
import Mathlib.Algebra.Group.Invertible.Defs

open Invertible
variable {α : Type*} [Monoid α]
variable {α : Type u}
"
34209,Char.lt_irrefl,theorem Char.lt_irrefl (a : Char) : ¬ a < a := by sorry,"Irreflexivity of Less-Than Relation on Characters: \( \neg (a < a) \) : For any character \( a \) in the Unicode character type `Char`, it is not the case that \( a < a \). In other words, the less-than relation on characters is irreflexive, i.e., \( \neg (a < a) \).","theorem Char.lt_irrefl (a : Char) : ¬ a < a := by
  simp","To prove that for any character \( a \), \( \neg (a < a) \), we use the properties of the less-than relation on characters. The less-than relation on characters is defined such that \( a < b \) if and only if the integer value of \( a \) is less than the integer value of \( b \).

By the definition of the less-than relation, \( a < a \) would imply that the integer value of \( a \) is less than itself, which is a contradiction. Therefore, \( a < a \) is false. This means that \( \neg (a < a) \) holds for any character \( a \). Hence, the less-than relation on characters is irreflexive. This completes the proof.","theorem Char.lt_irrefl (a : Char) : ¬ a < a := by
  simp","import Init.Data.Char.Basic
import Init.Data.UInt.Lemmas
import Init.Data.Char.Lemmas

open Char

"
34440,SheafOfModules.sectionMap_freeMap_freeSection,"theorem SheafOfModules.sectionMap_freeMap_freeSection (i : I) :
    sectionsMap (freeMap (R := R) f) (freeSection i) = freeSection (f i) := by sorry","Induced Map on Sections of Free Sheaves of Modules Preserves Tautological Sections : For any types \( I \) and \( J \), and a function \( f : I \to J \), the induced map on sections of the free sheaves of modules \( \text{SheafOfModules.sectionsMap}(\text{SheafOfModules.freeMap } f) \) maps the tautological section of \( \text{free } I \) corresponding to an element \( i \in I \) to the tautological section of \( \text{free } J \) corresponding to the element \( f(i) \in J \). Formally, this is expressed as:
\[
\text{SheafOfModules.sectionsMap}(\text{SheafOfModules.freeMap } f)(\text{SheafOfModules.freeSection } i) = \text{SheafOfModules.freeSection } (f(i))
\]","theorem SheafOfModules.sectionMap_freeMap_freeSection (i : I) :
    sectionsMap (freeMap (R := R) f) (freeSection i) = freeSection (f i) := by
  simp [← freeHomEquiv_comp_apply]","To prove the theorem, we use the equivalence that the composition of the free homomorphism and the application of the free section is the same as the free section of the image. Specifically, we have:
\[
\text{SheafOfModules.sectionsMap}(\text{SheafOfModules.freeMap } f)(\text{SheafOfModules.freeSection } i) = \text{SheafOfModules.freeSection } (f(i))
\]
This equivalence allows us to simplify the proposition we want to show to the desired equality. Therefore, the induced map on sections of the free sheaves of modules \( \text{SheafOfModules.sectionsMap}(\text{SheafOfModules.freeMap } f) \) maps the tautological section of \( \text{free } I \) corresponding to an element \( i \in I \) to the tautological section of \( \text{free } J \) corresponding to the element \( f(i) \in J \). This completes the proof.","theorem SheafOfModules.sectionMap_freeMap_freeSection (i : I) :
    sectionsMap (freeMap (R := R) f) (freeSection i) = freeSection (f i) := by
/- Using the equivalence that the composition of the free homomorphism and the application of the free section is the same as the free section of the image, we can simplify the proposition we want to show to the desired equality. Specifically, the induced map on sections of the free sheaves of modules \( \text{SheafOfModules.sectionsMap}(\text{SheafOfModules.freeMap } f) \) maps the tautological section of \( \text{free } I \) corresponding to an element \( i \in I \) to the tautological section of \( \text{free } J \) corresponding to the element \( f(i) \in J \). -/
  simp [← freeHomEquiv_comp_apply]","import Mathlib.Algebra.Category.ModuleCat.Presheaf.Colimits
import Mathlib.Algebra.Category.ModuleCat.Sheaf.Colimits
import Mathlib.Algebra.Category.ModuleCat.Sheaf.Free

open SheafOfModules
open CategoryTheory Limits
variable {C : Type u'} [Category.{v'} C] {J : GrothendieckTopology C} {R : Sheaf J RingCat.{u}}
  [HasWeakSheafify J AddCommGrp.{u}] [J.WEqualsLocallyBijective AddCommGrp.{u}]
  [J.HasSheafCompose (forget₂ RingCat.{u} AddCommGrp.{u})]
variable {I J : Type u} (f : I → J)
"
36165,Projectivization.independent_iff,theorem Projectivization.independent_iff : Independent f ↔ LinearIndependent K (Projectivization.rep ∘ f) := by sorry,"Independence of Points in Projective Space is Equivalent to Linear Independence of Representatives : A family of points \( f : \iota \to \mathbb{P}(K, V) \) in a projective space \(\mathbb{P}(K, V)\) over a division ring \(K\) and a vector space \(V\) is independent if and only if the corresponding family of nonzero vectors \( \{ \text{Projectivization.rep}(f(i)) \}_{i \in \iota} \) in the ambient vector space \(V\) is linearly independent.","theorem Projectivization.independent_iff : Independent f ↔ LinearIndependent K (Projectivization.rep ∘ f) := by
  refine ⟨?_, fun h => ?_⟩
  · rintro ⟨ff, hff, hh⟩
    choose a ha using fun i : ι => exists_smul_eq_mk_rep K (ff i) (hff i)
    convert hh.units_smul a
    ext i
    exact (ha i).symm
  · convert Independent.mk _ _ h
    · simp only [mk_rep, Function.comp_apply]
    · intro i
      apply rep_nonzero","To prove the equivalence, we need to show two implications:

1. **If the family of points \( f \) is independent, then the corresponding family of vectors \( \text{Projectivization.rep} \circ f \) is linearly independent.**

Assume that the family of points \( f \) is independent. By the definition of independence in projective space, there exists a family of nonzero vectors \( ff : \iota \to V \) such that \( f(i) = \text{Projectivization.mk}(K, ff(i)) \) for all \( i \in \iota \), and the family \( ff \) is linearly independent over \( K \).

For each \( i \in \iota \), there exists a nonzero scalar \( a(i) \in K \) such that \( a(i) \cdot ff(i) = \text{Projectivization.rep}(f(i)) \). We choose such a family of scalars \( a : \iota \to K^\times \) and the corresponding equalities \( ha : \forall i \in \iota, a(i) \cdot ff(i) = \text{Projectivization.rep}(f(i)) \).

Since the family \( ff \) is linearly independent and the family \( a \) consists of units in \( K \), the family \( a \cdot ff \) is also linearly independent. Therefore, the family \( \text{Projectivization.rep} \circ f \) is linearly independent.

2. **If the family of vectors \( \text{Projectivization.rep} \circ f \) is linearly independent, then the family of points \( f \) is independent.**

Assume that the family of vectors \( \text{Projectivization.rep} \circ f \) is linearly independent. We need to show that the family of points \( f \) is independent. By the definition of independence in projective space, we need to show that for each \( i \in \iota \), \( f(i) = \text{Projectivization.mk}(K, \text{Projectivization.rep}(f(i))) \) and that \( \text{Projectivization.rep}(f(i)) \neq 0 \) for all \( i \in \iota \).

Using the properties of the projective point construction and the representative function, we have \( f(i) = \text{Projectivization.mk}(K, \text{Projectivization.rep}(f(i))) \) for all \( i \in \iota \).

To show that \( \text{Projectivization.rep}(f(i)) \neq 0 \) for all \( i \in \iota \), we consider an arbitrary \( i \in \iota \). By the property of the representative function, \( \text{Projectivization.rep}(f(i)) \neq 0 \) for all \( i \in \iota \).

Thus, we have shown both implications, completing the proof. \(\blacksquare\)","theorem Projectivization.independent_iff : Independent f ↔ LinearIndependent K (Projectivization.rep ∘ f) := by
/- To prove the equivalence, we need to show two implications: (1) if the family of points \( f \) is independent, then the corresponding family of vectors \( \text{Projectivization.rep} \circ f \) is linearly independent, and (2) if the family of vectors \( \text{Projectivization.rep} \circ f \) is linearly independent, then the family of points \( f \) is independent. We will prove these implications separately. -/
  refine ⟨?_, fun h => ?_⟩
/- First, assume that the family of points \( f \) is independent. By the definition of independence in projective space, there exists a family of nonzero vectors \( ff : \iota \to V \) such that \( f(i) = \text{Projectivization.mk}(K, ff(i)) \) for all \( i \in \iota \), and the family \( ff \) is linearly independent over \( K \). We need to show that the family of vectors \( \text{Projectivization.rep} \circ f \) is linearly independent. -/
  · rintro ⟨ff, hff, hh⟩
/- For each \( i \in \iota \), there exists a nonzero scalar \( a(i) \in K \) such that \( a(i) \cdot ff(i) = \text{Projectivization.rep}(f(i)) \). We choose such a family of scalars \( a : \iota \to K^\times \) and the corresponding equalities \( ha : \forall i \in \iota, a(i) \cdot ff(i) = \text{Projectivization.rep}(f(i)) \). -/
    choose a ha using fun i : ι => exists_smul_eq_mk_rep K (ff i) (hff i)
/- Since the family \( ff \) is linearly independent and the family \( a \) consists of units in \( K \), the family \( a \cdot ff \) is also linearly independent. Therefore, the family \( \text{Projectivization.rep} \circ f \) is linearly independent. -/
    convert hh.units_smul a
/- To show that \( \text{Projectivization.rep} \circ f = a \cdot ff \), we need to show that for every \( i \in \iota \), \( \text{Projectivization.rep}(f(i)) = a(i) \cdot ff(i) \). -/
    ext i
/- By the choice of \( a \) and \( ha \), we have \( a(i) \cdot ff(i) = \text{Projectivization.rep}(f(i)) \). Therefore, \( \text{Projectivization.rep}(f(i)) = a(i) \cdot ff(i) \) holds for every \( i \in \iota \). -/
    exact (ha i).symm
/- Now, assume that the family of vectors \( \text{Projectivization.rep} \circ f \) is linearly independent. We need to show that the family of points \( f \) is independent. By the definition of independence in projective space, we need to show that for each \( i \in \iota \), \( f(i) = \text{Projectivization.mk}(K, \text{Projectivization.rep}(f(i))) \) and that \( \text{Projectivization.rep}(f(i)) \neq 0 \) for all \( i \in \iota \). -/
  · convert Independent.mk _ _ h
/- Using the properties of the projective point construction and the representative function, we have \( f(i) = \text{Projectivization.mk}(K, \text{Projectivization.rep}(f(i))) \) for all \( i \in \iota \). -/
    · simp only [mk_rep, Function.comp_apply]
/- To show that \( \text{Projectivization.rep}(f(i)) \neq 0 \) for all \( i \in \iota \), we consider an arbitrary \( i \in \iota \). -/
    · intro i
/- By the property of the representative function, \( \text{Projectivization.rep}(f(i)) \neq 0 \) for all \( i \in \iota \). -/
      apply rep_nonzero","import Mathlib.LinearAlgebra.Projectivization.Basic
import Mathlib.LinearAlgebra.Projectivization.Independence

open Projectivization
open scoped LinearAlgebra.Projectivization
variable {ι K V : Type*} [DivisionRing K] [AddCommGroup V] [Module K V] {f : ι → ℙ K V}
"
36432,Quiver.Push.lift_unique,"theorem Quiver.Push.lift_unique (Φ : Push σ ⥤q W') (Φ₀ : Φ.obj = τ) (Φcomp : (of σ ⋙q Φ) = φ) :
    Φ = lift σ φ τ h := by sorry","Uniqueness of Prefunctor Lift in Pushed Quivers: \( \Phi = \text{Quiver.Push.lift}(\sigma, \phi, \tau, h) \) : For any types \( V \) and \( W \) with quiver structures, and a map \( \sigma : V \to W \), if \( \phi : V \rightarrow_q W' \) is a prefunctor and \( \tau : W \to W' \) is a map such that for every vertex \( x \in V \), \( \phi(x) = \tau(\sigma(x)) \), and if \( \Phi : \text{Push}(\sigma) \rightarrow_q W' \) is another prefunctor, then \( \Phi \) is uniquely determined by \( \tau \) and \( \phi \). Specifically, if \( \Phi \) maps each vertex \( \sigma(x) \) to \( \tau(\sigma(x)) \) and each arrow \( \sigma(v) \to \sigma(v') \) to the same arrow as \( \phi \) maps \( v \to v' \), then \( \Phi \) is equal to the prefunctor \( \text{Quiver.Push.lift}(\sigma, \phi, \tau, h) \).","theorem Quiver.Push.lift_unique (Φ : Push σ ⥤q W') (Φ₀ : Φ.obj = τ) (Φcomp : (of σ ⋙q Φ) = φ) :
    Φ = lift σ φ τ h := by
  dsimp only [of, lift]
  fapply Prefunctor.ext
  · intro X
    simp only
    rw [Φ₀]
  · rintro _ _ ⟨⟩
    subst_vars
    simp only [Prefunctor.comp_map, cast_eq]
    rfl","We start by simplifying the goal using the definitions of `of` and `lift`. This reduces the goal to showing that the object mapping of \( \Phi \) is equal to \( \tau \) and the arrow mapping of \( \Phi \) is defined by the recursion principle on the pushed quiver.

To show that \( \Phi \) is equal to \( \text{Quiver.Push.lift}(\sigma, \phi, \tau, h) \), it suffices to show that their object mappings and arrow mappings are equal. We will prove these two conditions separately.

1. **Object Mapping:**
- Let \( X \) be an arbitrary vertex in the pushed quiver \( \text{Push}(\sigma) \).
- We need to show that \( \Phi.\text{obj}(X) = \tau(X) \).
- Simplify the proposition we want to show. This simplifies the goal to \( \Phi.\text{obj}(X) = \tau(X) \).
- Since \( \Phi.\text{obj} = \tau \) by assumption, we can replace \( \Phi.\text{obj}(X) \) with \( \tau(X) \). This completes the proof for the object mapping.

2. **Arrow Mapping:**
- Let \( X \) and \( Y \) be arbitrary vertices in \( V \), and let \( f : X \to Y \) be an arrow in \( V \).
- We need to show that \( \Phi.\text{map}(\text{PushQuiver.arrow}(f)) = \text{id}(\text{Eq.recOn}(\text{Eq.recOn}(\text{Quiver.Push.lift}(\sigma, \phi, \tau, h).\text{map}(\text{PushQuiver.arrow}(f)))) \).
- Substitute the variables to simplify the goal. This simplifies the goal to showing that \( \Phi.\text{map}(\text{PushQuiver.arrow}(f)) = \text{id}(\text{Eq.recOn}(\text{Eq.recOn}(\Phi.\text{map}((\text{Quiver.Push.of}(\sigma)).\text{map}(f)))) \).
- Simplify the proposition we want to show using the properties of prefunctor composition and type cast identity. This simplifies the goal to \( \Phi.\text{map}(\text{PushQuiver.arrow}(f)) = \text{id}(\Phi.\text{map}((\text{Quiver.Push.of}(\sigma)).\text{map}(f))) \).
- The current goal is trivially true due to the reflexive property. This completes the proof for the arrow mapping.

Thus, we have shown that \( \Phi \) is equal to \( \text{Quiver.Push.lift}(\sigma, \phi, \tau, h) \). This completes the proof.","theorem Quiver.Push.lift_unique (Φ : Push σ ⥤q W') (Φ₀ : Φ.obj = τ) (Φcomp : (of σ ⋙q Φ) = φ) :
    Φ = lift σ φ τ h := by
/- First, we simplify the current goal by expanding the definitions of `of` and `lift`. This simplifies the goal to showing that the object mapping of \( \Phi \) is equal to \( \tau \) and the arrow mapping of \( \Phi \) is defined by the recursion principle on the pushed quiver. -/
  dsimp only [of, lift]
/- To show that \( \Phi \) is equal to \( \text{Quiver.Push.lift}(\sigma, \phi, \tau, h) \), it suffices to show that their object mappings and arrow mappings are equal. We will prove these two conditions separately. -/
  fapply Prefunctor.ext
/- Let \( X \) be an arbitrary vertex in the pushed quiver \( \text{Push}(\sigma) \). We need to show that \( \Phi.\text{obj}(X) = \tau(X) \). -/
  · intro X
/- Simplify the proposition we want to show. This simplifies the goal to \( \Phi.\text{obj}(X) = \tau(X) \). -/
    simp only
/- Since \( \Phi.\text{obj} = \tau \) by assumption, we can replace \( \Phi.\text{obj}(X) \) with \( \tau(X) \). This completes the proof for the object mapping. -/
    rw [Φ₀]
/- Let \( X \) and \( Y \) be arbitrary vertices in \( V \), and let \( f : X \to Y \) be an arrow in \( V \). We need to show that \( \Phi.\text{map}(\text{PushQuiver.arrow}(f)) = \text{id}(\text{Eq.recOn}(\text{Eq.recOn}(\text{Quiver.Push.lift}(\sigma, \phi, \tau, h).\text{map}(\text{PushQuiver.arrow}(f)))) \). -/
  · rintro _ _ ⟨⟩
/- Substitute the variables to simplify the goal. This simplifies the goal to showing that \( \Phi.\text{map}(\text{PushQuiver.arrow}(f)) = \text{id}(\text{Eq.recOn}(\text{Eq.recOn}(\Phi.\text{map}((\text{Quiver.Push.of}(\sigma)).\text{map}(f)))) \). -/
    subst_vars
/- Simplify the proposition we want to show using the properties of prefunctor composition and type cast identity. This simplifies the goal to \( \Phi.\text{map}(\text{PushQuiver.arrow}(f)) = \text{id}(\Phi.\text{map}((\text{Quiver.Push.of}(\sigma)).\text{map}(f))) \). -/
    simp only [Prefunctor.comp_map, cast_eq]
/- The current goal is trivially true due to the reflexive property. This completes the proof for the arrow mapping. -/
    rfl","import Mathlib.Combinatorics.Quiver.Basic
import Mathlib.Combinatorics.Quiver.Push

open Quiver
open Push
variable {V : Type*} [Quiver V] {W : Type*} (σ : V → W)
variable {W' : Type*} [Quiver W'] (φ : V ⥤q W') (τ : W → W') (h : ∀ x, φ.obj x = τ (σ x))
"
36660,Bitraversable.tfst_eq_fst_id,"theorem Bitraversable.tfst_eq_fst_id {α α' β} (f : α → α') (x : t α β) :
    tfst (F := Id) (pure ∘ f) x = pure (fst f x) := by sorry","Traverse on First Argument of Bitraversable Type with Pure Embedding : For any bitraversable type \( t \) and any lawful bitraversable instance, and for any types \( \alpha \), \( \alpha' \), and \( \beta \), if \( f : \alpha \to \alpha' \) is a function, then for any element \( x \) of type \( t \, \alpha \, \beta \), the following equality holds:
\[
\text{tfst}(\text{pure} \circ f)(x) = \text{pure}(\text{fst} \, f \, x)
\]
Here, \(\text{tfst}\) is the function that traverses the first functor argument of a bitraversable type, and \(\text{pure}\) is the function that embeds a value into a monadic context. The function \(\text{fst}\) applies the function \( f \) to the first type argument of a bifunctor while keeping the second type argument fixed.","theorem Bitraversable.tfst_eq_fst_id {α α' β} (f : α → α') (x : t α β) :
    tfst (F := Id) (pure ∘ f) x = pure (fst f x) := by
  apply bitraverse_eq_bimap_id","To prove the equality \(\text{tfst}(\text{pure} \circ f)(x) = \text{pure}(\text{fst} \, f \, x)\), we use the theorem that for any bitraversable and lawful bitraversable type \( t \), and for any functions \( f : \alpha \to \alpha' \) and \( f' : \alpha' \to \beta' \), the `bitraverse` function applied to the pure embeddings of \( f \) and \( f' \) on an element \( x \) of type \( t \alpha \alpha' \) is equal to the pure embedding of the result of applying the `bimap` function to \( f \) and \( f' \) on \( x \). Specifically, we have:
\[
\text{bitraverse} (\text{pure} \circ f) (\text{pure} \circ \text{id}) x = \text{pure} (\text{bimap} f \, \text{id} \, x).
\]
Since \(\text{tfst}(\text{pure} \circ f)(x)\) is defined as \(\text{bitraverse} (\text{pure} \circ f) (\text{pure} \circ \text{id}) x\) and \(\text{bimap} f \, \text{id} \, x\) is equivalent to \(\text{fst} f x\), the theorem directly implies the desired equality:
\[
\text{tfst}(\text{pure} \circ f)(x) = \text{pure}(\text{fst} \, f \, x).
\]
This completes the proof.","theorem Bitraversable.tfst_eq_fst_id {α α' β} (f : α → α') (x : t α β) :
    tfst (F := Id) (pure ∘ f) x = pure (fst f x) := by
/- To prove the equality \(\text{tfst}(\text{pure} \circ f)(x) = \text{pure}(\text{fst} \, f \, x)\), we use the theorem that for any bitraversable and lawful bitraversable type \( t \), and for any functions \( f : \alpha \to \alpha' \) and \( f' : \alpha' \to \beta' \), the `bitraverse` function applied to the pure embeddings of \( f \) and \( f' \) on an element \( x \) of type \( t \alpha \alpha' \) is equal to the pure embedding of the result of applying the `bimap` function to \( f \) and \( f' \) on \( x \). Specifically, we have:
\[
\text{bitraverse} (\text{pure} \circ f) (\text{pure} \circ \text{id}) x = \text{pure} (\text{bimap} f \, \text{id} \, x).
\]
Since \(\text{tfst}(\text{pure} \circ f)(x)\) is defined as \(\text{bitraverse} (\text{pure} \circ f) (\text{pure} \circ \text{id}) x\) and \(\text{bimap} f \, \text{id} \, x\) is equivalent to \(\text{fst} f x\), the theorem directly implies the desired equality:
\[
\text{tfst}(\text{pure} \circ f)(x) = \text{pure}(\text{fst} \, f \, x).
\] -/
  apply bitraverse_eq_bimap_id","import Mathlib.Control.Bitraversable.Basic
import Mathlib.Control.Bitraversable.Lemmas

open Bitraversable
variable {t : Type u → Type u → Type u} [Bitraversable t]
variable {β : Type u}
variable {β : Type u}
open Functor LawfulApplicative
variable {F G : Type u → Type u} [Applicative F] [Applicative G]
variable [LawfulBitraversable t] [LawfulApplicative F] [LawfulApplicative G]
open Bifunctor
open Function
"
36882,List.nextOr_cons_of_ne,"theorem List.nextOr_cons_of_ne (xs : List α) (y x d : α) (h : x ≠ y) :
    nextOr (y :: xs) x d = nextOr xs x d := by sorry","Next Element or Default is Independent of First Element if First Element is Not Equal to Given Element : For any list \( xs \) of elements of type \( \alpha \) and any elements \( x, y, d \in \alpha \), if \( x \neq y \), then the next element of \( x \) in the list \( y :: xs \) or the default value \( d \) is the same as the next element of \( x \) in the list \( xs \) or the default value \( d \). In other words, if \( x \neq y \), then \( \text{nextOr}(y :: xs, x, d) = \text{nextOr}(xs, x, d) \).","theorem List.nextOr_cons_of_ne (xs : List α) (y x d : α) (h : x ≠ y) :
    nextOr (y :: xs) x d = nextOr xs x d := by
  cases' xs with z zs
  · rfl
  · exact if_neg h","We will consider two cases for the list \( xs \):

1. **Case 1: \( xs \) is the empty list \( [] \).**
- We need to show that \([y].\text{nextOr}(x, d) = [].\text{nextOr}(x, d)\).
- This is trivially true because both sides are definitionally equal.

2. **Case 2: \( xs \) is a non-empty list of the form \( z :: zs \).**
- We need to show that \((y :: z :: zs).\text{nextOr}(x, d) = (z :: zs).\text{nextOr}(x, d)\).
- Since \( x \neq y \), the if-then-else function \(\text{ite}(x = y, \text{nextOr}(z :: zs, x, d), \text{nextOr}(z :: zs, x, d))\) returns \(\text{nextOr}(z :: zs, x, d)\) because the condition \( x = y \) is false.
- Therefore, the equality holds.

Thus, in both cases, the theorem is proved. This completes the proof.","theorem List.nextOr_cons_of_ne (xs : List α) (y x d : α) (h : x ≠ y) :
    nextOr (y :: xs) x d = nextOr xs x d := by
/- We will consider two cases for the list \( xs \):
1. \( xs \) is the empty list \( [] \).
2. \( xs \) is a non-empty list of the form \( z :: zs \). -/
  cases' xs with z zs
/- In the first case, where \( xs \) is the empty list \( [] \), we need to show that \([y].\text{nextOr}(x, d) = [].\text{nextOr}(x, d)\). This is trivially true because both sides are definitionally equal. -/
  · rfl
/- In the second case, where \( xs \) is a non-empty list of the form \( z :: zs \), we need to show that \((y :: z :: zs).\text{nextOr}(x, d) = (z :: zs).\text{nextOr}(x, d)\). Since \( x \neq y \), the if-then-else function \(\text{ite}(x = y, \text{nextOr}(z :: zs, x, d), \text{nextOr}(z :: zs, x, d))\) returns \(\text{nextOr}(z :: zs, x, d)\) because the condition \( x = y \) is false. Therefore, the equality holds. -/
  · exact if_neg h","import Mathlib.Data.Fintype.List
import Mathlib.Data.List.Cycle

open List
variable {α : Type*} [DecidableEq α]
"
36921,List.prev_getLast_cons',"theorem List.prev_getLast_cons' (y : α) (hxy : x ∈ y :: l) (hx : x = y) :
    prev (y :: l) x hxy = getLast (y :: l) (cons_ne_nil _ _) := by sorry","Previous Element in List Cycle is the Last Element if First Element is Given and Equal : For any list \( l \) of elements of type \( \alpha \) and any elements \( x, y \in \alpha \), if \( x \) is in the list \( y :: l \) and \( x = y \), then the previous element of \( x \) in the list \( y :: l \) is the last element of the list \( y :: l \).","theorem List.prev_getLast_cons' (y : α) (hxy : x ∈ y :: l) (hx : x = y) :
    prev (y :: l) x hxy = getLast (y :: l) (cons_ne_nil _ _) := by
  cases l <;> simp [prev, hx]","We need to show that for any list \( l \) of elements of type \( \alpha \) and any elements \( x, y \in \alpha \), if \( x \) is in the list \( y :: l \) and \( x = y \), then the previous element of \( x \) in the list \( y :: l \) is the last element of the list \( y :: l \).

We will consider every possible case of the list \( l \).

**Case 1: \( l \) is the empty list \( [] \)**
- In this case, the list \( y :: l \) is \( [y] \).
- Since \( x = y \) and \( x \in [y] \), the previous element of \( x \) in the list \( [y] \) is \( y \).
- The last element of the list \( [y] \) is also \( y \).
- Therefore, \( [y].\text{prev}(x, hxy) = y \) and \( [y].\text{getLast} = y \).
- Hence, \( [y].\text{prev}(x, hxy) = [y].\text{getLast} \).

**Case 2: \( l \) is a non-empty list \( y :: \text{head} :: \text{tail} \)**
- In this case, the list \( y :: l \) is \( y :: \text{head} :: \text{tail} \).
- Since \( x = y \) and \( x \in y :: \text{head} :: \text{tail} \), the previous element of \( x \) in the list \( y :: \text{head} :: \text{tail} \) is the last element of the list \( \text{head} :: \text{tail} \).
- The last element of the list \( y :: \text{head} :: \text{tail} \) is also the last element of the list \( \text{head} :: \text{tail} \).
- Therefore, \( (y :: \text{head} :: \text{tail}).\text{prev}(x, hxy) = \text{last}(\text{head} :: \text{tail}) \) and \( (y :: \text{head} :: \text{tail}).\text{getLast} = \text{last}(\text{head} :: \text{tail}) \).
- Hence, \( (y :: \text{head} :: \text{tail}).\text{prev}(x, hxy) = (y :: \text{head} :: \text{tail}).\text{getLast} \).

In both cases, the proposition holds. Therefore, the theorem is proved.","theorem List.prev_getLast_cons' (y : α) (hxy : x ∈ y :: l) (hx : x = y) :
    prev (y :: l) x hxy = getLast (y :: l) (cons_ne_nil _ _) := by
  cases l <;> simp [prev, hx]","import Mathlib.Data.Fintype.List
import Mathlib.Data.List.Cycle

open List
variable {α : Type*} [DecidableEq α]
variable (l : List α) (x : α)
"
36923,List.next_ne_head_ne_getLast,"theorem List.next_ne_head_ne_getLast (h : x ∈ l) (y : α) (h : x ∈ y :: l) (hy : x ≠ y)
    (hx : x ≠ getLast (y :: l) (cons_ne_nil _ _)) :
    next (y :: l) x h = next l x (by simpa [hy] using h) := by sorry","Next Element in List Cycle: \((y :: l). \text{next}(x) = l. \text{next}(x)\) if \( x \neq y \) and \( x \neq (y :: l). \text{getLast} \) : For any list \( l \) of elements of type \( \alpha \) and any element \( x \in l \), if \( x \) is not equal to the head element \( y \) of the list \( y :: l \) and \( x \) is not equal to the last element of the list \( y :: l \), then the next element in the list \( y :: l \) after \( x \) is the same as the next element in the list \( l \) after \( x \). Formally, if \( x \in y :: l \) and \( x \neq y \) and \( x \neq (y :: l). \text{getLast} \), then \((y :: l). \text{next}(x, h) = l. \text{next}(x, \cdots)\).","theorem List.next_ne_head_ne_getLast (h : x ∈ l) (y : α) (h : x ∈ y :: l) (hy : x ≠ y)
    (hx : x ≠ getLast (y :: l) (cons_ne_nil _ _)) :
    next (y :: l) x h = next l x (by simpa [hy] using h) := by
  rw [next, next, nextOr_cons_of_ne _ _ _ _ hy, nextOr_eq_nextOr_of_mem_of_ne]
  · rwa [getLast_cons] at hx
    exact ne_nil_of_mem (by assumption)
  · rwa [getLast_cons] at hx","We start by noting that the assumption that the cardinality of $n$ is zero is equivalent to, and thus can be converted to, $n$ being empty. We need to prove that the determinant of $M$ is $1$. First, we try to show that $M = 1$ (the identity matrix). To show this, we prove that for every $i \in n$, $M i = 1 i$. However, since $n$ is empty, $M i = 1 i$ holds for every $i \in n$ vacuously. Hence, $M = 1$. Therefore, the determinant of $M$ is $\det M = \det 1 = 1$. This completes the proof.

---

### Line by Line Informal Proof

#### Informal Step 1

**Formal proof line:**
rw [next, next, nextOr_cons_of_ne _ _ _ _ hy, nextOr_eq_nextOr_of_mem_of_ne]

**Informal translation:**
We use the definition of the next element in a list cycle and the properties of the `nextOr` function to rewrite the goal. Specifically, we use the fact that the next element in the list \( y :: l \) after \( x \) is the same as the next element in the list \( l \) after \( x \) if \( x \neq y \) and \( x \neq (y :: l). \text{getLast} \). This simplifies our goal to showing that \( x \in l \) and \( x \neq l. \text{getLast} \).

#### Informal Step 2

**Formal proof line:**
· rwa [getLast_cons] at hx

**Informal translation:**
We use the property that the last element of the list \( y :: l \) is the same as the last element of the list \( l \) to rewrite the assumption \( x \neq (y :: l). \text{getLast} \) to \( x \neq l. \text{getLast} \). This simplifies our goal to showing that \( l \neq [] \).

#### Informal Step 3

**Formal proof line:**
exact ne_nil_of_mem (by assumption)

**Informal translation:**
Since \( x \in l \), it follows that \( l \) is not the empty list. This completes the proof of the goal \( l \neq [] \).

#### Informal Step 4

**Formal proof line:**
· rwa [getLast_cons] at hx

**Informal translation:**
We use the property that the last element of the list \( y :: l \) is the same as the last element of the list \( l \) to rewrite the assumption \( x \neq (y :: l). \text{getLast} \) to \( x \neq l. \text{getLast} \). This completes the proof.

### Whole Informal Proof

**Theorem:**
For any list \( l \) of elements of type \( \alpha \) and any element \( x \in l \), if \( x \) is not equal to the head element \( y \) of the list \( y :: l \) and \( x \) is not equal to the last element of the list \( y :: l \), then the next element in the list \( y :: l \) after \( x \) is the same as the next element in the list \( l \) after \( x \). Formally, if \( x \in y :: l \) and \( x \neq y \) and \( x \neq (y :: l). \text{getLast} \), then \((y :: l). \text{next}(x, h) = l. \text{next}(x, \cdots)\).

**Proof:**
We start by using the definition of the next element in a list cycle and the properties of the `nextOr` function. Specifically, we use the fact that the next element in the list \( y :: l \) after \( x \) is the same as the next element in the list \( l \) after \( x \) if \( x \neq y \) and \( x \neq (y :: l). \text{getLast} \). This simplifies our goal to showing that \( x \in l \) and \( x \neq l. \text{getLast} \).

Next, we use the property that the last element of the list \( y :: l \) is the same as the last element of the list \( l \) to rewrite the assumption \( x \neq (y :: l). \text{getLast} \) to \( x \neq l. \text{getLast} \). This simplifies our goal to showing that \( l \neq [] \).

Since \( x \in l \), it follows that \( l \) is not the empty list. This completes the proof of the goal \( l \neq [] \).

Finally, we use the property that the last element of the list \( y :: l \) is the same as the last element of the list \( l \) to rewrite the assumption \( x \neq (y :: l). \text{getLast} \) to \( x \neq l. \text{getLast} \). This completes the proof.","theorem List.next_ne_head_ne_getLast (h : x ∈ l) (y : α) (h : x ∈ y :: l) (hy : x ≠ y)
    (hx : x ≠ getLast (y :: l) (cons_ne_nil _ _)) :
    next (y :: l) x h = next l x (by simpa [hy] using h) := by
/- First, we use the equivalence that the cardinality of a finite type is $0$ if and only if this type is empty, to convert the assumption of cardinality of $n = 0$ into $n$ is empty. -/
/- It suffices to show that $M = 1$, because if this is true, by using this $M = 1$ and simplifying, we get the determinant of $M$ is $1$ automatically. -/
/- Using extensionality, to prove that $M = 1$, we only need to show that $M i = 1 i$ for every $i \in n$. -/
/- Since $n$ is empty, this $M i = 1 i$ holds for every $i \in n$ vacuously. This finishes the proof. -/
  rw [next, next, nextOr_cons_of_ne _ _ _ _ hy, nextOr_eq_nextOr_of_mem_of_ne]
  · rwa [getLast_cons] at hx
    exact ne_nil_of_mem (by assumption)
  · rwa [getLast_cons] at hx","import Mathlib.Data.Fintype.List
import Mathlib.Data.List.Cycle

open List
variable {α : Type*} [DecidableEq α]
variable (l : List α) (x : α)
"
37709,HasFiniteFPowerSeriesAt.add,"theorem HasFiniteFPowerSeriesAt.add (hf : HasFiniteFPowerSeriesAt f pf x n)
    (hg : HasFiniteFPowerSeriesAt g pg x m) :
    HasFiniteFPowerSeriesAt (f + g) (pf + pg) x (max n m) := by sorry","For any two functions $f$ and $g$ with finite formal power series representations $pf$ and $pg$ at point $x$ up to degrees $n$ and $m$ respectively, show that the sum function $f + g$ has a finite formal power series representation at $x$ up to degree $\max(n, m)$.","theorem HasFiniteFPowerSeriesAt.add (hf : HasFiniteFPowerSeriesAt f pf x n)
    (hg : HasFiniteFPowerSeriesAt g pg x m) :
    HasFiniteFPowerSeriesAt (f + g) (pf + pg) x (max n m) := by
  rcases (hf.eventually.and hg.eventually).exists with ⟨r, hr⟩
  exact ⟨r, hr.1.add hr.2⟩","Given that $f$ has a finite formal power series representation $pf$ at $x$ up to degree $n$ and $g$ has a finite formal power series representation $pg$ at $x$ up to degree $m$, we need to show that the sum function $f + g$ has a finite formal power series representation at $x$ up to degree $\max(n, m)$.

1. **Existence of a Common Radius:**
Since $f$ has a finite formal power series representation $pf$ at $x$ up to degree $n$, there exists a positive radius $r$ such that $f$ has a finite formal power series representation on the ball of radius $r$ around $x$. Similarly, since $g$ has a finite formal power series representation $pg$ at $x$ up to degree $m$, there exists a positive radius $r$ such that $g$ has a finite formal power series representation on the ball of radius $r$ around $x$. We denote this common radius by $r$ and the corresponding properties by $hr.1$ and $hr.2$.

2. **Sum of Finite Formal Power Series:**
On the ball of radius $r$ around $x$, the sum function $f + g$ has a finite formal power series representation. This is because the sum of the formal power series representations $pf$ and $pg$ is also a finite formal power series representation. Specifically, the formal power series representation of $f + g$ is $pf + pg$ and the degree is $\max(n, m)$.

Therefore, the sum function $f + g$ has a finite formal power series representation at $x$ up to degree $\max(n, m)$. This completes the proof. $\blacksquare$","theorem HasFiniteFPowerSeriesAt.add (hf : HasFiniteFPowerSeriesAt f pf x n)
    (hg : HasFiniteFPowerSeriesAt g pg x m) :
    HasFiniteFPowerSeriesAt (f + g) (pf + pg) x (max n m) := by
/- Since $f$ has a finite formal power series representation $pf$ at $x$ up to degree $n$ and $g$ has a finite formal power series representation $pg$ at $x$ up to degree $m$, there exists a positive radius $r$ such that both $f$ and $g$ have finite formal power series representations on the ball of radius $r$ around $x$. We denote this radius by $r$ and the corresponding properties by $hr.1$ and $hr.2$. -/
  rcases (hf.eventually.and hg.eventually).exists with ⟨r, hr⟩
/- We now show that the sum function $f + g$ has a finite formal power series representation on the ball of radius $r$ around $x$ with degree $\max(n, m)$. This is true because $f$ and $g$ both have finite formal power series representations on this ball, and the sum of their formal power series representations is also a finite formal power series representation. Specifically, the formal power series representation of $f + g$ is $pf + pg$ and the degree is $\max(n, m)$. This completes the proof. -/
  exact ⟨r, hr.1.add hr.2⟩","import Mathlib.Analysis.Analytic.Basic
import Mathlib.Analysis.Analytic.CPolynomial

open HasFiniteFPowerSeriesAt
variable {𝕜 E F G : Type*} [NontriviallyNormedField 𝕜] [NormedAddCommGroup E] [NormedSpace 𝕜 E]
  [NormedAddCommGroup F] [NormedSpace 𝕜 F] [NormedAddCommGroup G] [NormedSpace 𝕜 G]
open scoped Classical Topology
open Set Filter Asymptotics NNReal ENNReal
variable {f g : E → F} {p pf pg : FormalMultilinearSeries 𝕜 E F} {x : E} {r r' : ℝ≥0∞} {n m : ℕ}
variable (𝕜)
variable {𝕜}
"
38992,ite_id,theorem ite_id [Decidable c] {α} (t : α) : (if c then t else t) = t := by sorry,"If-then-else Identity: $(\text{if } c \text{ then } t \text{ else } t) = t$ : For any proposition $c$ and any type $\alpha$, if $c$ is decidable, then the if-then-else expression `if c then t else t` is equal to $t$ for any element $t \in \alpha$. In other words, $(\text{if } c \text{ then } t \text{ else } t) = t$.","theorem ite_id [Decidable c] {α} (t : α) : (if c then t else t) = t := by
  split <;> rfl","We need to show that the if-then-else expression $(\text{if } c \text{ then } t \text{ else } t)$ is equal to $t$ for any element $t \in \alpha$. We discuss by cases based on the condition $c$:

1. **Case 1: $c$ is true.**
- In this case, the expression $(\text{if } c \text{ then } t \text{ else } t)$ simplifies to $t$.
- Therefore, we need to show that $t = t$.
- This is trivially true due to the reflexive property of equality.

2. **Case 2: $c$ is false.**
- In this case, the expression $(\text{if } c \text{ then } t \text{ else } t)$ simplifies to $t$.
- Therefore, we need to show that $t = t$.
- This is also trivially true due to the reflexive property of equality.

Since both cases are trivially true, we conclude that $(\text{if } c \text{ then } t \text{ else } t) = t$ for any element $t \in \alpha$. This completes the proof. $\blacksquare$","theorem ite_id [Decidable c] {α} (t : α) : (if c then t else t) = t := by
  split <;> rfl","import Init.Classical
import Init.ByCases



"
39562,GenContFract.convs'Aux_stable_step_of_terminated,"theorem GenContFract.convs'Aux_stable_step_of_terminated {s : Stream'.Seq <| Pair K}
    (terminatedAt_n : s.TerminatedAt n) : convs'Aux s (n + 1) = convs'Aux s n := by sorry","Stability of Generalised Continued Fraction Approximations at Next Step Under Termination : For any division ring \( K \), a sequence \( s \) of pairs \((a_i, b_i)\) in \( K \), and a natural number \( n \), if the sequence \( s \) terminates at position \( n \), then the approximation of the generalised continued fraction up to position \( n + 1 \) is equal to the approximation up to position \( n \). In other words, if \( s \) terminates at \( n \), then \( \text{convs'Aux}(s, n + 1) = \text{convs'Aux}(s, n) \).","theorem GenContFract.convs'Aux_stable_step_of_terminated {s : Stream'.Seq <| Pair K}
    (terminatedAt_n : s.TerminatedAt n) : convs'Aux s (n + 1) = convs'Aux s n := by
  change s.get? n = none at terminatedAt_n
  induction n generalizing s with
  | zero => simp only [convs'Aux, terminatedAt_n, Stream'.Seq.head]
  | succ n IH =>
    cases s_head_eq : s.head with
    | none => simp only [convs'Aux, s_head_eq]
    | some gp_head =>
      have : s.tail.TerminatedAt n := by
        simp only [Stream'.Seq.TerminatedAt, s.get?_tail, terminatedAt_n]
      have := IH this
      rw [convs'Aux] at this
      simp [this, Nat.add_eq, add_zero, convs'Aux, s_head_eq]","We need to show that if the sequence \( s \) terminates at position \( n \), then the approximation of the generalised continued fraction up to position \( n + 1 \) is equal to the approximation up to position \( n \).

1. **Base Case:**
- If \( n = 0 \), the sequence \( s \) terminates at position \( 0 \). This means the first element of \( s \) is `none`.
- By the definition of `convs'Aux`, the approximation at position \( 0 \) is \( 0 \).
- Therefore, the approximation at position \( 0 + 1 \) is also \( 0 \), and the base case is proved.

2. **Inductive Step:**
- Assume the statement holds for \( n \). That is, if the sequence \( s \) terminates at position \( n \), then \( \text{convs'Aux}(s, n + 1) = \text{convs'Aux}(s, n) \).
- We need to show that if the sequence \( s \) terminates at position \( n + 1 \), then \( \text{convs'Aux}(s, n + 1 + 1) = \text{convs'Aux}(s, n + 1) \).

3. **Case Analysis:**
- **Case 1: The head of the sequence \( s \) is `none`:**
- If the head of \( s \) is `none`, the sequence \( s \) terminates at position \( 0 \).
- By the definition of `convs'Aux`, the approximation at position \( 0 \) is \( 0 \).
- Therefore, the approximation at position \( 0 + 1 \) is also \( 0 \), and the case is proved.
- **Case 2: The head of the sequence \( s \) is `some gp_head`:**
- If the head of \( s \) is `some gp_head`, we need to show that the approximation of the generalised continued fraction up to position \( n + 1 + 1 \) is equal to the approximation up to position \( n + 1 \).
- We show that the tail of the sequence \( s \) terminates at position \( n \).
- Using the inductive hypothesis, we have \( \text{convs'Aux}(s_{\text{tail}}, n + 1) = \text{convs'Aux}(s_{\text{tail}}, n) \).
- By the definition of `convs'Aux`, the approximation at position \( n + 1 + 1 \) is equal to the approximation at position \( n + 1 \).

Thus, by induction, the theorem is proved. \(\blacksquare\)","theorem GenContFract.convs'Aux_stable_step_of_terminated {s : Stream'.Seq <| Pair K}
    (terminatedAt_n : s.TerminatedAt n) : convs'Aux s (n + 1) = convs'Aux s n := by
/- We use the definition of termination to convert the assumption that the sequence \( s \) terminates at position \( n \) into the statement that the \( n \)-th element of \( s \) is `none`. -/
  change s.get? n = none at terminatedAt_n
  induction n generalizing s with
/- For the base case where \( n = 0 \), we use the definition of `convs'Aux` and the fact that the sequence terminates at \( 0 \) to simplify the goal. Since the sequence terminates at \( 0 \), the first element of the sequence is `none`, and thus the approximation at position \( 0 \) is \( 0 \). This completes the base case. -/
  | zero => simp only [convs'Aux, terminatedAt_n, Stream'.Seq.head]
/- For the inductive step, assume that the statement holds for \( n \). We need to show that the approximation of the generalised continued fraction up to position \( n + 1 + 1 \) is equal to the approximation up to position \( n + 1 \). -/
  | succ n IH =>
    cases s_head_eq : s.head with
/- If the head of the sequence \( s \) is `none`, we use the definition of `convs'Aux` and the fact that the head is `none` to simplify the goal. Since the head is `none`, the approximation at position \( n + 1 \) is \( 0 \), and thus the approximation at position \( n + 1 + 1 \) is also \( 0 \). This completes the case where the head is `none`. -/
    | none => simp only [convs'Aux, s_head_eq]
/- If the head of the sequence \( s \) is `some gp_head`, we need to show that the approximation of the generalised continued fraction up to position \( n + 1 + 1 \) is equal to the approximation up to position \( n + 1 \). -/
    | some gp_head =>
/- We show that the tail of the sequence \( s \) terminates at position \( n \). This is because the sequence \( s \) terminates at position \( n + 1 \), and the tail of \( s \) is the sequence starting from the second element. -/
      have : s.tail.TerminatedAt n := by
/- Using the definition of termination and the fact that the sequence \( s \) terminates at position \( n + 1 \), we simplify the goal to show that the tail of the sequence \( s \) terminates at position \( n \). -/
        simp only [Stream'.Seq.TerminatedAt, s.get?_tail, terminatedAt_n]
/- Using the inductive hypothesis, we show that the approximation of the generalised continued fraction up to position \( n + 1 \) for the tail of the sequence \( s \) is equal to the approximation up to position \( n \). -/
      have := IH this
/- We use the definition of `convs'Aux` to rewrite the inductive hypothesis. This shows that the approximation of the generalised continued fraction up to position \( n + 1 \) for the tail of the sequence \( s \) is equal to the approximation up to position \( n \). -/
      rw [convs'Aux] at this
/- Using the definition of `convs'Aux`, the fact that the sequence terminates at \( n \), and the properties of addition, we simplify the goal to show that the approximation of the generalised continued fraction up to position \( n + 1 \) is equal to the approximation up to position \( n \). -/
/- Using the inductive hypothesis, the properties of addition, and the definition of `convs'Aux`, we simplify the goal to show that the approximation of the generalised continued fraction up to position \( n + 1 + 1 \) is equal to the approximation up to position \( n + 1 \). -/
      simp [this, Nat.add_eq, add_zero, convs'Aux, s_head_eq]","import Mathlib.Algebra.ContinuedFractions.Translations
import Mathlib.Algebra.ContinuedFractions.TerminatedStable

open GenContFract
variable {K : Type*} {g : GenContFract K} {n m : ℕ}
variable [DivisionRing K]
"
39639,JoinedIn.mem,theorem JoinedIn.mem (h : JoinedIn F x y) : x ∈ F ∧ y ∈ F := by sorry,"Path in Set Implies Both Endpoints are in the Set : For any topological space \( X \) and any subset \( F \subseteq X \), if there is a path \( \gamma \) joining points \( x \) and \( y \) such that the image of \( \gamma \) is contained in \( F \), then both \( x \) and \( y \) are elements of \( F \).","theorem JoinedIn.mem (h : JoinedIn F x y) : x ∈ F ∧ y ∈ F := by
  rcases h with ⟨γ, γ_in⟩
  have : γ 0 ∈ F ∧ γ 1 ∈ F := by constructor <;> apply γ_in
  simpa using this","Given a topological space \( X \) and a subset \( F \subseteq X \), suppose there is a path \( \gamma \) joining points \( x \) and \( y \) such that the image of \( \gamma \) is contained in \( F \). This means there exists a continuous function \( \gamma : [0, 1] \to X \) such that \( \gamma(0) = x \), \( \gamma(1) = y \), and \( \gamma(t) \in F \) for all \( t \in [0, 1] \).

To prove that \( x \in F \) and \( y \in F \), we note that since \( \gamma(t) \in F \) for all \( t \in [0, 1] \), it follows that \( \gamma(0) \in F \) and \( \gamma(1) \in F \). By the definition of the path \( \gamma \), we have \( \gamma(0) = x \) and \( \gamma(1) = y \). Therefore, \( x \in F \) and \( y \in F \).

This completes the proof. \(\blacksquare\)","theorem JoinedIn.mem (h : JoinedIn F x y) : x ∈ F ∧ y ∈ F := by
/- We start by deconstructing the hypothesis \( h \) that there is a path \( \gamma \) joining \( x \) and \( y \) such that the image of \( \gamma \) is contained in \( F \). This gives us a path \( \gamma \) from \( x \) to \( y \) and a function \( \gamma_{\text{in}} \) that ensures \( \gamma(t) \in F \) for all \( t \in [0, 1] \). -/
  rcases h with ⟨γ, γ_in⟩
/- We need to show that both \( \gamma(0) \) and \( \gamma(1) \) are in \( F \). To do this, we construct the conjunction \( \gamma(0) \in F \) and \( \gamma(1) \in F \) by applying the function \( \gamma_{\text{in}} \) to \( 0 \) and \( 1 \) respectively. Since \( \gamma_{\text{in}} \) ensures that \( \gamma(t) \in F \) for all \( t \in [0, 1] \), it follows that \( \gamma(0) \in F \) and \( \gamma(1) \in F \). -/
  have : γ 0 ∈ F ∧ γ 1 ∈ F := by constructor <;> apply γ_in
/- We have shown that \( \gamma(0) \in F \) and \( \gamma(1) \in F \). Since \( \gamma(0) = x \) and \( \gamma(1) = y \) by the definition of a path, it follows that \( x \in F \) and \( y \in F \). This completes the proof. -/
  simpa using this","import Mathlib.Topology.Order.ProjIcc
import Mathlib.Topology.CompactOpen
import Mathlib.Topology.UnitInterval
import Mathlib.Topology.Connected.PathConnected

open JoinedIn
open Topology Filter unitInterval Set Function
variable {X Y : Type*} [TopologicalSpace X] [TopologicalSpace Y] {x y z : X} {ι : Type*}
variable (γ : Path x y)
open ContinuousMap
variable {a₁ a₂ a₃ : X} {b₁ b₂ b₃ : Y}
variable {χ : ι → Type*} [∀ i, TopologicalSpace (χ i)] {as bs cs : ∀ i, χ i}
variable (X)
variable {X}
variable {F : Set X}
"
39720,List.any_iff_exists_prop,"theorem List.any_iff_exists_prop : (any l fun a => p a) ↔ ∃ a ∈ l, p a := by sorry","List Any Predicate is True if and only if Exists an Element Satisfying the Predicate: \( (l.\text{any} \, p) = \text{true} \leftrightarrow \exists a \in l, \, p(a) \) : For any type \( \alpha \) and a decidable predicate \( p : \alpha \to \text{Prop} \), a list \( l \) of elements of type \( \alpha \) satisfies the predicate \( p \) for at least one element if and only if there exists an element \( a \) in \( l \) such that \( p(a) \) holds. Formally, this is expressed as:
\[
(l.\text{any} \, (\lambda a, \text{decide} \, (p \, a))) = \text{true} \leftrightarrow \exists a \in l, \, p(a)
\]","theorem List.any_iff_exists_prop : (any l fun a => p a) ↔ ∃ a ∈ l, p a := by
  simp","To prove the equivalence, we need to show two directions:

1. **(\(\Rightarrow\)) If \( (l.\text{any} \, (\lambda a, \text{decide} \, (p \, a))) = \text{true} \), then there exists an element \( a \in l \) such that \( p(a) \) holds.**

- Assume \( (l.\text{any} \, (\lambda a, \text{decide} \, (p \, a))) = \text{true} \).
- By the definition of `List.any`, this means that there is at least one element \( a \) in the list \( l \) such that \( \text{decide} \, (p \, a) = \text{true} \).
- Since \( \text{decide} \, (p \, a) = \text{true} \) implies \( p(a) \), we have \( p(a) \) holds for some \( a \in l \).

2. **(\(\Leftarrow\)) If there exists an element \( a \in l \) such that \( p(a) \) holds, then \( (l.\text{any} \, (\lambda a, \text{decide} \, (p \, a))) = \text{true} \).**

- Assume there exists an element \( a \in l \) such that \( p(a) \) holds.
- By the definition of `List.any`, this means that \( \text{decide} \, (p \, a) = \text{true} \) for some \( a \in l \).
- Therefore, \( (l.\text{any} \, (\lambda a, \text{decide} \, (p \, a))) = \text{true} \).

Since both directions are proven, we conclude that:
\[
(l.\text{any} \, (\lambda a, \text{decide} \, (p \, a))) = \text{true} \leftrightarrow \exists a \in l, \, p(a)
\]
This completes the proof.","theorem List.any_iff_exists_prop : (any l fun a => p a) ↔ ∃ a ∈ l, p a := by
  simp","import Batteries.Tactic.Alias
import Mathlib.Tactic.TypeStar
import Mathlib.Data.Bool.AllAny

open List
variable {α : Type*} {p : α → Prop} [DecidablePred p] {l : List α} {a : α}
"
41089,HasStrictDerivAt.mul,"theorem HasStrictDerivAt.mul (hc : HasStrictDerivAt c c' x) (hd : HasStrictDerivAt d d' x) :
    HasStrictDerivAt (fun y => c y * d y) (c' * d x + c x * d') x := by sorry","Strict Product Rule for Derivatives : For a nontrivially normed field \( \mathbb{K} \), let \( x \) be an element of \( \mathbb{K} \), and let \( \mathbb{A} \) be a normed ring that is also a normed algebra over \( \mathbb{K} \). Suppose \( c \) and \( d \) are functions from \( \mathbb{K} \) to \( \mathbb{A} \) with strict derivatives \( c' \) and \( d' \) at \( x \), respectively. Then, the function \( f(y) = c(y) \cdot d(y) \) has the strict derivative \( f'(x) = c'(x) \cdot d(x) + c(x) \cdot d'(x) \) at \( x \).","theorem HasStrictDerivAt.mul (hc : HasStrictDerivAt c c' x) (hd : HasStrictDerivAt d d' x) :
    HasStrictDerivAt (fun y => c y * d y) (c' * d x + c x * d') x := by
  have := (HasStrictFDerivAt.mul' hc hd).hasStrictDerivAt
  rwa [ContinuousLinearMap.add_apply, ContinuousLinearMap.smul_apply,
    ContinuousLinearMap.smulRight_apply, ContinuousLinearMap.smulRight_apply,
    ContinuousLinearMap.smulRight_apply, ContinuousLinearMap.one_apply, one_smul, one_smul,
    add_comm] at this","To prove the theorem, we start by using the fact that if \( c \) has a strict derivative \( c' \) at \( x \) and \( d \) has a strict derivative \( d' \) at \( x \), then the function \( y \mapsto c(y) \cdot d(y) \) has a strict derivative at \( x \) given by \( c(x) \cdot d' + c'(x) \cdot d(x) \). This gives us a new hypothesis that \( y \mapsto c(y) \cdot d(y) \) has a strict derivative at \( x \) given by \( c(x) \cdot d' + c'(x) \cdot d(x) \).

Next, we simplify the expression for the strict derivative of \( y \mapsto c(y) \cdot d(y) \) at \( x \) using the properties of continuous linear maps and scalar multiplication:
1. The addition of continuous linear maps applied to an element: \((f + g)(x) = f(x) + g(x)\).
2. Scalar multiplication on continuous linear maps: \((c \cdot f)(x) = c \cdot f(x)\).
3. Application of continuous scalar multiplication on the right: \((c \cdot f)(x) = c(x) \cdot f\).
4. The identity map in continuous linear maps: \(\text{id}(x) = x\).
5. The multiplicative identity action: \(1 \cdot b = b\).
6. The commutativity of addition: \(a + b = b + a\).

After applying these properties, we find that the strict derivative of \( y \mapsto c(y) \cdot d(y) \) at \( x \) is \( c'(x) \cdot d(x) + c(x) \cdot d'(x) \).

Thus, the function \( f(y) = c(y) \cdot d(y) \) has the strict derivative \( f'(x) = c'(x) \cdot d(x) + c(x) \cdot d'(x) \) at \( x \). This completes the proof. \(\blacksquare\)","theorem HasStrictDerivAt.mul (hc : HasStrictDerivAt c c' x) (hd : HasStrictDerivAt d d' x) :
    HasStrictDerivAt (fun y => c y * d y) (c' * d x + c x * d') x := by
/- First, we use the fact that if \( c \) has a strict derivative \( c' \) at \( x \) and \( d \) has a strict derivative \( d' \) at \( x \), then the function \( y \mapsto c(y) \cdot d(y) \) has a strict derivative at \( x \) given by \( c(x) \cdot d' + c'(x) \cdot d(x) \). This gives us a new hypothesis that \( y \mapsto c(y) \cdot d(y) \) has a strict derivative at \( x \) given by \( c(x) \cdot d' + c'(x) \cdot d(x) \). -/
  have := (HasStrictFDerivAt.mul' hc hd).hasStrictDerivAt
/- We simplify the expression for the strict derivative of \( y \mapsto c(y) \cdot d(y) \) at \( x \) using the properties of continuous linear maps and scalar multiplication. Specifically, we use the following properties:
1. The addition of continuous linear maps applied to an element: \((f + g)(x) = f(x) + g(x)\).
2. Scalar multiplication on continuous linear maps: \((c \cdot f)(x) = c \cdot f(x)\).
3. Application of continuous scalar multiplication on the right: \((c \cdot f)(x) = c(x) \cdot f\).
4. The identity map in continuous linear maps: \(\text{id}(x) = x\).
5. The multiplicative identity action: \(1 \cdot b = b\).
6. The commutativity of addition: \(a + b = b + a\).

After applying these properties, we find that the strict derivative of \( y \mapsto c(y) \cdot d(y) \) at \( x \) is \( c'(x) \cdot d(x) + c(x) \cdot d'(x) \). -/
  rwa [ContinuousLinearMap.add_apply, ContinuousLinearMap.smul_apply,
    ContinuousLinearMap.smulRight_apply, ContinuousLinearMap.smulRight_apply,
    ContinuousLinearMap.smulRight_apply, ContinuousLinearMap.one_apply, one_smul, one_smul,
    add_comm] at this","import Mathlib.Analysis.Calculus.Deriv.Basic
import Mathlib.Analysis.Calculus.FDeriv.Mul
import Mathlib.Analysis.Calculus.FDeriv.Add
import Mathlib.Analysis.Calculus.Deriv.Mul

open HasStrictDerivAt
open scoped Topology Filter ENNReal
open Filter Asymptotics Set
open ContinuousLinearMap (smulRight smulRight_one_eq_iff)
variable {𝕜 : Type u} [NontriviallyNormedField 𝕜]
variable {F : Type v} [NormedAddCommGroup F] [NormedSpace 𝕜 F]
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {G : Type*} [NormedAddCommGroup G] [NormedSpace 𝕜 G]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {F : Type v} [NormedAddCommGroup F] [NormedSpace 𝕜 F]
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {G : Type*} [NormedAddCommGroup G] [NormedSpace 𝕜 G]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {E : Type w} [NormedAddCommGroup E] [NormedSpace 𝕜 E]
variable {G : Type*} [NormedAddCommGroup G] [NormedSpace 𝕜 G]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {G : Type*} [NormedAddCommGroup G] [NormedSpace 𝕜 G]
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {f f₀ f₁ g : 𝕜 → F}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {f' f₀' f₁' g' : F}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {x : 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {s t : Set 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {L L₁ L₂ : Filter 𝕜}
variable {B : E →L[𝕜] F →L[𝕜] G} {u : 𝕜 → E} {v : 𝕜 → F} {u' : E} {v' : F}
variable {𝕜' : Type*} [NontriviallyNormedField 𝕜'] [NormedAlgebra 𝕜 𝕜'] [NormedSpace 𝕜' F]
  [IsScalarTower 𝕜 𝕜' F] {c : 𝕜 → 𝕜'} {c' : 𝕜'}
variable {R : Type*} [Semiring R] [Module R F] [SMulCommClass 𝕜 R F] [ContinuousConstSMul R F]
variable {𝕜' 𝔸 : Type*} [NormedField 𝕜'] [NormedRing 𝔸] [NormedAlgebra 𝕜 𝕜'] [NormedAlgebra 𝕜 𝔸]
  {c d : 𝕜 → 𝔸} {c' d' : 𝔸} {u v : 𝕜 → 𝕜'}
"
43560,PEquiv.inj,"theorem PEquiv.inj (f : α ≃. β) {a₁ a₂ : α} {b : β} (h₁ : b ∈ f a₁) (h₂ : b ∈ f a₂) :
    a₁ = a₂ := by sorry","Injectivity of Partial Equivalence: $b \in f(a_1)$ and $b \in f(a_2)$ implies $a_1 = a_2$ : For any partial equivalence $f : \alpha \simeq. \beta$ between types $\alpha$ and $\beta$, and for any elements $a_1, a_2 \in \alpha$ and $b \in \beta$, if $b$ is in the image of $a_1$ under $f$ and $b$ is also in the image of $a_2$ under $f$, then $a_1$ must be equal to $a_2$. In other words, if $f(a_1) = \text{some } b$ and $f(a_2) = \text{some } b$, then $a_1 = a_2$.","theorem PEquiv.inj (f : α ≃. β) {a₁ a₂ : α} {b : β} (h₁ : b ∈ f a₁) (h₂ : b ∈ f a₂) :
    a₁ = a₂ := by
  rw [← mem_iff_mem] at *; cases h : f.symm b <;> simp_all","We start with the given partial equivalence \( f : \alpha \simeq. \beta \) and elements \( a_1, a_2 \in \alpha \) and \( b \in \beta \). We are given that \( b \in f(a_1) \) and \( b \in f(a_2) \). Using the equivalence that \( a \in f^{\text{symm}}(b) \) if and only if \( b \in f(a) \), we can rewrite the assumptions as \( a_1 \in f^{\text{symm}}(b) \) and \( a_2 \in f^{\text{symm}}(b) \).

Since \( f \) is a partial equivalence, \( f^{\text{symm}} \) is the inverse of \( f \). Therefore, \( f^{\text{symm}}(b) \) is a set containing exactly one element, which is the unique element \( a \in \alpha \) such that \( b \in f(a) \). Given that both \( a_1 \) and \( a_2 \) are in \( f^{\text{symm}}(b) \), it follows that \( a_1 \) and \( a_2 \) must be the same element. Hence, \( a_1 = a_2 \).

This completes the proof.","theorem PEquiv.inj (f : α ≃. β) {a₁ a₂ : α} {b : β} (h₁ : b ∈ f a₁) (h₂ : b ∈ f a₂) :
    a₁ = a₂ := by
  rw [← mem_iff_mem] at *; cases h : f.symm b <;> simp_all","import Mathlib.Data.Option.Basic
import Mathlib.Data.Set.Basic
import Batteries.Tactic.Congr
import Mathlib.Data.PEquiv

open PEquiv
variable {α : Type u} {β : Type v} {γ : Type w} {δ : Type x}
open Function Option
"
